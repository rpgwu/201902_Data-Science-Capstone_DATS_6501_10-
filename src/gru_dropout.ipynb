{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending LSTMs: LSTMs with Peepholes and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\software\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure president stories are downloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  ../data\\speech_1.txt\n",
      "File  speech_1.txt  already exists.\n",
      "Downloading file:  ../data\\speech_2.txt\n",
      "File  speech_2.txt  already exists.\n",
      "Downloading file:  ../data\\speech_3.txt\n",
      "File  speech_3.txt  already exists.\n",
      "Downloading file:  ../data\\speech_4.txt\n",
      "File  speech_4.txt  already exists.\n",
      "Downloading file:  ../data\\speech_5.txt\n",
      "File  speech_5.txt  already exists.\n",
      "Downloading file:  ../data\\speech_6.txt\n",
      "File  speech_6.txt  already exists.\n",
      "Downloading file:  ../data\\speech_7.txt\n",
      "File  speech_7.txt  already exists.\n",
      "Downloading file:  ../data\\speech_8.txt\n",
      "File  speech_8.txt  already exists.\n",
      "Downloading file:  ../data\\speech_9.txt\n",
      "File  speech_9.txt  already exists.\n",
      "Downloading file:  ../data\\speech_10.txt\n",
      "File  speech_10.txt  already exists.\n",
      "Downloading file:  ../data\\speech_11.txt\n",
      "File  speech_11.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a directory if needed\n",
    "dir_name = \"../data\"\n",
    "num_files = 11\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  #Download a file if not present\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "\n",
    "filenames = [\"speech_\"+format(i, '01d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "11 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    print( filenames)\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "   # assert file_exists\n",
    "print('%d files found.'%len(filenames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file ../data\\speech_1.txt\n",
      "Data size (Characters) (Document 0) 3443\n",
      "Sample string (Document 0) ['fo', 'r ', 'my', 'se', 'lf', ' a', 'nd', ' f', 'or', ' o', 'ur', ' n', 'at', 'io', 'n,', ' i', ' w', 'an', 't ', 'to', ' t', 'ha', 'nk', ' m', 'y ', 'pr', 'ed', 'ec', 'es', 'so', 'r ', 'fo', 'r ', 'al', 'l ', 'he', ' h', 'as', ' d', 'on', 'e ', 'to', ' h', 'ea', 'l ', 'ou', 'r ', 'la', 'nd', '.\\n']\n",
      "\n",
      "Processing file ../data\\speech_2.txt\n",
      "Data size (Characters) (Document 1) 6871\n",
      "Sample string (Document 1) ['se', 'na', 'to', 'r ', 'ha', 'tf', 'ie', 'ld', ', ', 'mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'mo', 'nd', 'al', 'e,', ' s', 'en']\n",
      "\n",
      "Processing file ../data\\speech_3.txt\n",
      "Data size (Characters) (Document 2) 7320\n",
      "Sample string (Document 2) ['se', 'na', 'to', 'r ', 'ma', 'th', 'ia', 's,', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ' b', 'ur', 'ge', 'r,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'sp', 'ea', 'ke', 'r ', \"o'\", 'ne', 'il', 'l,', ' s', 'en', 'at', 'or', ' d', 'ol', 'e,', ' r', 'ev', 'er', 'en', 'd ']\n",
      "\n",
      "Processing file ../data\\speech_4.txt\n",
      "Data size (Characters) (Document 3) 6255\n",
      "Sample string (Document 3) ['mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' q', 'ua', 'yl', 'e,', ' s', 'en', 'at', 'or', ' m', 'it', 'ch', 'el', 'l,', ' s', 'pe', 'ak', 'er', ' w', 'ri', 'gh', 't,', ' s', 'en', 'at', 'or', ' d']\n",
      "\n",
      "Processing file ../data\\speech_5.txt\n",
      "Data size (Characters) (Document 4) 4540\n",
      "Sample string (Document 4) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'to', 'da', 'y ', 'we', ' c', 'el', 'eb', 'ra', 'te', ' t', 'he', ' m', 'ys', 'te', 'ry', ' o', 'f ', 'am', 'er', 'ic', 'an', ' r', 'en', 'ew', 'al', '. ', 'th', 'is', ' c', 'er', 'em', 'on', 'y ', 'is', ' h', 'el', 'd ', 'in', ' t', 'he']\n",
      "\n",
      "Processing file ../data\\speech_6.txt\n",
      "Data size (Characters) (Document 5) 6082\n",
      "Sample string (Document 5) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'at', ' t', 'hi', 's ', 'la', 'st', ' p', 're', 'si', 'de', 'nt', 'ia', 'l ', 'in', 'au', 'gu', 'ra', 'ti', 'on', ' o', 'f ', 'th', 'e ', '20', 'th', ' c', 'en', 'tu', 'ry', ', ', 'le', 't ', 'us', ' l', 'if', 't ', 'ou', 'r ', 'ey', 'es']\n",
      "\n",
      "Processing file ../data\\speech_7.txt\n",
      "Data size (Characters) (Document 6) 4520\n",
      "Sample string (Document 6) ['th', 'an', 'k ', 'yo', 'u,', ' a', 'll', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 're', 'hn', 'qu', 'is', 't,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'di', 'st', 'in']\n",
      "\n",
      "Processing file ../data\\speech_8.txt\n",
      "Data size (Characters) (Document 7) 5961\n",
      "Sample string (Document 7) ['vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' c', 'he', 'ne', 'y,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'pr', 'es', 'id', 'en', 't ', 'ca', 'rt', 'er', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'cl', 'in', 'to', 'n,', ' m', 'em', 'be']\n",
      "\n",
      "Processing file ../data\\speech_9.txt\n",
      "Data size (Characters) (Document 8) 6680\n",
      "Sample string (Document 8) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'i ', 'st', 'an', 'd ', 'he', 're', ' t', 'od', 'ay', ' h', 'um', 'bl', 'ed', ' b', 'y ', 'th', 'e ', 'ta', 'sk', ' b', 'ef', 'or', 'e ', 'us', ', ', 'gr', 'at', 'ef', 'ul', ' f', 'or', ' t', 'he', ' t', 'ru', 'st', ' y', 'ou', ' h', 'av']\n",
      "\n",
      "Processing file ../data\\speech_10.txt\n",
      "Data size (Characters) (Document 9) 5973\n",
      "Sample string (Document 9) ['th', 'an', 'k ', 'yo', 'u.', ' t', 'ha', 'nk', ' y', 'ou', ' s', 'o ', 'mu', 'ch', '.\\n', '\\nv', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bi', 'de', 'n,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'me', 'mb', 'er', 's ', 'of', ' t', 'he', ' u', 'ni', 'te', 'd ', 'st', 'at', 'es']\n",
      "\n",
      "Processing file ../data\\speech_11.txt\n",
      "Data size (Characters) (Document 10) 4223\n",
      "Sample string (Document 10) ['ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 'ro', 'be', 'rt', 's,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'ob', 'am', 'a,', ' f', 'el', 'lo', 'w ']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    # make all the text lowercase\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Breaking the text into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    # Creates a list of lists with the bigrams (outer loop different stories)\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries (Bigrams)\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61868 Characters found.\n",
      "Most common words (+UNK) [('e ', 1945), (' t', 1623), (' a', 1378), ('th', 1378), ('s ', 1110)]\n",
      "Least common words (+UNK) [('yâ', 1), ('tp', 1), ('”f', 1), ('”u', 1), ('kf', 1), ('-l', 1), ('40', 1), ('\\nl', 1), ('hm', 1), ('ja', 1), ('n:', 1), ('zo', 1), ('uy', 1), ('r:', 1), ('ky', 1)]\n",
      "Sample data [78, 15, 250, 63, 298, 3, 16, 33, 24, 7]\n",
      "Sample data [63, 121, 32, 15, 34, 0, 103, 117, 17, 0]\n",
      "Vocabulary:  351\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the RNN. More specifically the generator breaks a given sequence of words into `batch_size` segments. We also maintain a cursor for each segment. So whenever we create a batch of data, we sample one item from each segment and update the cursor of each segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\tpr (97), \tr  (15), \the (6), \te  (1), \tou (22), \n",
      "\tOutput:\n",
      "\ted (49), \tfo (78), \t h (50), \tto (32), \tr  (15), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\ted (49), \tfo (78), \t h (50), \tto (32), \tr  (15), \n",
      "\tOutput:\n",
      "\tec (114), \tr  (15), \tas (88), \t h (50), \tla (144), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tec (114), \tr  (15), \tas (88), \t h (50), \tla (144), \n",
      "\tOutput:\n",
      "\tes (26), \tal (58), \t d (59), \tea (52), \tnd (16), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\tes (26), \tal (58), \t d (59), \tea (52), \tnd (16), \n",
      "\tOutput:\n",
      "\tso (129), \tl  (51), \ton (21), \tl  (51), \t.\n",
      " (115), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\tso (129), \tl  (51), \ton (21), \tl  (51), \tou (22), \n",
      "\tOutput:\n",
      "\tr  (15), \the (6), \te  (1), \tou (22), \tr  (15), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM, LSTM with Peepholes and GRUs\n",
    "\n",
    "* A LSTM has 5 main components\n",
    "  * Cell state, Hidden state, Input gate, Forget gate, Output gate\n",
    "* A LSTM with peephole connections\n",
    "  * Introduces several new sets of weights that connects the cell state to the gates\n",
    "* A GRU has 3 main components\n",
    "  * Hidden state, Reset gate and a Update gate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "\n",
    "Here we define several hyperparameters and are very similar to the ones we defined . However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "batch_size = 64\n",
    "#num_unrollings = 50\n",
    "num_unrollings = 100\n",
    "dropout = 0.2\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test input (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "valid_inputs = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters and Cell Computation\n",
    "\n",
    "We define parameters and cell computation functions for all the different variants (LSTM, LSTM with peepholes and GRUs). **Make sure you only run a single cell withing this section (either the LSTM/ LSTM with peepholes or GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard LSTM\n",
    "\n",
    "Here we define the parameters and the cell computation function for a standard LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_dropout.csv\n"
     ]
    }
   ],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "#algorithm = 'lstm'\n",
    "#algorithm = 'lstm_peephole'\n",
    "algorithm = 'gru'\n",
    "\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "print( filename_to_save)\n",
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTMs with Peephole Connections\n",
    "\n",
    "We define the parameters and cell computation for a LSTM with peepholes. Note that we are using diagonal peephole connections (for more details refer the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "ic = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "fc = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],0.0,0.01))\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "oc = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],0.0,0.01))\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "algorithm = 'lstm_peephole'\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "# Definition of the cell computation.\n",
    "def lstm_with_peephole_cell(i, o, state):\n",
    "    '''\n",
    "    LSTM with peephole connections\n",
    "    Our implementation for peepholes is based on \n",
    "    https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf    \n",
    "    '''\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + state*ic + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + state*fc + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + state*oc + tf.matmul(o, om) + ob)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Units (GRUs)\n",
    "\n",
    "Finally we define the parameters and cell computations for the GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Reset gate: input, previous output, and bias.\n",
    "rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "rh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "rb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Hidden State: input, previous output, and bias.\n",
    "hx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "hh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "hb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Update gate: input, previous output, and bias.\n",
    "zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "zh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "zb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "algorithm = 'gru'\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "\n",
    "# Definition of the cell computation.\n",
    "def gru_cell(i, o):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    reset_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rh) + rb)\n",
    "    h_tilde = tf.tanh(tf.matmul(i,hx) + tf.matmul(reset_gate * o, hh) + hb)\n",
    "    z = tf.sigmoid(tf.matmul(i,zx) + tf.matmul(o, zh) + zb)\n",
    "    h = (1-z)*o + z*h_tilde\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM/GRU/LSTM-Peephole Computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "  state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "# Note: there is no cell state for GRUs\n",
    "for i in train_inputs:\n",
    "    if algorithm=='lstm':\n",
    "      output, state = lstm_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "    elif algorithm=='lstm_peephole':\n",
    "      output, state = lstm_with_peephole_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "    elif algorithm=='gru':\n",
    "      output = gru_cell(i, output)\n",
    "      train_state_update_ops = [saved_output.assign(output)]\n",
    "        \n",
    "    output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "valid_output = saved_valid_output\n",
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "  valid_state = saved_valid_state\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "if algorithm=='lstm':\n",
    "    valid_output, valid_state = lstm_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "    \n",
    "elif algorithm=='lstm_peephole':\n",
    "    valid_output, valid_state = lstm_with_peephole_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "elif algorithm=='gru':\n",
    "    valid_output = gru_cell(valid_inputs, valid_output)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output)]\n",
    "\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(valid_state_update_ops):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "if algorithm=='lstm':\n",
    "  test_output, test_state = lstm_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "elif algorithm=='lstm_peephole':\n",
    "  test_output, test_state = lstm_with_peephole_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "elif algorithm=='gru':\n",
    "  test_output = gru_cell(test_input, saved_test_output)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output)]\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(test_state_update_ops):\n",
    "    test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "We calculate the training loss of the LSTM here. It's a typical cross entropy loss calculated over all the scores we obtained for training data (`loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies(train_state_update_ops):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch). But since GRU doesn't have a cell state we have a conditioned reset_state ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "    # Reset train state\n",
    "    reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "    reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = tf.group(\n",
    "        saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01)),\n",
    "        saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.01)))\n",
    "    \n",
    "elif algorithm=='gru':\n",
    "    # Reset train state\n",
    "    reset_train_state = [tf.assign(saved_output, tf.zeros([batch_size, num_nodes]))]\n",
    "\n",
    "    # Reset valid state\n",
    "    reset_valid_state = [tf.assign(saved_valid_output, tf.zeros([1, num_nodes]))]\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = [saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using the Adam optimizer as it is one of the best optimizers out there. Furthermore we use gradient clipping to prevent any gradient explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for decaying learning rate\n",
    "gstep = tf.Variable(0, trainable=False)\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# Decays learning rate everytime the gstep increases\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Sampling to Break the Repetition\n",
    "Here we write some simple logic to break the repetition in text. Specifically instead of always getting the word that gave this highest prediction probability, we sample randomly where the probability of being selected given by their prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "  '''Greedy Sampling\n",
    "  We pick the three best predictions given by the LSTM and sample\n",
    "  one of them with very high probability of picking the best one'''\n",
    "  best_inds = np.argsort(distribution)[-3:]\n",
    "  best_probs = distribution[best_inds]/np.sum(distribution[best_inds])\n",
    "  best_idx = np.random.choice(best_inds,p=best_probs)\n",
    "  return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM to Generate Text\n",
    "\n",
    "Here we train the model on the available data and generate text using the trained model for several steps. From each document we extract text for `steps_per_document` steps to train the model on. We also report the train perplexity at the end of each step. Finally we test the model by asking it to generate some new text starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n",
      "(10).(4).(3).(0).(7).(2).(5).(8).(6).(9).\n",
      "Average loss at step 1: 5.323976\n",
      "\tPerplexity at step 1: 205.198057\n",
      "\n",
      "Valid Perplexity: 274.57\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      "\t aue  t a a a t te  a te  t a a a a te  a t t te e e  ae e e  a t te e  a a t a t t t ae e e  t te  te e  a t ae  ae  t a ae  ae  t a a a t a ae  t t t te  a a a ae  a te  a ae  ae e e e  te e  a a t t t a t te e  te  t t ae  t t a t t a a t ae e e  a te  te  t t ae  a ae  t te e  a a te e  te  t ae e  t te  a t a ae e e e e e e  t t a t te e e  t ae  t t t a ae e  a a ae  ae  t ae  a ae e  a t te  a t t a te  a ae  t ae e  te  te  t t ae  a ae e  a ae  te  t te  a t t a te  t t t a a a a t t ae e  a a a ae  t te e  te e e  a t a a ae e e  te  ae e  a a a ae e  a a t ae  ae  t ae e  a ae  t t a t a ae e  te e e  a t ae  a ae e e  a t ae e  te  a te e e  ae  a t te  t t te  ae  t ae  t ae  a a t ae e  a t te e  te  t a t ae  ae  t a te  ae e  t t te e  te  a a ae  t te e e  ae  ae  t t t ae e  t ae  a a t a te  ae e e e e  t te  te e  a ae e  t te  a ae e  ae  ae  ae e e e  t t te  te  ae  a te  a a t t a ae  te  a t t t te  t ae  ae e  a a a ae  te  a a t a t te  t t a te  te  a te  te e \n",
      "====================================================================\n",
      "\n",
      "(9).(1).(0).(4).(2).(8).(5).(7).(6).(3).\n",
      "Average loss at step 2: 5.172937\n",
      "\tPerplexity at step 2: 176.432187\n",
      "\n",
      "Valid Perplexity: 232.50\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      "\t de and te e e  a t te e  ae  a t ae  a t t a t a a ae e e e e  t t ae  a ae  a a t a te  te  t ae  a te  ae  te  ae  t a a ae e e  a ae  a ae  a a t te e  te e  te e  t te  ae  a a a t a t a ae  te e  t a a t te  te  te e  te  a a te  t a ae  a t a te  a ae e  t a a te e  a t a ae  t t t a te  a a t a a t t a t t a te e  ae  t a t t t a ae  a a a a te  a t t a te  t a t a ae e  a te  t ae e e  a t t t ae e e e e  a ae e e  t te  te  te  ae  a t a a ae  t t t te  a t a t t te  ae e e e  te e e e e  te  te  ae  te e e  t a a a ae  t t ae e  te  te  a a t t t te e e e  t ae  a ae e  te  a t te  te  a t a a t a a te  te e  ae e e e  te e  a ae  t ae  a a te  te e  te  a te e  a ae  t te e  a ae  ae  t ae  te  t a a te e  ae  t a a a ae  a te  t t a t ae e  ae e  a ae e e  ae  t te e e  ae e e  te e  t ae  ae  a a te e e  a te  te e e  ae e  te  te  a a te e e  a a te e  t a te e  a t ae  ae e  te e e  ae e e  ae e  a t a t a a te e  te  t a t t t te e  a t te e  a ae  te  a te  te  te e  te \n",
      "====================================================================\n",
      "\n",
      "(1).(8).(9).(0).(2).(5).(3).(7).(10).(6).\n",
      "Average loss at step 3: 5.003276\n",
      "\tPerplexity at step 3: 148.900228\n",
      "\n",
      "Valid Perplexity: 145.30\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  will d d the and and the ther as an and as and and th a a the to d th and a and and to th and to and and and anthe an a a to d th a and te the th and we and the anthe d d d d and the an a and a a and and we the and the d d d and th and and whe a and and a and and a and the the as an and te d d the th a a and and and as the the d th a a the and and and and the and to there the d and d there ou wer the the and were there th and we and the d d d ther as ther the to there an a and wer a and we and d and th a and the and and a and as the the and the and d the d anthe the the and and the and d d ther and we th and to th as the the and d the and ther the we the an and the the and the and we ther the to ther the a a the and to an a and and a a and to an and wer a a and as d ther and whe we d and th as anthe the there our the and the and th and and whe a and we and the ther a and the te and the and there and th as anther and the and we the and an a and were our and and the the ther to and the th\n",
      "====================================================================\n",
      "\n",
      "(1).(5).(4).(3).(9).(7).(0).(2).(8).(10).\n",
      "Average loss at step 4: 4.530468\n",
      "\tPerplexity at step 4: 92.802000\n",
      "\n",
      "Valid Perplexity: 74.33\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "\t d to to our our our that wor and we the and and our the to and thas the the to we and and that the the wor and to the and and the and and the the the the and the to the and we ther the the and the the we and there our and our that the we that the ame the wore and the that and wor ame and wore our and the to and wore the to the the and wor and will the to ame and and the wor and we and ther are the wor the and and are and are the are wor and to our and our and our our our there the the the and the to and and we ther and we ther and and will our and that to our the the the that will ant the to to the the the and and and and ther the the ame and we our the the to wore that to the and and the the to we our and that will the and and and and the and ther to our the ther and the we our and the ant and the to wore the and that we that we that and the and to the and the to and ame the we and an and we the and and the and our our ther and we that the wild the the there that we and and our and that\n",
      "====================================================================\n",
      "\n",
      "(7).(0).(2).(4).(10).(6).(8).(9).(5).(1).\n",
      "Average loss at step 5: 4.053712\n",
      "\tPerplexity at step 5: 57.610938\n",
      "\n",
      "Valid Perplexity: 45.96\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "\t thation, and american the with that the will the hearts the the will to the will and of of and of our will the sent the this of our the of the hery and our of the his to the the the of the sour that that of to the of to the with the hise the will and and of the the with to the wire of the that on thation and our of that the of and of thation and that the strention of the the with the sour the this to will and and and and of that the hery the this to of the we the this our we of to thation thation the of the hearty and the the of our heace of of the heat of the hise of of of and and and of our will and the hearty the of and and and of our will and the we of american ther to the this of the that of of the we of and the sour the the will and that and of the wore wire to that of this are the wort and the the of of the the the withe of of that the sour thations that the that of thation the the withe will and our heat of our hise the will to the of to the the with that the that the sour the wi\n",
      "====================================================================\n",
      "\n",
      "(4).(6).(5).(10).(2).(9).(8).(1).(3).(7).\n",
      "Average loss at step 6: 3.730259\n",
      "\tPerplexity at step 6: 41.689896\n",
      "\n",
      "Valid Perplexity: 29.05\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      "\t sk to the thation the the will the stake and stry and the world thation, we will and ames that the hope of our seedom of and the heat bere the will this americans of the words of and strences the word our seedom of to se the we this stall american and thations americans, and the work the seedom and the work of our bestred the of of our the of our heach our hearts and of the hopes, and our that the we that ament amering that of are to that and the whose to thations and stry work our have work and the have of american the will and seedom the will of this the strent this that the of the stand to ber of ames the the world the of to this and and and seed of and the have with and and the strences and the work the will that the whose the we world and, that the world the world that that the world of that the of of to thave and american the will to se all our have the world of of our berestore that to the of the hearts of the will the work that the work to our bered the of of our of and and the w\n",
      "====================================================================\n",
      "\n",
      "(0).(1).(10).(2).(4).(8).(7).(6).(3).(5).\n",
      "Average loss at step 7: 3.518092\n",
      "\tPerplexity at step 7: 33.720045\n",
      "\n",
      "Valid Perplexity: 22.72\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ng american our thation and our the work and stry of and to that and the stake the stries and the world the will and americans, and the will the will the tory and that the the world that and stand and seedom.\n",
      "\n",
      "the words.\n",
      "\n",
      "and to and stry tory that that the come the to the could the stand to be are and the prom is we the work the stake and the stake to the work and to bere and and and stand americans ament and stall the strences the world the with the stakes that we that the we will and and the seed that all the will make the strengthen that the will the with the wore the will to be are the world and and that of the stakes and the tory that the will and the conters to the work and to se world and amerity our come that the stand our cone of our come this the cour the with and stake and the preations and the prought and to se work to aments and the tory the cought the work the cone of the seed of and and the stand of our this the wore this stake the comis and the seen to americans and that \n",
      "====================================================================\n",
      "\n",
      "(8).(1).(0).(10).(2).(6).(7).(3).(4).(9).\n",
      "Average loss at step 8: 3.364628\n",
      "\tPerplexity at step 8: 28.922739\n",
      "\n",
      "Valid Perplexity: 18.02\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      "\t but the will we must that the undents and the work america's to the world the unity and the unity, we will and to seUNKUNK the service, and the work of our country of the stake and the service, and to are and comething the will the world and to and and and chang to this the wort the word of our world this to that and to se we with our seed the world this will not that the we must and to the will and and the strends, the world the will we will and the stry of to been that the work our progrest that we much the will that we the work the cour countrence, and to that that the cand that the strength and the untry of and the world we this some and our counity and the unity and the will america's chat this with the unity of our seedom and this stall the world and common that we must american the world the will the consing to selfill mand the stall the we will the serving to that to serve the words the world an and the comple and the world and to but and chat the will that the with to the world to\n",
      "====================================================================\n",
      "\n",
      "(7).(2).(1).(4).(8).(6).(9).(5).(0).(10).\n",
      "Average loss at step 9: 3.201829\n",
      "\tPerplexity at step 9: 24.577447\n",
      "\n",
      "Valid Perplexity: 15.56\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "\t gue the words of our place america's we must we will we must to be my and the preclated of and we much with and we meach our strence, we heak of our people with all nation and that we with will berty with will be work the world of our promise will make and to begive that that and will make the people of our most the words of the people and the world of the strengthen and the world of the progress of the we must and the powerica chill may and that the prouder, we will make that the world of our promilitiment the strength and the will america we will will now the world of the people of our most and the world of the people and we have the world the problems and all nation of our meach to make all nation of our meach the worl make all to mall to the will be with the progress the work our must of and the will not the problession and our nation, and we hope of the people will my will with of and the strength and the world the words.\n",
      "\n",
      "that and will make to the probless on a permate and the worl\n",
      "====================================================================\n",
      "\n",
      "(3).(0).(8).(1).(9).(4).(6).(5).(7).(10).\n",
      "Average loss at step 10: 3.097859\n",
      "\tPerplexity at step 10: 22.150471\n",
      "\n",
      "Valid Perplexity: 13.78\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      "\t e they american our chare the words of our people and will because the world our couldrents of our greatess and the words with god's god because will now we america we will all america will mand the will now that will not the country. and to serve the will be that the story and freedom the stand american our counity, we will american that the strong the will the will the world of our promise our promise and for the world and the story of our liver the sacrife and our nation, and we will make america we will because and for the stand the people they with and the same, we work of america's freedom america's greation, american and in american and our community, that berty the community and the words and our countants the will america's history america's greations, and that the world we will and the strengthen the words of our progres, and the words of our generation and the world of the progred, we will not berty and division, and the countrents and the stall and the stake to the saments of\n",
      "====================================================================\n",
      "\n",
      "(10).(5).(0).(8).(7).(2).(4).(1).(6).(3).\n",
      "Average loss at step 11: 3.023614\n",
      "\tPerplexity at step 11: 20.565480\n",
      "\n",
      "Valid Perplexity: 12.21\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "\t e the values and and stall. the world to the cance and stated the work of our community, and to the community and the unity, the country, and to to the unity and our country of and this and this stand american and the unity and the sames to seek a commities and the work the storm the will make the conving and stalled the cances of to the unity, are to make american the world to this and the world to belUNKUNK and and the work of the couldry. we here to be wime as this story, and the world of and this and our nation, we milt that to the world that all the convicts of this and the sames and our compley and that we will americans of our commits of our stand that is and strength and that we will and and the stand the canger the world of the storm the strong to the country and and all by with american and the same and stand and america's gooUNK\n",
      "\n",
      "the world to believe and strong of our stand all to be are not the strength and to servery and and and and all the stand all the cance and this of the s\n",
      "====================================================================\n",
      "\n",
      "(6).(1).(7).(9).(5).(4).(2).(0).(10).(3).\n",
      "Average loss at step 12: 2.925519\n",
      "\tPerplexity at step 12: 18.643901\n",
      "\n",
      "Valid Perplexity: 10.62\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  remocracy. and they america. we will make the proud to the unity, we must but and the world. we will make america we will the form the work and we will be to the will make american and the says and will make and thas not be will be my ace of the progreat that we had the says, and the sament and we must and america with and the untry to and the strongth the world. with and the unity, and the proud again the people and the world the world to thank and we have work of our people and that we will american of the world america with and we will make and the people. and and all all. we all be the world of the samernment of the will make all of our country and with and the stand america seek to that is thank you and we must america. we will make the world and to the world of and the strives. we are not the people.\n",
      "\n",
      "we will make that is the stand the weach the strength an to make america stand to be my fere and and will make america wite and we will not us we will to the says the same world of t\n",
      "====================================================================\n",
      "\n",
      "(3).(4).(8).(1).(0).(6).(9).(2).(7).(10).\n",
      "Average loss at step 13: 2.879544\n",
      "\tPerplexity at step 13: 17.806147\n",
      "\n",
      "Valid Perplexity: 11.24\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t great for our live again. we will make to the under country. with our believe and the world the words of the strong and the worlUNKs and the words of alled to the world to because we must all by and we will make the freedom to serve the unity the world and the words to be the called and the strength and the unity of our canizess of our couldry and the power america's great and the same of our greatess the call of the unities the unity and our bertUNKe and we will make america we much to the work and the sould. the words: the progress the called to the unity and the untrue to security and sound america weal make america with the cannot and the freedom of the unity and the same, we will not be and all the strong, human declaraged the words to belies and we are we must day to selfill the untrent of americans on a never best for the world by the world of the world be done. the called america will make alled by the same, and the world of one america's greatest and the world of to selfe the work\n",
      "====================================================================\n",
      "\n",
      "(3).(1).(9).(4).(0).(10).(2).(5).(8).(6).\n",
      "Average loss at step 14: 2.822293\n",
      "\tPerplexity at step 14: 16.815368\n",
      "\n",
      "Valid Perplexity: 10.76\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "\t sing the progress the work of our commitments the world to the strongther a change and soment the world of our country, and our country with the strong and the work to this the contion's grace to the unities. we have with and our country, and that we will be new strength and ought of our nations on our compling and the proud and the cantunity of our nation's service that that is the world of our sound the people.\n",
      "\n",
      "we will not the world of complete to be the worker, we will make our nation of our sound to be strong, and the same gle world of the consity. and the same, can this comple of this of the continue.\n",
      "\n",
      "we have will believe the country, and our besting to strent, and we must to the world, and world and to make that we will not the fair of our country to the presions of the promise to be seek to be done. we will make american the world with god's history. and the saments that it is spirit to the presion, and the same and spirit of common danger, and that is a new bridge, that is not \n",
      "====================================================================\n",
      "\n",
      "(9).(10).(7).(4).(5).(1).(8).(6).(2).(3).\n",
      "Average loss at step 15: 2.789022\n",
      "\tPerplexity at step 15: 16.265104\n",
      "\n",
      "Valid Perplexity: 10.68\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      "\t all to a story and story, we have that and thas not be stated and our nation's country, but the some and strong, and the people.\n",
      "\n",
      "the unity of the progress the world. we must america's help to sere to be the unity, and to make our country. they will may he consibility, and that in our nation's hopeful is the world of americans are not the people.\n",
      "\n",
      "thank a require of our couldn't is that we all our country, and the unity, who we with all that we have the undestory of the strength this day, we have the fair of the world of americans of this day, and that america's great the people of the strong, never and the strength and that and strong. and states of our strong.\n",
      "\n",
      "we will may he heach in the world.\n",
      "\n",
      "we are ameetUNKith and our common of this day, and the work and the states to all be to are is the world with america's grace the couldn't we will make of that will make americans of the under today, who work and that it the peach of one america's strong, as the world of our human spirity. the \n",
      "====================================================================\n",
      "\n",
      "(1).(0).(4).(3).(7).(8).(6).(10).(9).(5).\n",
      "Average loss at step 16: 2.702843\n",
      "\tPerplexity at step 16: 14.922093\n",
      "\n",
      "Valid Perplexity: 10.21\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ely to to the values, and the same the storm that we rese the same the progress the world of our complets will the people and divisions and the values, we will make american the can again.\n",
      "\n",
      "we will make and the story. they will not for the work of our common, we will may that and for the strength and the deepest the same, call be will be the call of our nation's grand to the words of our proud the promise to securican the world and our nation, and destrule or this the storm.\n",
      "UNKay that we are not believe to the words: yea, the words: you, and the work and we will be the words.\n",
      "\n",
      "and our country with the promise of from the people.\n",
      "\n",
      "we are all the storm.\n",
      "UNKod bless you, and we will make the proud again to make of the world with ament to the say in our courage to life our places of time and difference and fried to make our places that and form is to this work to the strong of our service of this the freedom, and the say the will rese the sacrifice with america's lone of our people will be wi\n",
      "====================================================================\n",
      "\n",
      "(4).(3).(2).(7).(0).(10).(1).(6).(8).(5).\n",
      "Average loss at step 17: 2.661558\n",
      "\tPerplexity at step 17: 14.318578\n",
      "\n",
      "Valid Perplexity: 10.22\n",
      "\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      "\t sk and something the strongth and the fair our conviction and destrong our courage to live in the world, thank you and american peopeful in the fair of the back to the world of the comple of our country, and the world of the same great the world and the fair of the fair of the world and the world with and spirit of the strongth of the strengthen our commitments the work, which the work the people of our nations that we will not on the same glorious of our country will be the world, the strength and our children, a new brides, they american and the world of the strengther a requal the canturness of our nation of the strong, here and the sacrifice will the couldry, the world.\n",
      "\n",
      "we all the stall. and our changes the work, who with the stand the strongth and our country will that we have the words of the couration, when the progress the same never the storm.\n",
      "UNKod's grand and spirit of complete the world and the world and the world, we will be not be stated the world with american people. we w\n",
      "====================================================================\n",
      "\n",
      "(0).(5).(3).(2).(6).(8).(1).(9).(7).(4).\n",
      "Average loss at step 18: 2.630759\n",
      "\tPerplexity at step 18: 13.884309\n",
      "\n",
      "Valid Perplexity: 11.07\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      "\t spect of freedom is the cannects and we will be does, we have common the strong, have the world and democracy. and all the same, be now,UNKUNK and american the world of the story and to serve to believe in the world of our history as we will not be america's grand americans and the friend of the stately in the call.\n",
      "\n",
      "these whose in the world of the price and something to the world of the strength and the words of commons, the world with all, we have belieful world our common danger, and we will make all, in our complete the unity and the world of citizens of the people, and to the words of the strengthen the call of freedom the unity, with the called the people of our history. and the unities and our believe in our complete can people of ourselves. we must the fore of our commons of the world of our sound that the unity and the world and democracy. we can fellow americans of freedom, that we reflect the progress.\n",
      "\n",
      "from the world. we will be a conschose world be depending to service to the \n",
      "====================================================================\n",
      "\n",
      "(8).(9).(1).(0).(5).(2).(10).(6).(4).(7).\n",
      "Average loss at step 19: 2.561195\n",
      "\tPerplexity at step 19: 12.951290\n",
      "\n",
      "Valid Perplexity: 10.82\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n",
      "\t tion to make our nations of our nation we must generous and directs the world in the story and story americans, we will make americans of the world and the world and the same, can fellow america's considence and that the story. we have complishmerica's brise we will be we will not be stand the people and that we had ways and that we refore the call of our own way and direction and our chartimes of our history and stand to service, with the world.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have continuing to this can spirit of our congress of our continue this spirit of citizens, we must time of our society, that the standing today, we must the wealthy of our nation's helUNK we will be done. we will never the words of compassion and that that is the changence if our own was of americans are not seek to all our history. we have here the cannot the world with good, yes, and our best americans, not be my ferene. we will never the eneres, we will make all, and our nates this stand that is the stately in the americUNKs of all, and th\n",
      "====================================================================\n",
      "\n",
      "(0).(10).(5).(1).(4).(3).(6).(7).(2).(9).\n",
      "Average loss at step 20: 2.534235\n",
      "\tPerplexity at step 20: 12.606786\n",
      "\n",
      "Valid Perplexity: 10.52\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      "\t t that we can fellow congrest of the world and with the cause of the stand and the world. and that will be stated by common danger. and they ready to the world the unities the call of our nation, we will make americans, we have heart the under the world and the words of compage the people.\n",
      "\n",
      "our nation we will ride the cause of freedom, we had ensured the strength and division, and the unities of the human desident of the promise of the call of this say on a pledice and the unities of the strengthen our nation our courage of our nation who are is the world, and the world and to serve to the strughts.\n",
      "\n",
      "the unity and the unities that we have by the words of the stand again.\n",
      "UNKod bless you, and our nation, and the unity and the unions the people and the unity, the people of all the world of the call of the unity, with the world, we will be not just and we cannot be the world of congress of our society, and that in that that is the world. we wave the words: your country will remember this con\n",
      "====================================================================\n",
      "\n",
      "(4).(8).(0).(7).(1).(9).(10).(3).(5).(6).\n",
      "Average loss at step 21: 2.493820\n",
      "\tPerplexity at step 21: 12.107439\n",
      "\n",
      "Valid Perplexity: 9.80\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  must be not this strong of our country and the great nations that we will by the same aUNKeace and the same dreams, and the great of the strong, hopes, and the world will believe in the world of the strives, today, we will be this storm.\n",
      "UNKod bless you, and a new bridge, and our children's country as and the world of the stately strong, herand our country and the world will the world. the same as the sacrifice, we will always of american people. we will no are and the freedom, and the world, they are not be make the world.\n",
      "\n",
      "americans of our compassion and our children, a new centuries. the states of citizens, the progress and divingth of the words to those seek to an this spirit is the proud to the unity, we much to the words with americans are in a new bridge, and that this story and, yes, and the same, can fere and change today that it a responsibirty and our belief in our nation, and our country will the world. the world and the same bight of our nation's grander, and that will make o\n",
      "====================================================================\n",
      "\n",
      "(10).(2).(5).(3).(1).(9).(7).(6).(0).(8).\n",
      "Average loss at step 22: 2.492174\n",
      "\tPerplexity at step 22: 12.087528\n",
      "\n",
      "Valid Perplexity: 10.35\n",
      "\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ed be preserve in the world with this storld by the words of the people to all our nation of that we had ensther, and our belief our not on a service that the people of the people and our belief of the world, and we must america's bright of the promise of this storm.\n",
      "UNKod bless you, and the bandon, we must the people, we must the worlUNK\n",
      "\n",
      "from the power their people and the world of the wealth, so that we cannot we will the work for that we had that the strength and our common the powerful make of this spirit to be not be my accomplete the power on this declared and the world and our believery and the world, the powerful responsibility is a new century with the stately storm.\n",
      "UNKhat we will not be see to that we will stand that the world with the world. and the world and democracy and our best prograted to the people of our commitment, but the words of our nation's befor in our sound. we are the world be depending the world with the weach of this stormUNK\n",
      "\n",
      "we have not be stand that this wor\n",
      "====================================================================\n",
      "\n",
      "(7).(3).(1).(5).(9).(8).(6).(4).(2).(10).\n",
      "Average loss at step 23: 2.475369\n",
      "\tPerplexity at step 23: 11.886092\n",
      "\n",
      "Valid Perplexity: 10.33\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  vall of our sound. and we have changed the union this stands of our sound. we will make the promise of the world with americans, we will make america who are is the unities. we must the stalled by the words: the preserve to service to the words of the stands of all of this spirit of the human dignity and pride our country, and we have chill make america wealth to all, we are america's bright of all of our country to service to begin in the hear of freedom, and the world with america will be not on the world.\n",
      "\n",
      "we will make the powerfully and destrong all the stand again. we will be not only responsibility, and they will make that we have work and story answer the call of oursonver the world with the weary. the world and destroy. and americans, not be always and, yet freedom.\n",
      "UNKay that we are the people and the world with the stand to the world, the faith and the story goes the americUNK and so, today and will make our hopes, the world and direction. and we will age of the world and that w\n",
      "====================================================================\n",
      "\n",
      "(8).(10).(4).(6).(7).(1).(2).(3).(5).(9).\n",
      "Average loss at step 24: 2.434409\n",
      "\tPerplexity at step 24: 11.409070\n",
      "\n",
      "Valid Perplexity: 10.23\n",
      "\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      "\t century, lives all to serve the world of the power of our country to a seek yoUNK god bless you, and the work of the cannot on a place of the progress this story. and our communities of the strong. so make america's greater of this say of our country with this is this day, he today are the problems, and the world will all of us of this day that the world.\n",
      "\n",
      "we are all our courage, the world.\n",
      "\n",
      "from the foreir for the world.\n",
      "\n",
      "the world of the story as a place and, to be new american dream and directs and the same aUNKeal that we cannot be done and who are to all of the words and strong, here the cannot be done. we will make american dream that is this work of the conviction.\n",
      "\n",
      "the world the work our strength and our problems with the world and the same, UNKe been there to make the progress and the world of the world's grace the heart as a party and that we can fere the future to the human destrong.\n",
      "\n",
      "to somether a nation, and a secults, the fortunity, and a children, with the states of the story\n",
      "====================================================================\n",
      "\n",
      "(2).(5).(0).(9).(6).(1).(8).(10).(3).(4).\n",
      "Average loss at step 25: 2.406072\n",
      "\tPerplexity at step 25: 11.090312\n",
      "\n",
      "Valid Perplexity: 10.26\n",
      "\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n",
      "\t , the world and the world, to make america spended to be see and endured, and they are to the world the sacrifice with the world all the same of one americans, the sacrifice. in the words of our country, and the same, with god's struggled the same great nation, and the world will be we will make america strengthen our country. we have american people, the world will alls and the says, UNKest and the same bignity. the enemies, the world. and the same aUNKear of those who are trule to make to make americans, and the unities.\n",
      "\n",
      "this we called there all of this work until our children, will strength and we are infidence. we will not be always and the same, called in the world will be the world with god will make american dream thank yoUNK god bless you all, and your demembers. and the same aUNKUNKUNKUNK americans, the world of our hopes, and you are the world will always, americans, we are all, and will be the words will make the world.\n",
      "\n",
      "the world and democracy and the work, we will make america str\n",
      "====================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4).(8).(9).(0).(10).(3).(2).(1).(7).(5).\n",
      "Average loss at step 26: 2.404949\n",
      "\tPerplexity at step 26: 11.077867\n",
      "\n",
      "Valid Perplexity: 10.31\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      "\t nsted the world, and the world of that we had ward the world. and the same and the freedom.\n",
      "\n",
      "and that is the forld of the world of the same of the human future with the strengthen ourselves to be problems, the called in the banderstrong that we will make america's longer the sacrifice, the powerful in our sounded by our children, will the same glorious and the world and americans, we have chance in the world, the foreign and to the under the world of the world. the enemies this day, we will make the future of our history. and the unity, we will be the world with action of the strong, here to the heart on a place of american president is the same drams. americans, the world of our compassion this day, we will make american promise that we will ready for the foreir journey of the world. we will make americans, the end of the hard. things of the heart of this say to be always the call.\n",
      "\n",
      "they and the same, cans our societyUNKUNKhat it is a new bring back our heroes of our strength and the same\n",
      "====================================================================\n",
      "\n",
      "<_io.TextIOWrapper name='gru_dropout.csv' mode='wt' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 10\n",
    "docs_per_step = 10\n",
    "valid_summary = 1\n",
    "train_doc_count = num_files\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Capture the behavior of train/valid perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            \n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity\n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity\n",
    "        \n",
    "        # shows the training progress\n",
    "        print('(%d).'%di,end='') \n",
    "        \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "        session.run(reset_train_state) # resetting hidden state for each document\n",
    "        \n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses  \n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "\n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        test_word[0,data_list[np.random.randint(0,num_files)][np.random.randint(0,100)]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})            \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write the perplexity data to a CSV\n",
    "\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    print(f)\n",
    "    writer = csv.writer(f,delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
