{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with LSTMs using Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\software\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "# I have separated word vector learning algorithm to\n",
    "# separate file as we have already gone through the details\n",
    "# We will be only focusing on the language generation part\n",
    "import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure president stories are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  ../data\\speech_1.txt\n",
      "File  speech_1.txt  already exists.\n",
      "Downloading file:  ../data\\speech_2.txt\n",
      "File  speech_2.txt  already exists.\n",
      "Downloading file:  ../data\\speech_3.txt\n",
      "File  speech_3.txt  already exists.\n",
      "Downloading file:  ../data\\speech_4.txt\n",
      "File  speech_4.txt  already exists.\n",
      "Downloading file:  ../data\\speech_5.txt\n",
      "File  speech_5.txt  already exists.\n",
      "Downloading file:  ../data\\speech_6.txt\n",
      "File  speech_6.txt  already exists.\n",
      "Downloading file:  ../data\\speech_7.txt\n",
      "File  speech_7.txt  already exists.\n",
      "Downloading file:  ../data\\speech_8.txt\n",
      "File  speech_8.txt  already exists.\n",
      "Downloading file:  ../data\\speech_9.txt\n",
      "File  speech_9.txt  already exists.\n",
      "Downloading file:  ../data\\speech_10.txt\n",
      "File  speech_10.txt  already exists.\n",
      "Downloading file:  ../data\\speech_11.txt\n",
      "File  speech_11.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory if needed\n",
    "dir_name = \"../data\"\n",
    "num_files = 11\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  #Download a file if not present\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "\n",
    "filenames = [\"speech_\"+format(i, '01d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print('%d files found.'%len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file ../data\\speech_1.txt\n",
      "Data size (Characters) (Document 0) 1375\n",
      "Sample string (Document 0) ['for', 'myself', 'and', 'for', 'our', 'nation', ',', 'i', 'want', 'to', 'thank', 'my', 'predecessor', 'for', 'all', 'he', 'has', 'done', 'to', 'heal', 'our', 'land', '.', 'in', 'this', 'outward', 'and', 'physical', 'ceremony', ',', 'we', 'attest', 'once', 'again', 'to', 'the', 'inner', 'and', 'spiritual', 'strength', 'of', 'our', 'nation', '.', 'as', 'my', 'high', 'school', 'teacher', ',']\n",
      "\n",
      "Processing file ../data\\speech_2.txt\n",
      "Data size (Characters) (Document 1) 2791\n",
      "Sample string (Document 1) ['senator', 'hatfield', ',', 'mr.', 'chief', 'justice', ',', 'mr.', 'president', ',', 'vice', 'president', 'bush', ',', 'vice', 'president', 'mondale', ',', 'senator', 'baker', ',', 'speaker', \"o'neill\", ',', 'reverend', 'moomaw', ',', 'and', 'my', 'fellow', 'citizens', ':', 'to', 'a', 'few', 'of', 'us', 'here', 'today', 'this', 'is', 'a', 'solemn', 'and', 'most', 'momentous', 'occasion', ',', 'and', 'yet']\n",
      "\n",
      "Processing file ../data\\speech_3.txt\n",
      "Data size (Characters) (Document 2) 2941\n",
      "Sample string (Document 2) ['senator', 'mathias', ',', 'chief', 'justice', 'burger', ',', 'vice', 'president', 'bush', ',', 'speaker', \"o'neill\", ',', 'senator', 'dole', ',', 'reverend', 'clergy', ',', 'and', 'members', 'of', 'my', 'family', 'and', 'friends', 'and', 'my', 'fellow', 'citizens', ':', 'this', 'day', 'has', 'been', 'made', 'brighter', 'with', 'the', 'presence', 'here', 'of', 'one', 'who', ',', 'for', 'a', 'time', ',']\n",
      "\n",
      "Processing file ../data\\speech_4.txt\n",
      "Data size (Characters) (Document 3) 2682\n",
      "Sample string (Document 3) ['mr.', 'chief', 'justice', ',', 'mr.', 'president', ',', 'vice', 'president', 'quayle', ',', 'senator', 'mitchell', ',', 'speaker', 'wright', ',', 'senator', 'dole', ',', 'congressman', 'michel', ',', 'and', 'fellow', 'citizens', ',', 'neighbors', ',', 'and', 'friends', ':', 'there', 'is', 'a', 'man', 'here', 'who', 'has', 'earned', 'a', 'lasting', 'place', 'in', 'our', 'hearts', 'and', 'in', 'our', 'history']\n",
      "\n",
      "Processing file ../data\\speech_5.txt\n",
      "Data size (Characters) (Document 4) 1828\n",
      "Sample string (Document 4) ['my', 'fellow', 'citizens', ',', 'today', 'we', 'celebrate', 'the', 'mystery', 'of', 'american', 'renewal', '.', 'this', 'ceremony', 'is', 'held', 'in', 'the', 'depth', 'of', 'winter', ',', 'but', 'by', 'the', 'words', 'we', 'speak', 'and', 'the', 'faces', 'we', 'show', 'the', 'world', ',', 'we', 'force', 'the', 'spring', ',', 'a', 'spring', 'reborn', 'in', 'the', 'world', \"'s\", 'oldest']\n",
      "\n",
      "Processing file ../data\\speech_6.txt\n",
      "Data size (Characters) (Document 5) 2437\n",
      "Sample string (Document 5) ['my', 'fellow', 'citizens', ',', 'at', 'this', 'last', 'presidential', 'inauguration', 'of', 'the', '20th', 'century', ',', 'let', 'us', 'lift', 'our', 'eyes', 'toward', 'the', 'challenges', 'that', 'await', 'us', 'in', 'the', 'next', 'century', '.', 'it', 'is', 'our', 'great', 'good', 'fortune', 'that', 'time', 'and', 'chance', 'have', 'put', 'us', 'not', 'only', 'at', 'the', 'edge', 'of', 'a']\n",
      "\n",
      "Processing file ../data\\speech_7.txt\n",
      "Data size (Characters) (Document 6) 1819\n",
      "Sample string (Document 6) ['thank', 'you', ',', 'all', '.', 'chief', 'justice', 'rehnquist', ',', 'president', 'carter', ',', 'president', 'bush', ',', 'president', 'clinton', ',', 'distinguished', 'guests', ',', 'and', 'my', 'fellow', 'citizens', '.', 'the', 'peaceful', 'transfer', 'of', 'authority', 'is', 'rare', 'in', 'history', ',', 'yet', 'common', 'in', 'our', 'country', '.', 'with', 'a', 'simple', 'oath', ',', 'we', 'affirm', 'old']\n",
      "\n",
      "Processing file ../data\\speech_8.txt\n",
      "Data size (Characters) (Document 7) 2323\n",
      "Sample string (Document 7) ['vice', 'president', 'cheney', ',', 'mr.', 'chief', 'justice', ',', 'president', 'carter', ',', 'president', 'bush', ',', 'president', 'clinton', ',', 'members', 'of', 'the', 'united', 'states', 'congress', ',', 'reverend', 'clergy', ',', 'distinguished', 'guests', ',', 'fellow', 'citizens', ':', 'on', 'this', 'day', ',', 'prescribed', 'by', 'law', 'and', 'marked', 'by', 'ceremony', ',', 'we', 'celebrate', 'the', 'durable', 'wisdom']\n",
      "\n",
      "Processing file ../data\\speech_9.txt\n",
      "Data size (Characters) (Document 8) 2692\n",
      "Sample string (Document 8) ['my', 'fellow', 'citizens', ',', 'i', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', ',', 'grateful', 'for', 'the', 'trust', 'you', 'have', 'bestowed', ',', 'mindful', 'of', 'the', 'sacrifices', 'borne', 'by', 'our', 'ancestors', '.', 'i', 'thank', 'president', 'bush', 'for', 'his', 'service', 'to', 'our', 'nation', ',', 'as', 'well', 'as', 'the', 'generosity', 'and', 'cooperation']\n",
      "\n",
      "Processing file ../data\\speech_10.txt\n",
      "Data size (Characters) (Document 9) 2343\n",
      "Sample string (Document 9) ['thank', 'you', '.', 'thank', 'you', 'so', 'much', '.', 'vice', 'president', 'biden', ',', 'mr.', 'chief', 'justice', ',', 'members', 'of', 'the', 'united', 'states', 'congress', ',', 'distinguished', 'guests', ',', 'and', 'fellow', 'citizens', ':', 'each', 'time', 'we', 'gather', 'to', 'inaugurate', 'a', 'president', 'we', 'bear', 'witness', 'to', 'the', 'enduring', 'strength', 'of', 'our', 'constitution', '.', 'we']\n",
      "\n",
      "Processing file ../data\\speech_11.txt\n",
      "Data size (Characters) (Document 10) 1683\n",
      "Sample string (Document 10) ['chief', 'justice', 'roberts', ',', 'president', 'carter', ',', 'president', 'clinton', ',', 'president', 'bush', ',', 'president', 'obama', ',', 'fellow', 'americans', ',', 'and', 'people', 'of', 'the', 'world', ':', 'thank', 'you', '.', 'we', ',', 'the', 'citizens', 'of', 'america', ',', 'are', 'now', 'joined', 'in', 'a', 'great', 'national', 'effort', 'to', 'rebuild', 'our', 'country', 'and', 'restore', 'its']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = nltk.word_tokenize(data)\n",
    "    \n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Unlike in the previous instances we break the text in to words\n",
    "    # this time\n",
    "    words = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    documents.append(words)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries (Bigrams)\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24914 Words found.\n",
      "Most common words (+UNK) [(',', 1409), ('the', 1153), ('.', 1134), ('and', 984), ('of', 795)]\n",
      "Least common words (+UNK) [('disease', 1), ('energies', 1), ('stir', 1), ('brown', 1), ('bleed', 1), ('red', 1), ('urban', 1), ('sprawl', 1), ('windswept', 1), ('plains', 1), ('nebraska', 1), ('sky', 1), ('near', 1), ('ignored', 1), ('wealthy', 1)]\n",
      "Sample data [13, 0, 4, 13, 7, 36, 1, 31, 0, 6]\n",
      "Sample data [0, 0, 1, 264, 0, 158, 1, 264, 79, 1]\n",
      "Vocabulary:  279\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Words found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the word sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each word by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW: Learning Word Vectors\n",
    "\n",
    "In this section we learn word vectors using CBOW algorithm. This process can take a long time to run (~30 mins). Therefore, we have saved a version of final embeddings learnt by the algorithm. This will be loaded straight from the disk during text generation training. Therefore, you don't have to run this part. However, we have included this for the sake of completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with window_size = 1:\n",
      "    batch: [['for', 'and'], ['UNK', 'for'], ['and', 'our'], ['for', 'nation'], ['our', ','], ['nation', 'i'], [',', 'UNK'], ['i', 'to']]\n",
      "    labels: ['UNK', 'and', 'for', 'our', 'nation', ',', 'i', 'UNK']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: [['my', 'UNK', 'all', 'he'], ['UNK', 'for', 'he', 'has'], ['for', 'all', 'has', 'done'], ['all', 'he', 'done', 'to'], ['he', 'has', 'to', 'UNK'], ['has', 'done', 'UNK', 'our'], ['done', 'to', 'our', 'land'], ['to', 'UNK', 'land', '.']]\n",
      "    labels: ['for', 'all', 'he', 'has', 'done', 'to', 'UNK', 'our']\n",
      "Defining 6 embedding lookups representing each word in the context\n",
      "Stacked embedding size: [128, 128, 6]\n",
      "Reduced mean embedding size: [128, 128]\n",
      "Initialized\n",
      "Average loss at step 1: 3585.376476\n",
      "Average loss at step 2: 10.193522\n",
      "Average loss at step 3: 3.636279\n",
      "Average loss at step 4: 6.971015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajee\\Documents\\Capstone\\FinalProject\\201902_Data-Science-Capstone_DATS_6501_10-\\src\\word2vec.py:230: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  average_loss = average_loss / (doc_id*steps_per_doc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 5: inf\n",
      "Nearest to it: better, while, there, an,\n",
      "Nearest to 's: the, forward, of, has,\n",
      "Nearest to will: must, know, can, that,\n",
      "Nearest to has: is, called, against, on,\n",
      "Nearest to our: an, unity, all, its,\n",
      "Nearest to by: against, to, into, upon,\n",
      "Nearest to be: take, help, go, strength,\n",
      "Nearest to not: than, system, faith, young,\n",
      "Nearest to nation: day, country, UNK, economy,\n",
      "Nearest to :: ., in, ,, that,\n",
      "Nearest to for: across, under, did, of,\n",
      "Nearest to america: his, all, again, thank,\n",
      "Nearest to ,: as, which, hand, have,\n",
      "Nearest to the: public, a, nations, economic,\n",
      "Nearest to i: we, long, they, thank,\n",
      "Nearest to UNK: end, hope, system, century,\n",
      "Average loss at step 6: 3.272749\n",
      "Average loss at step 7: 2.831963\n",
      "Average loss at step 8: 2.491097\n",
      "Average loss at step 9: 2.441366\n",
      "Average loss at step 10: 4.001250\n",
      "Nearest to it: better, there, much, while,\n",
      "Nearest to 's: the, of, forward, this,\n",
      "Nearest to will: must, can, know, should,\n",
      "Nearest to has: is, on, against, called,\n",
      "Nearest to our: its, an, their, unity,\n",
      "Nearest to by: against, to, into, upon,\n",
      "Nearest to be: go, help, take, strength,\n",
      "Nearest to not: faith, best, than, as,\n",
      "Nearest to nation: day, economy, country, land,\n",
      "Nearest to :: ., in, ,, ;,\n",
      "Nearest to for: across, of, under, still,\n",
      "Nearest to america: his, remember, home, what,\n",
      "Nearest to ,: as, :, â€, .,\n",
      "Nearest to the: public, millions, nations, economic,\n",
      "Nearest to i: we, they, your, you,\n",
      "Nearest to UNK: end, century, system, power,\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "word2vec.define_data_and_hyperparameters(\n",
    "    num_files, data_list, reverse_dictionary, embedding_size, vocabulary_size)\n",
    "word2vec.print_some_batches()\n",
    "word2vec.define_word2vec_tensorflow()\n",
    "\n",
    "# We save the resulting embeddings as embeddings-tmp.npy \n",
    "# If you want to use this embedding for the following steps\n",
    "# please change the name to embeddings.npy and replace the existing\n",
    "word2vec.run_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the LSTM. More specifically the generator breaks a given sequence of words into `batch_size` segments. We also maintain a cursor for each segment. So whenever we create a batch of data, we sample one item from each segment and update the cursor of each segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\tUNK (0), \twe (8), \tthe (2), \tof (5), \tmy (58), \n",
      "\tOutput:\n",
      "\tand (4), \tUNK (0), \tUNK (0), \tour (7), \tUNK (0), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\tand (4), \tUNK (0), \tUNK (0), \tour (7), \tUNK (0), \n",
      "\tOutput:\n",
      "\tUNK (0), \tonce (194), \tand (4), \tnation (36), \tUNK (0), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tUNK (0), \tonce (194), \tand (4), \tnation (36), \tUNK (0), \n",
      "\tOutput:\n",
      "\tUNK (0), \tagain (94), \tUNK (0), \t. (3), \tUNK (0), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\tUNK (0), \tagain (94), \tUNK (0), \t. (3), \tUNK (0), \n",
      "\tOutput:\n",
      "\t, (1), \tto (6), \tstrength (106), \tas (35), \t, (1), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\t, (1), \tto (6), \tstrength (106), \tas (35), \tmy (58), \n",
      "\tOutput:\n",
      "\twe (8), \tthe (2), \tof (5), \tmy (58), \tUNK (0), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorSeq(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size, vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b] = self._text[self._cursor[b]]\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorSeq(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM\n",
    "\n",
    "This is a standard LSTM. The LSTM has 5 main components.\n",
    "* Cell state\n",
    "* Hidden state\n",
    "* Input gate\n",
    "* Forget gate\n",
    "* Output gate\n",
    "\n",
    "Each gate has three sets of weights (1 set for the current input, 1 set for the previous hidden state and 1 bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "\n",
    "Here we define several hyperparameters and are very similar to the ones we defined in Chapter 6. However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neurons in the hidden state variables\n",
    "num_nodes = 128\n",
    "\n",
    "# Number of data points in a batch we process\n",
    "batch_size = 64\n",
    "\n",
    "# Number of time steps we unroll for during optimization\n",
    "num_unrollings = 50\n",
    "\n",
    "dropout = 0.2 # We use dropout\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "    \n",
    "filename_to_save = 'lstm_word2vec'+filename_extension + '.csv' # use to save perplexity values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test inputs (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "# Validation data placeholders\n",
    "valid_inputs = tf.placeholder(tf.int32, shape=[1],name='valid_inputs')\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1,vocabulary_size], name = 'valid_labels')\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.placeholder(tf.int32, shape=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Word Embeddings to TensorFlow\n",
    "We load the previously learned and stored embeddings to TensorFlow and define tensors to hold embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to change the embedding matrix to something you newly generated,\n",
    "## Simply change embeddings.npy to embeddings-tmp.npy\n",
    "embed_mat = np.load('embeddings.npy')\n",
    "embed_init = tf.constant(embed_mat)\n",
    "embeddings = tf.Variable(embed_init,name='embeddings')\n",
    "embedding_size = embed_mat.shape[1]\n",
    "# Defining embedding lookup operations for all the unrolled\n",
    "# trianing inputs\n",
    "train_inputs_embeds = []\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs_embeds.append(tf.nn.embedding_lookup(embeddings,train_inputs[ui]))\n",
    "\n",
    "# Defining embedding lookup for operations for all the validation data\n",
    "valid_inputs_embeds = tf.nn.embedding_lookup(embeddings,valid_inputs)\n",
    "\n",
    "# Defining embedding lookup for operations for all the testing data\n",
    "test_input_embeds = tf.nn.embedding_lookup(embeddings, test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters and Cell Computation\n",
    "\n",
    "Now we define model parameters. Compared to RNNs, LSTMs have a large number of parameters. Each gate (input, forget, memory and output) has three different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "# Same variables for validation phase\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM Computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'lstm_word2vec_beamsearch'\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "\n",
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "for i in train_inputs_embeds:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "valid_output, valid_state = lstm_cell(\n",
    "    valid_inputs_embeds, saved_valid_output, saved_valid_state)\n",
    "\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_valid_output.assign(valid_output),\n",
    "                            saved_valid_state.assign(valid_state)]):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "test_output, test_state = lstm_cell(\n",
    "test_input_embeds, saved_test_output, saved_test_state)\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]):\n",
    "    test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "We calculate the training loss of the LSTM here. It's a typical cross entropy loss calculated over all the scores we obtained for training data (`loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies([saved_output.assign(output),\n",
    "                            saved_state.assign(state)]):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using the Adam optimizer as it is one of the best optimizers out there. Furthermore we use gradient clipping to prevent any gradient explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for decaying learning rate\n",
    "gstep = tf.Variable(0, trainable=False)\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset train state\n",
    "reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "# Reset valid state\n",
    "reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "\n",
    "# Reset test state\n",
    "reset_test_state = tf.group(\n",
    "    saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.05)),\n",
    "    saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.05)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Beam-Search\n",
    "\n",
    "Here we alter the previously defined prediction related TensorFlow operations to employ beam-search. Beam search is a way of predicting several time steps ahead. Concretely instead of predicting the best prediction we have at a given time step, we get predictions for several time steps and get the sequence of highest joint probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_length = 20\n",
    "beam_neighbors = 5\n",
    "\n",
    "# We redefine the sample generation with beam search\n",
    "sample_beam_inputs = [tf.placeholder(tf.int32, shape=[1]) for _ in range(beam_neighbors)]\n",
    "sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "\n",
    "# Embedding lookups for each beam\n",
    "sample_beam_inputs_embeds = [tf.nn.embedding_lookup(embeddings, inp) for inp in sample_beam_inputs]\n",
    "sample_input_embeds = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "\n",
    "best_beam_index = tf.placeholder(shape=None, dtype=tf.int32)\n",
    "best_neighbor_beam_indices = tf.placeholder(shape=[beam_neighbors], dtype=tf.int32)\n",
    "\n",
    "# Maintains output of each beam\n",
    "saved_sample_beam_output = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "# Maintains the state of each beam\n",
    "saved_sample_beam_state = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "\n",
    "# Resetting the sample beam states (should be done at the beginning of each text snippet generation)\n",
    "reset_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We stack them to perform gather operation below\n",
    "stacked_beam_outputs = tf.stack(saved_sample_beam_output)\n",
    "stacked_beam_states = tf.stack(saved_sample_beam_state)\n",
    "\n",
    "# The beam states for each beam (there are beam_neighbor-many beams) needs to be updated at every depth of tree\n",
    "# Consider an example where you have 3 classes where we get the best two neighbors (marked with star)\n",
    "#     a`      b*       c  \n",
    "#   / | \\   / | \\    / | \\\n",
    "#  a  b c  a* b` c  a  b  c\n",
    "# Since both the candidates from level 2 comes from the parent b\n",
    "# We need to update both states/outputs from saved_sample_beam_state/output to have index 1 (corresponding to parent b)\n",
    "update_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.gather_nd(stacked_beam_outputs,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.gather_nd(stacked_beam_states,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We calculate lstm_cell state and output for each beam\n",
    "sample_beam_outputs, sample_beam_states = [],[] \n",
    "for vi in range(beam_neighbors):\n",
    "    tmp_output, tmp_state = lstm_cell(\n",
    "        sample_beam_inputs_embeds[vi], saved_sample_beam_output[vi], saved_sample_beam_state[vi]\n",
    "    )\n",
    "    sample_beam_outputs.append(tmp_output)\n",
    "    sample_beam_states.append(tmp_state)\n",
    "\n",
    "# For a given set of beams, outputs a list of prediction vectors of size beam_neighbors\n",
    "# each beam having the predictions for full vocabulary\n",
    "sample_beam_predictions = []\n",
    "for vi in range(beam_neighbors):\n",
    "    with tf.control_dependencies([saved_sample_beam_output[vi].assign(sample_beam_outputs[vi]),\n",
    "                                saved_sample_beam_state[vi].assign(sample_beam_states[vi])]):\n",
    "        sample_beam_predictions.append(tf.nn.softmax(tf.nn.xw_plus_b(sample_beam_outputs[vi], w, b)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM with Beam Search to Generate Text\n",
    "\n",
    "Here we train the LSTM on the available data and generate text using the trained LSTM for several steps. From each document we extract text for `steps_per_document` steps to train the LSTM on. We also report the train perplexity at the end of each step. Finally we test the LSTM by asking it to generate some new text with beam search starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Beam Prediction Logic\n",
    "Here we define function that takes in the session as an argument and output a beam of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = None\n",
    "\n",
    "def get_beam_prediction(session):\n",
    "    '''\n",
    "    Outputs a single beam of predictions of a specified length\n",
    "    '''\n",
    "    \n",
    "    # Generating words within a segment with Beam Search\n",
    "    # To make some calculations clearer, we use the example as follows\n",
    "    # We have three classes with beam_neighbors=2 (best candidate denoted by *, second best candidate denoted by `)\n",
    "    # For simplicity we assume best candidate always have probability of 0.5 in output prediction\n",
    "    # second best has 0.2 output prediction\n",
    "    #           a`                   b*                   c                <--- root level\n",
    "    #    /     |     \\         /     |     \\        /     |     \\   \n",
    "    #   a      b      c       a*     b`     c      a      b      c         <--- depth 1\n",
    "    # / | \\  / | \\  / | \\   / | \\  / | \\  / | \\  / | \\  / | \\  / | \\\n",
    "    # a b c  a b c  a b c   a*b c  a`b c  a b c  a b c  a b c  a b c       <--- depth 2\n",
    "    # So the best beams at depth 2 would be\n",
    "    # b-a-a and b-b-a\n",
    "    \n",
    "    global test_word\n",
    "    global sample_beam_predictions, update_sample_beam_state\n",
    "    \n",
    "    # Calculate the candidates at the root level\n",
    "    feed_dict = {}\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        feed_dict.update({sample_beam_inputs[b_n_i]: [test_word]})\n",
    "\n",
    "    # We calculate sample predictions for all neighbors with the same starting word/character\n",
    "    # This is important to update the state for all instances of beam search\n",
    "    sample_preds_root = session.run(sample_beam_predictions, feed_dict = feed_dict)  \n",
    "    sample_preds_root = sample_preds_root[0]\n",
    "\n",
    "    # indices of top-k candidates\n",
    "    # b and a in our example (root level)\n",
    "    this_level_candidates =  (np.argsort(sample_preds_root,axis=1).ravel()[::-1])[:beam_neighbors].tolist() \n",
    "    \n",
    "    # probabilities of top-k candidates\n",
    "    # 0.5 and 0.2\n",
    "    this_level_probs = sample_preds_root[0,this_level_candidates] #probabilities of top-k candidates\n",
    "\n",
    "    # Update test sequence produced by each beam from the root level calculation\n",
    "    # Test sequence looks like for our example (at root)\n",
    "    # [b,a]\n",
    "    test_sequences = ['' for _ in range(beam_neighbors)]\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]] + ' '\n",
    "\n",
    "    # Make the calculations for the rest of the depth of the beam search tree\n",
    "    for b_i in range(beam_length-1):\n",
    "        \n",
    "        test_words = [] # candidate words for each beam\n",
    "        pred_words = [] # Predicted words of each beam\n",
    "        \n",
    "        # computing feed_dict for the beam search (except root)\n",
    "        # feed dict should contain the best words/chars/bigrams found by the previous level of search\n",
    "\n",
    "        # For level 1 in our example this would be\n",
    "        # sample_beam_inputs[0]: b, sample_beam_inputs[1]:a\n",
    "        feed_dict = {}\n",
    "        for p_idx, pred_i in enumerate(this_level_candidates):                    \n",
    "            # Updating the feed_dict for getting next predictions\n",
    "            test_words.append(this_level_candidates[p_idx])\n",
    "\n",
    "            feed_dict.update({sample_beam_inputs[p_idx]:[test_words[p_idx]]})\n",
    "\n",
    "        # Calculating predictions for all neighbors in beams\n",
    "        # This is a list of vectors where each vector is the prediction vector for a certain beam\n",
    "        # For level 1 in our example, the prediction values for \n",
    "        #      b             a  (previous beam search results)\n",
    "        # [a,  b,  c],  [a,  b,  c] (current level predictions) would be\n",
    "        # [0.1,0.1,0.1],[0.5,0.2,0]\n",
    "        sample_preds_all_neighbors = session.run(sample_beam_predictions, feed_dict=feed_dict)\n",
    "\n",
    "        # Create a single vector with \n",
    "        # Making our example [0.1,0.1,0.1,0.5,0.2,0] \n",
    "        sample_preds_all_neighbors_concat = np.concatenate(sample_preds_all_neighbors,axis=1)\n",
    "        \n",
    "        # normalize this_level_candidates to fall between [0,vocabulary_size]\n",
    "        # In this example this would be [0,1]\n",
    "        this_level_candidates = np.argsort(sample_preds_all_neighbors_concat.ravel())[::-1][:beam_neighbors]\n",
    "        \n",
    "        # In the example this would be [1,1]\n",
    "        parent_beam_indices = this_level_candidates//vocabulary_size\n",
    "\n",
    "        # normalize this_level_candidates to fall between [0,vocabulary_size]\n",
    "        this_level_candidates = (this_level_candidates%vocabulary_size).tolist()\n",
    "\n",
    "        # Here we update the final state of each beam to be\n",
    "        # the state that was at the index 1. Because for both the candidates at this level the parent is \n",
    "        # at index 1 (that is b from root level)\n",
    "        session.run(update_sample_beam_state, feed_dict={best_neighbor_beam_indices: parent_beam_indices})\n",
    "\n",
    "        # Here we update the joint probabilities of each beam and add the newly found candidates to the sequence\n",
    "        tmp_this_level_probs = np.asarray(this_level_probs) # This is currently [0.5,0.2]\n",
    "        tmp_test_sequences = list(test_sequences) # This is currently [b,a]\n",
    "\n",
    "        for b_n_i in range(beam_neighbors):\n",
    "            # We make the b_n_i element of this_level_probs to be the probability of parents\n",
    "            # In the example the parent indices are [1,1]\n",
    "            # So this_level_probs become [0.5,0.5]\n",
    "            this_level_probs[b_n_i] = tmp_this_level_probs[parent_beam_indices[b_n_i]]\n",
    "            \n",
    "            # Next we multipyle these by the probabilities of the best candidates from current level \n",
    "            # [0.5*0.5, 0.5*0.2] = [0.25,0.1]\n",
    "            this_level_probs[b_n_i] *= sample_preds_all_neighbors[parent_beam_indices[b_n_i]][0,this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Make the b_n_i element of test_sequences to be the correct parent of the current best candidates\n",
    "            # In the example this becomes [b, b]\n",
    "            test_sequences[b_n_i] = tmp_test_sequences[parent_beam_indices[b_n_i]]\n",
    "            \n",
    "            # Now we append the current best candidates\n",
    "            # In this example this becomes [ba,bb]\n",
    "            test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]] + ' '\n",
    "\n",
    "            # Create one-hot-encoded representation for each candidate\n",
    "            pred_words.append(this_level_candidates[b_n_i])\n",
    "\n",
    "\n",
    "    # Calculate best beam id based on the highest beam probability\n",
    "    best_beam_id = parent_beam_indices[np.asscalar(np.argmax(this_level_probs))]\n",
    "\n",
    "    # Update state and output variables for test prediction\n",
    "    session.run(update_sample_beam_state,feed_dict={best_neighbor_beam_indices:[best_beam_id for _ in range(beam_neighbors)]})\n",
    "    \n",
    "    # Make the last word/character/bigram from the best beam\n",
    "    test_word = pred_words[best_beam_id]\n",
    "    \n",
    "    return test_sequences[best_beam_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\software\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Processing step: 0, Using learning rate: 0.00100\n",
      "(0).(7).(8).(5).(10).(2).(4).(1).(9).(6).\n",
      "Average loss at step 1: 4.248968\n",
      "\tPerplexity at step 1: 70.033079\n",
      "\n",
      "Valid Perplexity: 215.03\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      " . UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " of UNK . UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 1, Using learning rate: 0.00100\n",
      "(7).(0).(1).(9).(6).(8).(4).(5).(2).(3).\n",
      "Average loss at step 2: 3.948076\n",
      "\tPerplexity at step 2: 51.835546\n",
      "\n",
      "Valid Perplexity: 193.38\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      " responsibility and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 2, Using learning rate: 0.00100\n",
      "(4).(3).(10).(5).(6).(2).(0).(1).(8).(9).\n",
      "Average loss at step 3: 3.811042\n",
      "\tPerplexity at step 3: 45.197509\n",
      "\n",
      "Valid Perplexity: 171.91\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      " UNK and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 3, Using learning rate: 0.00100\n",
      "(10).(2).(5).(9).(8).(3).(1).(7).(0).(6).\n",
      "Average loss at step 4: 3.678118\n",
      "\tPerplexity at step 4: 39.571854\n",
      "\n",
      "Valid Perplexity: 134.57\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " long UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " our UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 4, Using learning rate: 0.00100\n",
      "(10).(5).(2).(1).(4).(9).(3).(7).(8).(0).\n",
      "Average loss at step 5: 3.579698\n",
      "\tPerplexity at step 5: 35.862694\n",
      "\n",
      "Valid Perplexity: 109.38\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      " you of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " character and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 5, Using learning rate: 0.00100\n",
      "(5).(2).(0).(8).(6).(10).(7).(9).(1).(3).\n",
      "Average loss at step 6: 3.484822\n",
      "\tPerplexity at step 6: 32.616634\n",
      "\n",
      "Valid Perplexity: 81.82\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      " for the UNK UNK , and UNK UNK , and UNK UNK , and UNK UNK , and UNK UNK .  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK to UNK UNK , and UNK UNK , and UNK UNK , and UNK UNK , and UNK UNK .  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and  we are UNK UNK . we will UNK UNK . we are UNK UNK . we are UNK UNK ,  we will UNK UNK . we are UNK UNK . we are UNK UNK , and UNK UNK , and   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 6, Using learning rate: 0.00100\n",
      "(6).(5).(4).(9).(7).(0).(1).(3).(10).(8).\n",
      "Average loss at step 7: 3.435568\n",
      "\tPerplexity at step 7: 31.049055\n",
      "\n",
      "Valid Perplexity: 63.53\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n",
      " be UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 7, Using learning rate: 0.00100\n",
      "(2).(10).(0).(4).(7).(6).(8).(5).(1).(3).\n",
      "Average loss at step 8: 3.387138\n",
      "\tPerplexity at step 8: 29.581163\n",
      "\n",
      "Valid Perplexity: 79.29\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      " act UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK  of the UNK UNK , and UNK UNK UNK UNK UNK UNK , and UNK UNK UNK UNK , and  a UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " , a UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,  and UNK UNK , and UNK UNK UNK UNK , and UNK UNK UNK UNK , and UNK UNK ,   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 8, Using learning rate: 0.00100\n",
      "(4).(6).(2).(5).(1).(3).(10).(8).(7).(9).\n",
      "Average loss at step 9: 3.330288\n",
      "\tPerplexity at step 9: 27.946400\n",
      "\n",
      "Valid Perplexity: 58.17\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      " UNK the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the  same UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of the   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 9, Using learning rate: 0.00100\n",
      "(2).(6).(3).(4).(0).(5).(10).(1).(7).(9).\n",
      "Average loss at step 10: 3.297314\n",
      "\tPerplexity at step 10: 27.039909\n",
      "\n",
      "Valid Perplexity: 51.17\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      " all the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK  of the UNK of the UNK of the UNK of the UNK of the UNK of the UNK of UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 10, Using learning rate: 0.00100\n",
      "(2).(1).(4).(6).(10).(7).(5).(3).(9).(8).\n",
      "Average loss at step 11: 3.258967\n",
      "\tPerplexity at step 11: 26.022643\n",
      "\n",
      "Valid Perplexity: 50.40\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " is the UNK of UNK , and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 11, Using learning rate: 0.00100\n",
      "(8).(6).(7).(10).(0).(9).(5).(3).(2).(4).\n",
      "Average loss at step 12: 3.221936\n",
      "\tPerplexity at step 12: 25.076624\n",
      "\n",
      "Valid Perplexity: 48.08\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      " UNK , UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " and the UNK of the UNK of UNK , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , and UNK , we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 12, Using learning rate: 0.00100\n",
      "(10).(7).(3).(8).(0).(1).(9).(4).(6).(2).\n",
      "Average loss at step 13: 3.178678\n",
      "\tPerplexity at step 13: 24.014983\n",
      "\n",
      "Valid Perplexity: 50.12\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n",
      " UNK to UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " today . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 13, Using learning rate: 0.00100\n",
      "(2).(1).(6).(0).(9).(3).(4).(5).(10).(7).\n",
      "Average loss at step 14: 3.160318\n",
      "\tPerplexity at step 14: 23.578081\n",
      "\n",
      "Valid Perplexity: 35.12\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      " that the UNK of UNK , and UNK , and UNK , and UNK , and UNK , and UNK ,  the UNK of UNK , and UNK , UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and  the UNK UNK . and UNK , and UNK , and UNK , and UNK , and UNK , and   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 14, Using learning rate: 0.00100\n",
      "(8).(10).(9).(1).(2).(4).(5).(6).(7).(0).\n",
      "Average loss at step 15: 3.117426\n",
      "\tPerplexity at step 15: 22.588166\n",
      "\n",
      "Valid Perplexity: 43.00\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      " on a UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 15, Using learning rate: 0.00100\n",
      "(2).(7).(4).(0).(3).(5).(9).(1).(8).(6).\n",
      "Average loss at step 16: 3.119812\n",
      "\tPerplexity at step 16: 22.642122\n",
      "\n",
      "Valid Perplexity: 42.31\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      " of the UNK of UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of our UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 16, Using learning rate: 0.00100\n",
      "(10).(1).(9).(2).(4).(8).(7).(6).(0).(3).\n",
      "Average loss at step 17: 3.074035\n",
      "\tPerplexity at step 17: 21.628996\n",
      "\n",
      "Valid Perplexity: 39.46\n",
      "\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      " is a UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " the UNK of UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and  our UNK UNK , and UNK , and UNK , and UNK , and UNK , and UNK , and   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 17, Using learning rate: 0.00100\n",
      "(1).(9).(6).(5).(7).(4).(10).(0).(8).(2).\n",
      "Average loss at step 18: 3.049043\n",
      "\tPerplexity at step 18: 21.095148\n",
      "\n",
      "Valid Perplexity: 34.76\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UNK , and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " of the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,  the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , and UNK ,   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 18, Using learning rate: 0.00100\n",
      "(9).(0).(5).(2).(3).(8).(6).(7).(4).(1).\n",
      "Average loss at step 19: 3.058870\n",
      "\tPerplexity at step 19: 21.303467\n",
      "\n",
      "Valid Perplexity: 40.50\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n",
      " of the UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  the UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK in the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 19, Using learning rate: 0.00100\n",
      "(0).(4).(10).(6).(5).(8).(9).(2).(7).(1).\n",
      "Average loss at step 20: 3.001620\n",
      "\tPerplexity at step 20: 20.118111\n",
      "\n",
      "Valid Perplexity: 35.19\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      " UNK in the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK in the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,  UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK ,   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 20, Using learning rate: 0.00100\n",
      "(2).(4).(10).(5).(6).(7).(8).(0).(3).(9).\n",
      "Average loss at step 21: 3.000573\n",
      "\tPerplexity at step 21: 20.097051\n",
      "\n",
      "Valid Perplexity: 34.93\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      " UNK , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " but a UNK UNK , UNK , and UNK , and UNK , UNK , and UNK , and UNK ,  the UNK of our UNK , UNK , and UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK  , UNK , and UNK , and UNK , UNK , and UNK , and UNK , and UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 21, Using learning rate: 0.00100\n",
      "(1).(9).(6).(8).(4).(0).(10).(3).(5).(2).\n",
      "Average loss at step 22: 2.980927\n",
      "\tPerplexity at step 22: 19.706081\n",
      "\n",
      "Valid Perplexity: 32.93\n",
      "\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " , we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 22, Using learning rate: 0.00100\n",
      "(3).(8).(4).(5).(9).(7).(2).(6).(1).(0).\n",
      "Average loss at step 23: 2.976556\n",
      "\tPerplexity at step 23: 19.620124\n",
      "\n",
      "Valid Perplexity: 38.96\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      " through a UNK UNK , UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  and UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " UNK . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  . we will UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 23, Using learning rate: 0.00100\n",
      "(3).(9).(2).(10).(0).(8).(7).(5).(4).(1).\n",
      "Average loss at step 24: 2.949489\n",
      "\tPerplexity at step 24: 19.096193\n",
      "\n",
      "Valid Perplexity: 37.17\n",
      "\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      " country , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " first of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK  and UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 24, Using learning rate: 0.00100\n",
      "(9).(3).(8).(4).(5).(2).(7).(10).(6).(1).\n",
      "Average loss at step 25: 2.950862\n",
      "\tPerplexity at step 25: 19.122429\n",
      "\n",
      "Valid Perplexity: 32.09\n",
      "\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n",
      " the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be the UNK of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK  of UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK  and UNK , UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK   \n",
      "====================================================================\n",
      "\n",
      "Processing step: 25, Using learning rate: 0.00100\n",
      "(0).(8).(7).(1).(10).(6).(2).(3).(4).(9).\n",
      "Average loss at step 26: 2.900876\n",
      "\tPerplexity at step 26: 18.190076\n",
      "\n",
      "Valid Perplexity: 35.12\n",
      "\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      " and the UNK of our UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "======================== New text Segment ==========================\n",
      " been a UNK UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK  , and UNK , UNK , UNK , UNK , UNK , UNK , UNK UNK UNK UNK UNK UNK   \n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 26\n",
    "#steps_per_document = 100\n",
    "steps_per_document = 10\n",
    "#docs_per_step = 10\n",
    "docs_per_step = 10\n",
    "valid_summary = 1\n",
    "#train_doc_count = 100\n",
    "train_doc_count = 11\n",
    "\n",
    "beam_nodes = []\n",
    "\n",
    "word2vec_train_perplexity_ot = []\n",
    "word2vec_valid_perplexity_ot = []\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than (num_steps+1)*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>(num_steps+1)*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorSeq(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorSeq(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorSeq(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    print('Processing step: %d, Using learning rate: %.5f'%(step, session.run(tf_learning_rate)))\n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat.reshape(-1).astype(np.int32)\n",
    "                feed_dict[train_labels[ui]] = lbl                \n",
    "            \n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            \n",
    "            doc_perplexity += step_perplexity\n",
    "            \n",
    "            average_loss += step_perplexity\n",
    "            \n",
    "        \n",
    "        session.run(reset_train_state) # resetting hidden state for each document\n",
    "        \n",
    "        # Show the printing progress <train_doc_id_1>.<train_doc_id_2>. ...\n",
    "        print('(%d).'%di,end='') \n",
    "        \n",
    "    print('')\n",
    "    \n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      average_loss = average_loss / (steps_per_document*docs_per_step*valid_summary)\n",
    "      \n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      word2vec_train_perplexity_ot.append(np.exp(average_loss))\n",
    "      average_loss = 0 # reset loss\n",
    "      \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      word2vec_valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "    \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "    \n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 2\n",
    "      chars_in_segment = 250//beam_length\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        # first word randomly generated\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word = rand_doc[np.random.randint(len(rand_doc))]\n",
    "        print(\"\",reverse_dictionary[test_word],end=' ')\n",
    "        \n",
    "        # Generating words within a segment with Beam Search\n",
    "        for _ in range(chars_in_segment):\n",
    "            \n",
    "            test_sequence = get_beam_prediction(session)\n",
    "            print(test_sequence,end=' ')\n",
    "            \n",
    "        print(\" \")\n",
    "        session.run(reset_sample_beam_state)\n",
    "        \n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(word2vec_train_perplexity_ot)\n",
    "    writer.writerow(word2vec_valid_perplexity_ot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "442.86px",
    "left": "2810.33px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
