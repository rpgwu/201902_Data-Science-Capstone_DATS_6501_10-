{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending LSTMs: LSTMs with Peepholes and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\software\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure president stories are downloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  ../data\\speech_1.txt\n",
      "File  speech_1.txt  already exists.\n",
      "Downloading file:  ../data\\speech_2.txt\n",
      "File  speech_2.txt  already exists.\n",
      "Downloading file:  ../data\\speech_3.txt\n",
      "File  speech_3.txt  already exists.\n",
      "Downloading file:  ../data\\speech_4.txt\n",
      "File  speech_4.txt  already exists.\n",
      "Downloading file:  ../data\\speech_5.txt\n",
      "File  speech_5.txt  already exists.\n",
      "Downloading file:  ../data\\speech_6.txt\n",
      "File  speech_6.txt  already exists.\n",
      "Downloading file:  ../data\\speech_7.txt\n",
      "File  speech_7.txt  already exists.\n",
      "Downloading file:  ../data\\speech_8.txt\n",
      "File  speech_8.txt  already exists.\n",
      "Downloading file:  ../data\\speech_9.txt\n",
      "File  speech_9.txt  already exists.\n",
      "Downloading file:  ../data\\speech_10.txt\n",
      "File  speech_10.txt  already exists.\n",
      "Downloading file:  ../data\\speech_11.txt\n",
      "File  speech_11.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a directory if needed\n",
    "dir_name = \"../data\"\n",
    "num_files = 11\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  #Download a file if not present\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "\n",
    "filenames = [\"speech_\"+format(i, '01d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "11 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    print( filenames)\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "   # assert file_exists\n",
    "print('%d files found.'%len(filenames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file ../data\\speech_1.txt\n",
      "Data size (Characters) (Document 0) 3443\n",
      "Sample string (Document 0) ['fo', 'r ', 'my', 'se', 'lf', ' a', 'nd', ' f', 'or', ' o', 'ur', ' n', 'at', 'io', 'n,', ' i', ' w', 'an', 't ', 'to', ' t', 'ha', 'nk', ' m', 'y ', 'pr', 'ed', 'ec', 'es', 'so', 'r ', 'fo', 'r ', 'al', 'l ', 'he', ' h', 'as', ' d', 'on', 'e ', 'to', ' h', 'ea', 'l ', 'ou', 'r ', 'la', 'nd', '.\\n']\n",
      "\n",
      "Processing file ../data\\speech_2.txt\n",
      "Data size (Characters) (Document 1) 6871\n",
      "Sample string (Document 1) ['se', 'na', 'to', 'r ', 'ha', 'tf', 'ie', 'ld', ', ', 'mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'mo', 'nd', 'al', 'e,', ' s', 'en']\n",
      "\n",
      "Processing file ../data\\speech_3.txt\n",
      "Data size (Characters) (Document 2) 7320\n",
      "Sample string (Document 2) ['se', 'na', 'to', 'r ', 'ma', 'th', 'ia', 's,', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ' b', 'ur', 'ge', 'r,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'sp', 'ea', 'ke', 'r ', \"o'\", 'ne', 'il', 'l,', ' s', 'en', 'at', 'or', ' d', 'ol', 'e,', ' r', 'ev', 'er', 'en', 'd ']\n",
      "\n",
      "Processing file ../data\\speech_4.txt\n",
      "Data size (Characters) (Document 3) 6255\n",
      "Sample string (Document 3) ['mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' q', 'ua', 'yl', 'e,', ' s', 'en', 'at', 'or', ' m', 'it', 'ch', 'el', 'l,', ' s', 'pe', 'ak', 'er', ' w', 'ri', 'gh', 't,', ' s', 'en', 'at', 'or', ' d']\n",
      "\n",
      "Processing file ../data\\speech_5.txt\n",
      "Data size (Characters) (Document 4) 4540\n",
      "Sample string (Document 4) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'to', 'da', 'y ', 'we', ' c', 'el', 'eb', 'ra', 'te', ' t', 'he', ' m', 'ys', 'te', 'ry', ' o', 'f ', 'am', 'er', 'ic', 'an', ' r', 'en', 'ew', 'al', '. ', 'th', 'is', ' c', 'er', 'em', 'on', 'y ', 'is', ' h', 'el', 'd ', 'in', ' t', 'he']\n",
      "\n",
      "Processing file ../data\\speech_6.txt\n",
      "Data size (Characters) (Document 5) 6082\n",
      "Sample string (Document 5) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'at', ' t', 'hi', 's ', 'la', 'st', ' p', 're', 'si', 'de', 'nt', 'ia', 'l ', 'in', 'au', 'gu', 'ra', 'ti', 'on', ' o', 'f ', 'th', 'e ', '20', 'th', ' c', 'en', 'tu', 'ry', ', ', 'le', 't ', 'us', ' l', 'if', 't ', 'ou', 'r ', 'ey', 'es']\n",
      "\n",
      "Processing file ../data\\speech_7.txt\n",
      "Data size (Characters) (Document 6) 4520\n",
      "Sample string (Document 6) ['th', 'an', 'k ', 'yo', 'u,', ' a', 'll', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 're', 'hn', 'qu', 'is', 't,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'di', 'st', 'in']\n",
      "\n",
      "Processing file ../data\\speech_8.txt\n",
      "Data size (Characters) (Document 7) 5961\n",
      "Sample string (Document 7) ['vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' c', 'he', 'ne', 'y,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'pr', 'es', 'id', 'en', 't ', 'ca', 'rt', 'er', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'cl', 'in', 'to', 'n,', ' m', 'em', 'be']\n",
      "\n",
      "Processing file ../data\\speech_9.txt\n",
      "Data size (Characters) (Document 8) 6680\n",
      "Sample string (Document 8) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'i ', 'st', 'an', 'd ', 'he', 're', ' t', 'od', 'ay', ' h', 'um', 'bl', 'ed', ' b', 'y ', 'th', 'e ', 'ta', 'sk', ' b', 'ef', 'or', 'e ', 'us', ', ', 'gr', 'at', 'ef', 'ul', ' f', 'or', ' t', 'he', ' t', 'ru', 'st', ' y', 'ou', ' h', 'av']\n",
      "\n",
      "Processing file ../data\\speech_10.txt\n",
      "Data size (Characters) (Document 9) 5973\n",
      "Sample string (Document 9) ['th', 'an', 'k ', 'yo', 'u.', ' t', 'ha', 'nk', ' y', 'ou', ' s', 'o ', 'mu', 'ch', '.\\n', '\\nv', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bi', 'de', 'n,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'me', 'mb', 'er', 's ', 'of', ' t', 'he', ' u', 'ni', 'te', 'd ', 'st', 'at', 'es']\n",
      "\n",
      "Processing file ../data\\speech_11.txt\n",
      "Data size (Characters) (Document 10) 4223\n",
      "Sample string (Document 10) ['ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 'ro', 'be', 'rt', 's,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'ob', 'am', 'a,', ' f', 'el', 'lo', 'w ']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    # make all the text lowercase\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Breaking the text into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    # Creates a list of lists with the bigrams (outer loop different stories)\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries (Bigrams)\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61868 Characters found.\n",
      "Most common words (+UNK) [('e ', 1945), (' t', 1623), (' a', 1378), ('th', 1378), ('s ', 1110)]\n",
      "Least common words (+UNK) [('yâ', 1), ('tp', 1), ('”f', 1), ('”u', 1), ('kf', 1), ('-l', 1), ('40', 1), ('\\nl', 1), ('hm', 1), ('ja', 1), ('n:', 1), ('zo', 1), ('uy', 1), ('r:', 1), ('ky', 1)]\n",
      "Sample data [78, 15, 250, 63, 298, 3, 16, 33, 24, 7]\n",
      "Sample data [63, 121, 32, 15, 34, 0, 103, 117, 17, 0]\n",
      "Vocabulary:  351\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the RNN. More specifically the generator breaks a given sequence of words into `batch_size` segments. We also maintain a cursor for each segment. So whenever we create a batch of data, we sample one item from each segment and update the cursor of each segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\tpr (97), \tr  (15), \the (6), \te  (1), \tou (22), \n",
      "\tOutput:\n",
      "\ted (49), \tfo (78), \t h (50), \tto (32), \tr  (15), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\ted (49), \tfo (78), \t h (50), \tto (32), \tr  (15), \n",
      "\tOutput:\n",
      "\tec (114), \tr  (15), \tas (88), \t h (50), \tla (144), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tec (114), \tr  (15), \tas (88), \t h (50), \tla (144), \n",
      "\tOutput:\n",
      "\tes (26), \tal (58), \t d (59), \tea (52), \tnd (16), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\tes (26), \tal (58), \t d (59), \tea (52), \tnd (16), \n",
      "\tOutput:\n",
      "\tso (129), \tl  (51), \ton (21), \tl  (51), \t.\n",
      " (115), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\tso (129), \tl  (51), \ton (21), \tl  (51), \tou (22), \n",
      "\tOutput:\n",
      "\tr  (15), \the (6), \te  (1), \tou (22), \tr  (15), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM, LSTM with Peepholes and GRUs\n",
    "\n",
    "* A LSTM has 5 main components\n",
    "  * Cell state, Hidden state, Input gate, Forget gate, Output gate\n",
    "* A LSTM with peephole connections\n",
    "  * Introduces several new sets of weights that connects the cell state to the gates\n",
    "* A GRU has 3 main components\n",
    "  * Hidden state, Reset gate and a Update gate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "\n",
    "Here we define several hyperparameters and are very similar to the ones we defined . However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "batch_size = 64\n",
    "#num_unrollings = 50\n",
    "num_unrollings = 100\n",
    "dropout = 0.2\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test input (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "valid_inputs = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters and Cell Computation\n",
    "\n",
    "We define parameters and cell computation functions for all the different variants (LSTM, LSTM with peepholes and GRUs). **Make sure you only run a single cell withing this section (either the LSTM/ LSTM with peepholes or GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard LSTM\n",
    "\n",
    "Here we define the parameters and the cell computation function for a standard LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_dropout.csv\n"
     ]
    }
   ],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "algorithm = 'lstm'\n",
    "#algorithm = 'lstm_peephole'\n",
    "#algorithm = 'gru'\n",
    "\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "print( filename_to_save)\n",
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTMs with Peephole Connections\n",
    "\n",
    "We define the parameters and cell computation for a LSTM with peepholes. Note that we are using diagonal peephole connections (for more details refer the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "ic = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "fc = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],0.0,0.01))\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "oc = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],0.0,0.01))\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "algorithm = 'lstm_peephole'\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "# Definition of the cell computation.\n",
    "def lstm_with_peephole_cell(i, o, state):\n",
    "    '''\n",
    "    LSTM with peephole connections\n",
    "    Our implementation for peepholes is based on \n",
    "    https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf    \n",
    "    '''\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + state*ic + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + state*fc + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + state*oc + tf.matmul(o, om) + ob)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Units (GRUs)\n",
    "\n",
    "Finally we define the parameters and cell computations for the GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Reset gate: input, previous output, and bias.\n",
    "rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "rh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "rb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Hidden State: input, previous output, and bias.\n",
    "hx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "hh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "hb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Update gate: input, previous output, and bias.\n",
    "zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "zh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "zb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "algorithm = 'gru'\n",
    "#filename_to_save = algorithm + filename_extension +'.csv'\n",
    "\n",
    "# Definition of the cell computation.\n",
    "def gru_cell(i, o):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    reset_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rh) + rb)\n",
    "    h_tilde = tf.tanh(tf.matmul(i,hx) + tf.matmul(reset_gate * o, hh) + hb)\n",
    "    z = tf.sigmoid(tf.matmul(i,zx) + tf.matmul(o, zh) + zb)\n",
    "    h = (1-z)*o + z*h_tilde\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM/GRU/LSTM-Peephole Computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "  state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "# Note: there is no cell state for GRUs\n",
    "for i in train_inputs:\n",
    "    if algorithm=='lstm':\n",
    "      output, state = lstm_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "    elif algorithm=='lstm_peephole':\n",
    "      output, state = lstm_with_peephole_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "    elif algorithm=='gru':\n",
    "      output = gru_cell(i, output)\n",
    "      train_state_update_ops = [saved_output.assign(output)]\n",
    "        \n",
    "    output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "valid_output = saved_valid_output\n",
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "  valid_state = saved_valid_state\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "if algorithm=='lstm':\n",
    "    valid_output, valid_state = lstm_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "    \n",
    "elif algorithm=='lstm_peephole':\n",
    "    valid_output, valid_state = lstm_with_peephole_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "elif algorithm=='gru':\n",
    "    valid_output = gru_cell(valid_inputs, valid_output)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output)]\n",
    "\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(valid_state_update_ops):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "if algorithm=='lstm':\n",
    "  test_output, test_state = lstm_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "elif algorithm=='lstm_peephole':\n",
    "  test_output, test_state = lstm_with_peephole_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "elif algorithm=='gru':\n",
    "  test_output = gru_cell(test_input, saved_test_output)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output)]\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(test_state_update_ops):\n",
    "    test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "We calculate the training loss of the LSTM here. It's a typical cross entropy loss calculated over all the scores we obtained for training data (`loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies(train_state_update_ops):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch). But since GRU doesn't have a cell state we have a conditioned reset_state ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "    # Reset train state\n",
    "    reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "    reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = tf.group(\n",
    "        saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01)),\n",
    "        saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.01)))\n",
    "    \n",
    "elif algorithm=='gru':\n",
    "    # Reset train state\n",
    "    reset_train_state = [tf.assign(saved_output, tf.zeros([batch_size, num_nodes]))]\n",
    "\n",
    "    # Reset valid state\n",
    "    reset_valid_state = [tf.assign(saved_valid_output, tf.zeros([1, num_nodes]))]\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = [saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using the Adam optimizer as it is one of the best optimizers out there. Furthermore we use gradient clipping to prevent any gradient explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for decaying learning rate\n",
    "gstep = tf.Variable(0, trainable=False)\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# Decays learning rate everytime the gstep increases\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Sampling to Break the Repetition\n",
    "Here we write some simple logic to break the repetition in text. Specifically instead of always getting the word that gave this highest prediction probability, we sample randomly where the probability of being selected given by their prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "  '''Greedy Sampling\n",
    "  We pick the three best predictions given by the LSTM and sample\n",
    "  one of them with very high probability of picking the best one'''\n",
    "  best_inds = np.argsort(distribution)[-3:]\n",
    "  best_probs = distribution[best_inds]/np.sum(distribution[best_inds])\n",
    "  best_idx = np.random.choice(best_inds,p=best_probs)\n",
    "  return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM to Generate Text\n",
    "\n",
    "Here we train the model on the available data and generate text using the trained model for several steps. From each document we extract text for `steps_per_document` steps to train the model on. We also report the train perplexity at the end of each step. Finally we test the model by asking it to generate some new text starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n",
      "(1).(0).(10).(5).(9).(7).(4).(6).(3).(8).\n",
      "Average loss at step 1: 5.321938\n",
      "\tPerplexity at step 1: 204.780332\n",
      "\n",
      "Valid Perplexity: 274.67\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      "\t the the e th te th tthe  tthe e e e  t tthe e e e th t tth t te e  tth te th te the  tththe the the e th te  te  te ththe the  t te  t tthe the e e e th t te e  tthe th tth t te the  t tthe e thth te  te thththth t te  tthth t te e  tththe e  te  te e e e th te  t t tthe th t tthe e th te th t tth t t t t te e e e thth t te the  tthe  tthe e e e  tthth t tthththth te the e  tthe the  tth te  te  tththth t t te  t t t t t tthe  tththe e e  tth tththth te the e  tthththe e e  tthe  t t te  tththth tth t tthe e the e e the e the e e  te th t t te the  tth te e  tthe e  tthth te ththth t t t tth tth tth tth te thth te th t te the  t t t te  t te thth t t te  te e e e ththe thththth t te the  te e  te  t t t te th te th tthe the th t tthe  te e the the the  t te thth tth t te th te  tthe e e e  te e ththe e  tthe th t te  tthe thth tthe ththe the thth tthe  te e e  te ththe e e e  t tthe e e the e  tthe e e  tthth te e e th te e  t t te  t tthe e  te ththe th t te  tth t t te e th te  te e  t\n",
      "====================================================================\n",
      "\n",
      "(2).(3).(5).(9).(6).(1).(10).(7).(0).(4).\n",
      "Average loss at step 2: 5.182668\n",
      "\tPerplexity at step 2: 178.157546\n",
      "\n",
      "Valid Perplexity: 235.84\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  her thethth tththe e  te ththe  te th tth te e e ththe th tthth te  tthth te e e e the e e  thethe e the the e e the e ththe the ththe  t thethe e e e e th te th te th te  tthe  te the the th tthe  tthe th t theththe  tthe  tthe e th te e e the  thee  te the  tthe e e e e the th tthth tth te th tth t t te e e th te e e the the e th tthe  the tthe e  the thethe e e  tthe e the the e  tththe e  tthe  tthe ththe  tthe e  tththe e  te th tthe e  te thth t te th te  tth te  tththe  t tthe ththe e the ththth t tthe  te th te the e  tth te  te  tththe e the the th tth te th te the e  thee  the te th tth te e the e thth tthe e  tththth tthe th tthe the  te  te  te e  te thththe  t te e ththththe th tthe e the e e the  tth te e th t tthe e e the ththe  tththe e e the e e th te  the tthe e  tthth t te e th te the  tthe  thee e ththe e the the  the tthe e e e the  te  the te e e  te e e the e e  te  thethe e th te thththe e the th tthe  tth t tth tthe thththth tthththe the the th te e thth t tthe \n",
      "====================================================================\n",
      "\n",
      "(4).(8).(2).(9).(6).(1).(3).(5).(0).(7).\n",
      "Average loss at step 3: 5.018999\n",
      "\tPerplexity at step 3: 151.259873\n",
      "\n",
      "Valid Perplexity: 163.31\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "\t f of to e and and thth the the thee the th the thee the th a thee e the th a thee the the anthe e anthe the th ande e e the and the th te th the the to the e e ane th the thee e e the and and ane e e the the the e anththe the e th thee the the e e the th te ane the e and th to the the th thee ane the e the the the th the the ande the ane anthe ane e e ane th and the thee anth the to the and ane th the to and e and ane the and and th the ande e e ane the and ane th a and the ande e th to the the th the thee ane the and ane e the e the the e the th te th te the anthe e e e the and e e e ane and e anthe and and and ththe e the the th to the the e anththe the the th thee the th the to th the a thee e e e e the e th ande ane anthe e the e e e the and e ththe and anthe and the the the and the e the the and e th to e ane th the thee and e th the a and a a a te the and the e the e e e and ane e the the the e ththe the and the th the thee e ane ane the and e e th and and the a and the a a a the a\n",
      "====================================================================\n",
      "\n",
      "(6).(5).(2).(10).(1).(3).(7).(8).(4).(9).\n",
      "Average loss at step 4: 4.569600\n",
      "\tPerplexity at step 4: 96.505477\n",
      "\n",
      "Valid Perplexity: 80.13\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  cour the the and and the and our our and the the the and the of and want the ther as and the and the and our and the and and the the the the and and there the the of to the the and and the the and the the the the the the and the thate our our our our the the and our the ther as and the of we and there and and the of the as the of and the wert the of we the ane and the ant our the and and the and ant and the of as the and and oue the the the of and we thate our our and the and the the the and the of we the of to our ther ames and the the of to the the the and the that the the we and the and and that to the of the and and and and we the of and wore the and the and the and the the the and our our the and and the and the of and and as thate our the the of the to and and our and the of to our and the the the the of the and and to the the the the and the the the the of we the ther the an the the the the the the and the and and our our thate the the the the of the to and our the and and the an\n",
      "====================================================================\n",
      "\n",
      "(2).(7).(4).(10).(8).(3).(6).(1).(5).(9).\n",
      "Average loss at step 5: 4.084496\n",
      "\tPerplexity at step 5: 59.411959\n",
      "\n",
      "Valid Perplexity: 44.79\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "\t . the the with the we the with and americans and the peave and and and to that of our will to the will the we the the with the pore the this our wore of to the the withe and the the the of to of and and the pore of of that the we will the we that and our the seetion our to the the will that the the the withe the with to of to our the of the peace and of the the that to the will the peat of the we withe the the with to the will and to of the we will and and and that the of and to the of to our that the porat to the that that the the will and and to will and are and to the the the the the the that the pore and the of that the the of the we the the the with and and to will of this the seed the this our we will our and that of and our are of the of the will the the this the we the the of and to of to of the some the the the of ames can this to our of the sould the we of the wory the of to the the the the the will and to the with and the somat and to the the the the the the of and our we will\n",
      "====================================================================\n",
      "\n",
      "(8).(3).(1).(10).(7).(0).(4).(5).(2).(9).\n",
      "Average loss at step 6: 3.749674\n",
      "\tPerplexity at step 6: 42.507237\n",
      "\n",
      "Valid Perplexity: 28.90\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      "\t long to our the to work the that of ours and to the world to and to that our prong to our of of the work and powerican to the the word our hople are the hearts that the some the to work the world to are the his the powericand the that the some and that and our peoplat the his of our world the this the proces of our cour store and the world to we the the world that americans, and the the that the prong the strengts that that the to and and our heach to the work of our world the the this the prom and the world that to the the to will the the word, and the the the world that the power, and the that amerity, and that on the that that the the to we can our the to we the the world the the work of that of our word, and the world of of our of that the world the world.\n",
      "\n",
      "the the world the to here of the stry the to to sed our that of this our that to the soment the the the the this the powe the world of to sed and the to amering our cought and to of the word, we to our to americans of our of ameri\n",
      "====================================================================\n",
      "\n",
      "(0).(3).(5).(8).(9).(4).(10).(1).(7).(6).\n",
      "Average loss at step 7: 3.518850\n",
      "\tPerplexity at step 7: 33.745586\n",
      "\n",
      "Valid Perplexity: 20.08\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t n the the comperican to sed the compless the cand the tory to serve the to by the counith that and to serve the to the stand to the to by and to but are the will the tory the stake americans of that to that the world and the will make the work the commity and our this the stand of the works of the stry and this and america's greedom of our lives and our the to the stand that that all be this to best americans of this the stand and and the stake the stry to the sought and the world of and americans of oursity of of our the tory and the stry of to the this the sour the the world to americans our seedom this sour of to bertUNK americans of the work of our counity, the stry to be the that the will the country the to the stake the strieng the will bere and stries to the cone and to are amerity of our lition the wore of the strieng america's stand the stand of that the cour this all by this stand that of the stand on and our that our strence. and our counity this to best americans our goday, we\n",
      "====================================================================\n",
      "\n",
      "(7).(4).(3).(9).(1).(6).(10).(2).(0).(8).\n",
      "Average loss at step 8: 3.362923\n",
      "\tPerplexity at step 8: 28.873475\n",
      "\n",
      "Valid Perplexity: 15.94\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  seedom that our people of our progres, we we have and stry to be that the stries to and the strences and thation and to the prom the have and we world the people worn the have will mall but of the words.\n",
      "\n",
      "and the world of of the world the work our spirity and we will reserican will make on the people to serve the penderica there and we hople are the people and thation the world to and will make our people and the world of our strength and to to the have to bestory and we we hat of that that our strength of to the hat ing the world to the progres, and the world of the hople and will not we will make and the people of the people and the world the world, and spirity, and we work to to serve the pend the we will to the world of our could america world we hat the world of the stands that we the world the words of the price of our world the world of all to the to the stated of the stry tory and the hople will that on the have aments this to se will that we have and seedom the people to that o\n",
      "====================================================================\n",
      "\n",
      "(0).(3).(7).(2).(10).(4).(8).(1).(9).(5).\n",
      "Average loss at step 9: 3.224367\n",
      "\tPerplexity at step 9: 25.137652\n",
      "\n",
      "Valid Perplexity: 13.94\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ces of the cone of of that we must of our country, will revernments of to the words of our proud of the words, to the faith of the contury and to berty the faces the words of our people and the world to the coughts and that is of the world the with our proce of thate the works of and to the fates and this to the work to the faces and the world that we much american the face of from of the world the faits of are to the fates the fates that wills and the seek of our conting of this and the cand that we can in our cone and.\n",
      "\n",
      "the world of the power, and the words of our cought of the strength that and we must to that the people the work and somet us of our power, we mart the work our stry to be the world of our people, the couldry of freedom is to the will all the stand of the world of our couldrent to the faith the world this strence that that we call be the world the words of our stries of our stries to be the words of our comple, the people of that is the power that of our stand that the \n",
      "====================================================================\n",
      "\n",
      "(8).(3).(2).(7).(4).(5).(10).(0).(9).(1).\n",
      "Average loss at step 10: 3.112904\n",
      "\tPerplexity at step 10: 22.486243\n",
      "\n",
      "Valid Perplexity: 12.67\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      "\t y and destory the world to be with to the problems and the will make to the words and the world the world to the stand the will make on our nation the strength and the words of to been america will make of the world of of the world of the will not those that that the precious and strengther the words of the proud and with our compless of freedom.\n",
      "\n",
      "the words of our proud the we call be today, and will be american the world.\n",
      "\n",
      "we will make that ourshere the strength and our belitizens, and the sacrifican will berty and depender, and our complems we will be with america we hat that we have the will not the words and our country of and strength this now all that we will now,UNK the powerful, american that is the words and the world the world to the we will rese the words: you are to serve and the sament, and the world of the people.\n",
      "\n",
      "that we hat one to to berty and this every the we can ing today and this the people to and the people and the words of and some they and some the words: to serves\n",
      "====================================================================\n",
      "\n",
      "(2).(9).(10).(6).(5).(1).(8).(0).(4).(7).\n",
      "Average loss at step 11: 3.000577\n",
      "\tPerplexity at step 11: 20.097121\n",
      "\n",
      "Valid Perplexity: 12.55\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "\t s, we can the heard of that been the will make the powerful, we call the work to the power, and our nal one the work of our charting amentUNKUNKUNKUNKt for the sament. and our nation to this americans, we will may heard the sament. we we must do that we can ing the world the world that the we must the stand that the world.\n",
      "\n",
      "the sament, and the world of the people to be that we have to bertUNKUNKt the worker. we will not be must the work and we call to belies our couldrent that the words. work and the world in the people of americans, and our nal ontion the people of that the world of our seek to the stand to the sacrifice and the sacrifice, we call the stalled to serve the same and the human service. that we have ament, and the world in our strong the world that in a complessed and with the people, that the world and americans, and we must this decious and we can in our strength and the world of the people and the world. and strongth and the same and the have the world the sames and our nation\n",
      "====================================================================\n",
      "\n",
      "(6).(9).(7).(4).(0).(2).(3).(1).(8).(10).\n",
      "Average loss at step 12: 2.943118\n",
      "\tPerplexity at step 12: 18.974925\n",
      "\n",
      "Valid Perplexity: 10.31\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ory and the world in america we will all the will not the words:UNKUNK the weach the couldress the contion, and the sacrifice of our striety of the fates and the words:UNKUNKUNKUNKUNKUNKUNKUNKUNKUNK we will all, and we will make to make and will be america. we are will not the world.\n",
      "\n",
      "we have will all, and we have that the will make to that we must be to the care the sacrife, but we will be the faces and the world to that the facter, and the work of our nation, and we we will that the work of our nation, and the worker, and the sames and the words:UNKUNKUNKUNK that we will make of freedom is the couldry. the words: you, and our besting of the seek of our struggle the world of our good to the count that the faces and the world to the will beling and all that we will make america. and we must of the seek to america's congrest this we will the country. and the world of our seen of our good in the will make america world will make america. and we must will be that and and to make and the strength and with a\n",
      "====================================================================\n",
      "\n",
      "(5).(6).(7).(0).(10).(2).(9).(3).(8).(1).\n",
      "Average loss at step 13: 2.886886\n",
      "\tPerplexity at step 13: 17.937360\n",
      "\n",
      "Valid Perplexity: 9.96\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t e that the work yoUNK god's helves of the world and the struggly of the cartion that is are the hearts of our more to that is the seen this work a responsibirty and the worker, and the same they that this strong to our commit of our chartizens, the untry to the stand of freedom, these worn to the couldry, and the world of our mory that the country, a country, with the world. the hear the sames and the faces of the country and to the unities, whose who will make american our nation. and to the stand that the unities to but we have been a startions to the concepter the strength and that we with the stands the facter, a new breeze but on the sacrosy and to be the world by the world and this of the stake. that we must and the staten to bely this stand to the untry, the couldn't is the consibility, that will, to make american unity, and the struggle of the country, and to be the facter. the words to believe that is this of and we can the faith therica nation to the world to thich the seek to t\n",
      "====================================================================\n",
      "\n",
      "(5).(1).(8).(9).(7).(0).(10).(2).(4).(6).\n",
      "Average loss at step 14: 2.790856\n",
      "\tPerplexity at step 14: 16.294967\n",
      "\n",
      "Valid Perplexity: 10.24\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ound the couldren and we will make american and our belief in our nation to the world to the powerful make our country. we will be must the work, but that we must and in our commitments of the serving and in our nal one this contion, and the sacrifice, we must the strength the world to americans. there and to all and in our nation, and we have the world to americans, we have today, and to the work is the work of our common our service, and this story and stand to be american that is the world to a so mon day and that we must powerful respection of our nation. and the words to a conving again. and our nation to security, we will make america's helver and the same, and we have to to be americans, we are is the same, can one today of our national life of the cannot by done and the canges the unity, will begive in our nation, and they are is this storms to be done and we will rese today and the called as and the words of our nations. we we must americans, and the world. americans, we will ma\n",
      "====================================================================\n",
      "\n",
      "(1).(8).(3).(5).(4).(2).(9).(6).(10).(7).\n",
      "Average loss at step 15: 2.777273\n",
      "\tPerplexity at step 15: 16.075131\n",
      "\n",
      "Valid Perplexity: 10.44\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  proud and we will refore of the story, and the same to make americans are americans whose in the under country, with there and the unity, and all, and the world to be the world.\n",
      "\n",
      "we have, but our couldress or a new this will make americans, this will make to the people, and the same and all the fored by responsion, and we americans with make america we will reach in the same as we will make america will make america will make america. we will make are not all be of all the freedom in the unity and the unity and that all to make america will make america. we will not the world to the strength the same nation of our source of america. the work and will make america wealthy and america will begive to the world of our people and our community the same and the same gleach of this world but our continue of our sounded the strength and our country, will make america. we will make america will be america's grance and directs of our sounded and america. we relUNKes and america wealthy and thas be\n",
      "====================================================================\n",
      "\n",
      "(7).(9).(10).(2).(1).(0).(3).(8).(4).(5).\n",
      "Average loss at step 16: 2.720075\n",
      "\tPerplexity at step 16: 15.181462\n",
      "\n",
      "Valid Perplexity: 10.23\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      "\t blest of the country, the work of the same as we must the world will america's helUNK the same of our nother complishment, build the world of the power, we must will be do not of our children and strong to be the said, UNK that we have done. the world to america's conviction and demost power. we will make the conviction of are now to that is the world.\n",
      "\n",
      "we had enabled the people and the people on the people. we will not be my fellow and we must power to the words of the promise there are nations the stand are in the world with and work to the stands of our sound. the world to be the world to the conving to the faith can fellow and the strong that is a passed, and we have work and something the cance americans, the words to a so mon danger, will be dom is all that we will begin the persity. we must american under of our source of the seedom.\n",
      "\n",
      "this the people to be done. the cannot of the work and stand the couldry, the world to believe and americans of our community the same, can the world \n",
      "====================================================================\n",
      "\n",
      "(7).(4).(6).(0).(5).(2).(9).(1).(10).(3).\n",
      "Average loss at step 17: 2.647738\n",
      "\tPerplexity at step 17: 14.122061\n",
      "\n",
      "Valid Perplexity: 9.89\n",
      "\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ept is the faith and we will be will and who are the people. and we will the stately that is that the same, and with to make america will make to make america's belies and will be will be america new centuries to the will new that we will the world. we are all be the world with and all, and the same to the unity that is the work, we will make the will make the work of one common purpose, will make america. they freedom are all the stately, they will make america. we are that is all as the sacrifice.\n",
      "\n",
      "thank your greatest that and we are the under of the weach of our belief on the world to but the struggly with the weach to mall of this will never the unity, and alliances and we are protect the words of our sound and strengthen our nation that is all, and the world of our strength and destory and with america will be in our nation, we will not be must they will be in our best acts of our courations of this of our country, and they will make america wealth to the weach our country, and we w\n",
      "====================================================================\n",
      "\n",
      "(4).(7).(9).(5).(10).(8).(3).(1).(0).(2).\n",
      "Average loss at step 18: 2.626768\n",
      "\tPerplexity at step 18: 13.829009\n",
      "\n",
      "Valid Perplexity: 9.58\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      "\t erfuture to the preclaw that we had we must be must require and the barrers.\n",
      "\n",
      "we will not been the human for equal of this strong peach the words.\n",
      "\n",
      "we will mand american unity of one on a great of the people, we will be this dream.\n",
      "\n",
      "to the world of our most of the heart of our national the human do the world that we are not be a strong, human for america weach is not just my best and in this strong and the bander, and that is not be a same to the state the people of our societyUNKUNKUNKUNKt is the powerful, to be american peUNKlet us to be are not be protect of our societyUNKUNKUNKand the world to the heart and strength and the world.\n",
      "\n",
      "the problems and we are the unity, with this dream.\n",
      "UNKUNK the struggle the bander, we will not be much has not be a prespect of our strength and the words of our mear the same and their people.\n",
      "\n",
      "the words of our most to the strength and the world of their people, but the strength and our country. we will reflect our heart, and the bander, and we must be and we have\n",
      "====================================================================\n",
      "\n",
      "(4).(8).(3).(5).(2).(7).(10).(1).(0).(6).\n",
      "Average loss at step 19: 2.581826\n",
      "\tPerplexity at step 19: 13.221257\n",
      "\n",
      "Valid Perplexity: 9.56\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n",
      "\t , we will not be my belief in the back of our nation, and not been our canselves to the work, we will not be must that we will be to the proud and strength and the world will be americans.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we relied and the world to the people of their people and our country. we do nations of the problems, we will be muse this strengthen our compless and the people, the people, the world in the words of our courage, we will not be my accomplity of this more to the struggle to be prothers are is not be and strengther the world with great we had not the words:UNKor the will be never know that we do it our country and the world with the state to that is we we will be the barries and government, and the world to the world in our nation, and not been own government to the same of their national completely their never knot just my accomplete the powerful, the same glorious, the words to by the world of the people of the progress.\n",
      "\n",
      "and the world with all, when struggle of our country, and our nation, and we wil\n",
      "====================================================================\n",
      "\n",
      "(7).(1).(10).(6).(4).(0).(8).(9).(5).(2).\n",
      "Average loss at step 20: 2.526238\n",
      "\tPerplexity at step 20: 12.506364\n",
      "\n",
      "Valid Perplexity: 10.26\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      "\t UNKUNKhang the people and that we wind the people of america's helUNK to the precious values, the proted the demands of american the world will be this work and the cannot on a pledge and the strong, not must be and the power the people of our source and the promine of our country, will refor the protect our sounded they will take our nation that the precious long respection, and that we cannot be done. and they are the strong, has are the same dream of our cance american the world with generess and the world of the progress of our cance americans, as world to the world. and the human dignity of the promise of america's believe and the hearts of our couldren and the world with to serve the promise of are all, and the promise of our strength the world.\n",
      "\n",
      "this country, and our believe us that the world that we will be to be promise, that we have now we more the world will always, always call of the world of the stake and the stands the world that we must as we have the words of our present of \n",
      "====================================================================\n",
      "\n",
      "(1).(5).(2).(3).(6).(7).(4).(0).(8).(10).\n",
      "Average loss at step 21: 2.503189\n",
      "\tPerplexity at step 21: 12.221405\n",
      "\n",
      "Valid Perplexity: 9.72\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      "\t t that the world of the work as well rese the words of our source of the people to the same globd by which refore of their people.\n",
      "\n",
      "the world.\n",
      "\n",
      "we will not just alwind in the stately struggle this storm.\n",
      "\n",
      "and we must and we will ready, we will make our courage, not the people, the says, UNKUNK the world but that the world of the world be problems which in the world, that we have changed and we had been our country to the stand again. they will not be mustice of those who heart this just that it our country to be promise of their people. their people and our best people of our nations of those ard the world of will not be my accomplishmentUNKUNKUNKhat the world which and we have hear america's bright of our new are thank you, and we had been a children, and the world of one service to that the same greater stand the problems with the world which the world whate the world with god's brive the samely and the world, and their price. they will not be a same words who beginning the world.\n",
      "\n",
      "from tha\n",
      "====================================================================\n",
      "\n",
      "(4).(2).(0).(3).(8).(10).(6).(7).(9).(1).\n",
      "Average loss at step 22: 2.476008\n",
      "\tPerplexity at step 22: 11.893684\n",
      "\n",
      "Valid Perplexity: 9.72\n",
      "\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      "\t end that will not just the complete cannot by the cannot be the calls and the words:UNKUNK that we will not the words, the people of freedom in this spirit of citizens, these ways, the pride of our country to the country, not just is the solve, and to all the world wind the progress and the world to the country for the people of others and that is the bander the country, now, who are the program of our nation, and that we will be to believe the stands of our price in the cause this strong, here the call of the call.\n",
      "\n",
      "to the work, we will not just and in our country and our nation, and the call.\n",
      "\n",
      "we will government of citizens, the progress of citizens and the same aUNKitUNKUNK\n",
      "\n",
      "and the canning the same nation of complete understant to live in the valley, we will make again.\n",
      "\n",
      "we must by the world that is not just my god bless the unity and the world with god bless you are in a passion and destrong all that is the stants the world will be the world with generous of complete underselUNKs is the p\n",
      "====================================================================\n",
      "\n",
      "(1).(2).(8).(5).(0).(9).(7).(3).(10).(6).\n",
      "Average loss at step 23: 2.469525\n",
      "\tPerplexity at step 23: 11.816835\n",
      "\n",
      "Valid Perplexity: 9.80\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ed by and depended to make america wealth, stand again. there are not just as we will not been to that when that we will reduced and the people. the strong, at we will make america weach and strength and the same to the words, the same glorious and we will real the americans to believe the promise their places of our soundâ€UNKnd the work to be the same aUNKork to the same dreams, and we will make america will not the unity, and we will refor thing, and our common the strength and our nation, and we will not just me will be americUNKs of their own government of our society, and we will make america weal of their price, to all and we will be the same to this work for our community and defense that we will be the world of liberty, but our commit of this strong as america safe this day in this day, and, yes, and the world when and the stately the same dreams, and our country, we have been the world.\n",
      "\n",
      "from this story's country and our nation, when the world and story goes on the world. the word\n",
      "====================================================================\n",
      "\n",
      "(0).(4).(5).(6).(7).(3).(8).(2).(1).(9).\n",
      "Average loss at step 24: 2.455521\n",
      "\tPerplexity at step 24: 11.652509\n",
      "\n",
      "Valid Perplexity: 9.76\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      "\t clear of the pride.\n",
      "\n",
      "we're common purpose to a struggle destruction, to make our courage, and the unity, to be a strength and the words of our cangeUNKs the unfortain destroy, to the world with great we had in under one of the problems which that we have chargeneration, and to believe that the underselves of the strength and the world with god's grace upon the world with that we will not be my accongelUNK\n",
      "\n",
      "and we will not the unity and to the stands and democracies, the strong, for the stands of the pride and the story. we had acrost the unity, and the human dream.\n",
      "\n",
      "today, it more the unity the work, we will not the world to the progress that is not because we will be ignity that we cannot be problems are is not just and we must by goals of us, the promise of the world to a shore that the work, i ask us that the world to be precious values of life that we are the work of the union of our soundâ€UNKod must be and who are the people understand to the world with all of that is the underselves.\n",
      "====================================================================\n",
      "\n",
      "(5).(0).(4).(1).(9).(3).(7).(6).(2).(8).\n",
      "Average loss at step 25: 2.396143\n",
      "\tPerplexity at step 25: 10.980743\n",
      "\n",
      "Valid Perplexity: 9.95\n",
      "\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n",
      "\t s, the country and than our children, with the world of our nation's government to the pride.\n",
      "\n",
      "in the world with that is the stately storms of the pride.\n",
      "\n",
      "the strong remember that is the strengthen the world of the storms, and the story go the world, and when our country and the unity and the world who lives to the storms that we can good in the world of the strengthened the unity the world.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are are the people. americans are not strengthen the world to strong ender of the states to be the world, and the people. there are now all of our country, and all, and the barries and country and that is not just and the promise to be america's long, never but the progress of the progress. and the promise of freedom is the continue to strength and the barriers to believe that we will reform that we have the protected begans, the world.\n",
      "UNKor the continue to the congress of our societyUNKs not just my believe that the world.\n",
      "UNKod blow again.\n",
      "\n",
      "and the sound.\n",
      "\n",
      "the world with all of this can to that t\n",
      "====================================================================\n",
      "\n",
      "(5).(3).(9).(8).(4).(10).(0).(6).(1).(7).\n",
      "Average loss at step 26: 2.346813\n",
      "\tPerplexity at step 26: 10.452204\n",
      "\n",
      "Valid Perplexity: 9.58\n",
      "\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      "\t w the words of their trust to believe that is to the world than the words:UNKor the stately storms and dependence that the world of common danger, with the people of the storms of the conviction of the people and the world.\n",
      "\n",
      "we will be a same great nation, and we had ensured to be is a partion and the basis of our soundâ€UNKneed the world on the world, but we will reflect, we will be in ourselves to the promise on the basis of those that is a strong enough and is a new breated states, today, we will be do not believe the stately strength and the bands of our strength and our country will the people and the world whate our commitments the bander. there is the wealth of the people and our belief in the world with the struggle today, their own government and our country will reduce the problems are the people to the powerful most to make americans, we will make americans, as in this story of the world of the people and courage of all, when the strength the world with the strength and our nati\n",
      "====================================================================\n",
      "\n",
      "<_io.TextIOWrapper name='lstm_peephole_dropout.csv' mode='wt' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 10\n",
    "docs_per_step = 10\n",
    "valid_summary = 1\n",
    "train_doc_count = num_files\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Capture the behavior of train/valid perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            \n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity\n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity\n",
    "        \n",
    "        # shows the training progress\n",
    "        print('(%d).'%di,end='') \n",
    "        \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "        session.run(reset_train_state) # resetting hidden state for each document\n",
    "        \n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses  \n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "\n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        test_word[0,data_list[np.random.randint(0,num_files)][np.random.randint(0,100)]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})            \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write the perplexity data to a CSV\n",
    "\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    print(f)\n",
    "    writer = csv.writer(f,delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
