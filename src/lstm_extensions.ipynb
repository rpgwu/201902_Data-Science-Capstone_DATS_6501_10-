{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending LSTMs: LSTMs with Peepholes and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\software\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure president stories are downloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  ../data\\speech_1.txt\n",
      "File  speech_1.txt  already exists.\n",
      "Downloading file:  ../data\\speech_2.txt\n",
      "File  speech_2.txt  already exists.\n",
      "Downloading file:  ../data\\speech_3.txt\n",
      "File  speech_3.txt  already exists.\n",
      "Downloading file:  ../data\\speech_4.txt\n",
      "File  speech_4.txt  already exists.\n",
      "Downloading file:  ../data\\speech_5.txt\n",
      "File  speech_5.txt  already exists.\n",
      "Downloading file:  ../data\\speech_6.txt\n",
      "File  speech_6.txt  already exists.\n",
      "Downloading file:  ../data\\speech_7.txt\n",
      "File  speech_7.txt  already exists.\n",
      "Downloading file:  ../data\\speech_8.txt\n",
      "File  speech_8.txt  already exists.\n",
      "Downloading file:  ../data\\speech_9.txt\n",
      "File  speech_9.txt  already exists.\n",
      "Downloading file:  ../data\\speech_10.txt\n",
      "File  speech_10.txt  already exists.\n",
      "Downloading file:  ../data\\speech_11.txt\n",
      "File  speech_11.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a directory if needed\n",
    "dir_name = \"../data\"\n",
    "num_files = 11\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  #Download a file if not present\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "\n",
    "filenames = [\"speech_\"+format(i, '01d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt', 'speech_11.txt']\n",
      "11 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    print( filenames)\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "   # assert file_exists\n",
    "print('%d files found.'%len(filenames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file ../data\\speech_1.txt\n",
      "Data size (Characters) (Document 0) 3443\n",
      "Sample string (Document 0) ['fo', 'r ', 'my', 'se', 'lf', ' a', 'nd', ' f', 'or', ' o', 'ur', ' n', 'at', 'io', 'n,', ' i', ' w', 'an', 't ', 'to', ' t', 'ha', 'nk', ' m', 'y ', 'pr', 'ed', 'ec', 'es', 'so', 'r ', 'fo', 'r ', 'al', 'l ', 'he', ' h', 'as', ' d', 'on', 'e ', 'to', ' h', 'ea', 'l ', 'ou', 'r ', 'la', 'nd', '.\\n']\n",
      "\n",
      "Processing file ../data\\speech_2.txt\n",
      "Data size (Characters) (Document 1) 6871\n",
      "Sample string (Document 1) ['se', 'na', 'to', 'r ', 'ha', 'tf', 'ie', 'ld', ', ', 'mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'mo', 'nd', 'al', 'e,', ' s', 'en']\n",
      "\n",
      "Processing file ../data\\speech_3.txt\n",
      "Data size (Characters) (Document 2) 7320\n",
      "Sample string (Document 2) ['se', 'na', 'to', 'r ', 'ma', 'th', 'ia', 's,', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ' b', 'ur', 'ge', 'r,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'sp', 'ea', 'ke', 'r ', \"o'\", 'ne', 'il', 'l,', ' s', 'en', 'at', 'or', ' d', 'ol', 'e,', ' r', 'ev', 'er', 'en', 'd ']\n",
      "\n",
      "Processing file ../data\\speech_4.txt\n",
      "Data size (Characters) (Document 3) 6255\n",
      "Sample string (Document 3) ['mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' q', 'ua', 'yl', 'e,', ' s', 'en', 'at', 'or', ' m', 'it', 'ch', 'el', 'l,', ' s', 'pe', 'ak', 'er', ' w', 'ri', 'gh', 't,', ' s', 'en', 'at', 'or', ' d']\n",
      "\n",
      "Processing file ../data\\speech_5.txt\n",
      "Data size (Characters) (Document 4) 4540\n",
      "Sample string (Document 4) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'to', 'da', 'y ', 'we', ' c', 'el', 'eb', 'ra', 'te', ' t', 'he', ' m', 'ys', 'te', 'ry', ' o', 'f ', 'am', 'er', 'ic', 'an', ' r', 'en', 'ew', 'al', '. ', 'th', 'is', ' c', 'er', 'em', 'on', 'y ', 'is', ' h', 'el', 'd ', 'in', ' t', 'he']\n",
      "\n",
      "Processing file ../data\\speech_6.txt\n",
      "Data size (Characters) (Document 5) 6082\n",
      "Sample string (Document 5) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'at', ' t', 'hi', 's ', 'la', 'st', ' p', 're', 'si', 'de', 'nt', 'ia', 'l ', 'in', 'au', 'gu', 'ra', 'ti', 'on', ' o', 'f ', 'th', 'e ', '20', 'th', ' c', 'en', 'tu', 'ry', ', ', 'le', 't ', 'us', ' l', 'if', 't ', 'ou', 'r ', 'ey', 'es']\n",
      "\n",
      "Processing file ../data\\speech_7.txt\n",
      "Data size (Characters) (Document 6) 4520\n",
      "Sample string (Document 6) ['th', 'an', 'k ', 'yo', 'u,', ' a', 'll', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 're', 'hn', 'qu', 'is', 't,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'di', 'st', 'in']\n",
      "\n",
      "Processing file ../data\\speech_8.txt\n",
      "Data size (Characters) (Document 7) 5961\n",
      "Sample string (Document 7) ['vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' c', 'he', 'ne', 'y,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'pr', 'es', 'id', 'en', 't ', 'ca', 'rt', 'er', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'cl', 'in', 'to', 'n,', ' m', 'em', 'be']\n",
      "\n",
      "Processing file ../data\\speech_9.txt\n",
      "Data size (Characters) (Document 8) 6680\n",
      "Sample string (Document 8) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'i ', 'st', 'an', 'd ', 'he', 're', ' t', 'od', 'ay', ' h', 'um', 'bl', 'ed', ' b', 'y ', 'th', 'e ', 'ta', 'sk', ' b', 'ef', 'or', 'e ', 'us', ', ', 'gr', 'at', 'ef', 'ul', ' f', 'or', ' t', 'he', ' t', 'ru', 'st', ' y', 'ou', ' h', 'av']\n",
      "\n",
      "Processing file ../data\\speech_10.txt\n",
      "Data size (Characters) (Document 9) 5973\n",
      "Sample string (Document 9) ['th', 'an', 'k ', 'yo', 'u.', ' t', 'ha', 'nk', ' y', 'ou', ' s', 'o ', 'mu', 'ch', '.\\n', '\\nv', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bi', 'de', 'n,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'me', 'mb', 'er', 's ', 'of', ' t', 'he', ' u', 'ni', 'te', 'd ', 'st', 'at', 'es']\n",
      "\n",
      "Processing file ../data\\speech_11.txt\n",
      "Data size (Characters) (Document 10) 4223\n",
      "Sample string (Document 10) ['ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 'ro', 'be', 'rt', 's,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'ob', 'am', 'a,', ' f', 'el', 'lo', 'w ']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    # make all the text lowercase\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Breaking the text into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    # Creates a list of lists with the bigrams (outer loop different stories)\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries (Bigrams)\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61868 Characters found.\n",
      "Most common words (+UNK) [('e ', 1945), (' t', 1623), (' a', 1378), ('th', 1378), ('s ', 1110)]\n",
      "Least common words (+UNK) [('yâ', 1), ('tp', 1), ('”f', 1), ('”u', 1), ('kf', 1), ('-l', 1), ('40', 1), ('\\nl', 1), ('hm', 1), ('ja', 1), ('n:', 1), ('zo', 1), ('uy', 1), ('r:', 1), ('ky', 1)]\n",
      "Sample data [78, 15, 250, 63, 298, 3, 16, 33, 24, 7]\n",
      "Sample data [63, 121, 32, 15, 34, 0, 103, 117, 17, 0]\n",
      "Vocabulary:  351\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the RNN. More specifically the generator breaks a given sequence of words into `batch_size` segments. We also maintain a cursor for each segment. So whenever we create a batch of data, we sample one item from each segment and update the cursor of each segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\tpr (97), \tr  (15), \the (6), \te  (1), \tou (22), \n",
      "\tOutput:\n",
      "\ted (49), \tfo (78), \t h (50), \tto (32), \tr  (15), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\ted (49), \tfo (78), \t h (50), \tto (32), \tr  (15), \n",
      "\tOutput:\n",
      "\tec (114), \tr  (15), \tas (88), \t h (50), \tla (144), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tec (114), \tr  (15), \tas (88), \t h (50), \tla (144), \n",
      "\tOutput:\n",
      "\tes (26), \tal (58), \t d (59), \tea (52), \tnd (16), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\tes (26), \tal (58), \t d (59), \tea (52), \tnd (16), \n",
      "\tOutput:\n",
      "\tso (129), \tl  (51), \ton (21), \tl  (51), \t.\n",
      " (115), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\tso (129), \tl  (51), \ton (21), \tl  (51), \tou (22), \n",
      "\tOutput:\n",
      "\tr  (15), \the (6), \te  (1), \tou (22), \tr  (15), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM, LSTM with Peepholes and GRUs\n",
    "\n",
    "* A LSTM has 5 main components\n",
    "  * Cell state, Hidden state, Input gate, Forget gate, Output gate\n",
    "* A LSTM with peephole connections\n",
    "  * Introduces several new sets of weights that connects the cell state to the gates\n",
    "* A GRU has 3 main components\n",
    "  * Hidden state, Reset gate and a Update gate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "\n",
    "Here we define several hyperparameters and are very similar to the ones we defined . However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "batch_size = 64\n",
    "#num_unrollings = 50\n",
    "num_unrollings = 100\n",
    "\"\"\"\n",
    "add drop out only for peep holes\n",
    "\"\"\"\n",
    "dropout = 0.0\n",
    "\n",
    "#dropout = 0.2\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.1:\n",
    "    filename_extension = '_dropout'\n",
    "    print(filename_extension)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test input (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "valid_inputs = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters and Cell Computation\n",
    "\n",
    "We define parameters and cell computation functions for all the different variants (LSTM, LSTM with peepholes and GRUs). **Make sure you only run a single cell withing this section (either the LSTM/ LSTM with peepholes or GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard LSTM\n",
    "\n",
    "Here we define the parameters and the cell computation function for a standard LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.csv\n"
     ]
    }
   ],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "algorithm = 'lstm'\n",
    "#algorithm = 'lstm_peephole'\n",
    "#algorithm = 'gru'\n",
    "\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "print( filename_to_save)\n",
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTMs with Peephole Connections\n",
    "\n",
    "We define the parameters and cell computation for a LSTM with peepholes. Note that we are using diagonal peephole connections (for more details refer the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "ic = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "fc = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],0.0,0.01))\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "oc = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],0.0,0.01))\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "algorithm = 'lstm_peephole'\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "# Definition of the cell computation.\n",
    "def lstm_with_peephole_cell(i, o, state):\n",
    "    '''\n",
    "    LSTM with peephole connections\n",
    "    Our implementation for peepholes is based on \n",
    "    https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf    \n",
    "    '''\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + state*ic + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + state*fc + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + state*oc + tf.matmul(o, om) + ob)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Units (GRUs)\n",
    "\n",
    "Finally we define the parameters and cell computations for the GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Reset gate: input, previous output, and bias.\n",
    "rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "rh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "rb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Hidden State: input, previous output, and bias.\n",
    "hx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "hh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "hb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Update gate: input, previous output, and bias.\n",
    "zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "zh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "zb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "algorithm = 'gru'\n",
    "#filename_to_save = algorithm + filename_extension +'.csv'\n",
    "\n",
    "# Definition of the cell computation.\n",
    "def gru_cell(i, o):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    reset_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rh) + rb)\n",
    "    h_tilde = tf.tanh(tf.matmul(i,hx) + tf.matmul(reset_gate * o, hh) + hb)\n",
    "    z = tf.sigmoid(tf.matmul(i,zx) + tf.matmul(o, zh) + zb)\n",
    "    h = (1-z)*o + z*h_tilde\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM/GRU/LSTM-Peephole Computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "  state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "# Note: there is no cell state for GRUs\n",
    "for i in train_inputs:\n",
    "    if algorithm=='lstm':\n",
    "      output, state = lstm_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "    elif algorithm=='lstm_peephole':\n",
    "      output, state = lstm_with_peephole_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "    elif algorithm=='gru':\n",
    "      output = gru_cell(i, output)\n",
    "      train_state_update_ops = [saved_output.assign(output)]\n",
    "        \n",
    "    output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "valid_output = saved_valid_output\n",
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "  valid_state = saved_valid_state\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "if algorithm=='lstm':\n",
    "    valid_output, valid_state = lstm_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "    \n",
    "elif algorithm=='lstm_peephole':\n",
    "    valid_output, valid_state = lstm_with_peephole_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "elif algorithm=='gru':\n",
    "    valid_output = gru_cell(valid_inputs, valid_output)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output)]\n",
    "\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(valid_state_update_ops):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "if algorithm=='lstm':\n",
    "  test_output, test_state = lstm_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "elif algorithm=='lstm_peephole':\n",
    "  test_output, test_state = lstm_with_peephole_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "elif algorithm=='gru':\n",
    "  test_output = gru_cell(test_input, saved_test_output)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output)]\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(test_state_update_ops):\n",
    "    test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "We calculate the training loss of the LSTM here. It's a typical cross entropy loss calculated over all the scores we obtained for training data (`loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies(train_state_update_ops):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch). But since GRU doesn't have a cell state we have a conditioned reset_state ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "    # Reset train state\n",
    "    reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "    reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = tf.group(\n",
    "        saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01)),\n",
    "        saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.01)))\n",
    "    \n",
    "elif algorithm=='gru':\n",
    "    # Reset train state\n",
    "    reset_train_state = [tf.assign(saved_output, tf.zeros([batch_size, num_nodes]))]\n",
    "\n",
    "    # Reset valid state\n",
    "    reset_valid_state = [tf.assign(saved_valid_output, tf.zeros([1, num_nodes]))]\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = [saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using the Adam optimizer as it is one of the best optimizers out there. Furthermore we use gradient clipping to prevent any gradient explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for decaying learning rate\n",
    "gstep = tf.Variable(0, trainable=False)\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# Decays learning rate everytime the gstep increases\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Sampling to Break the Repetition\n",
    "Here we write some simple logic to break the repetition in text. Specifically instead of always getting the word that gave this highest prediction probability, we sample randomly where the probability of being selected given by their prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "  '''Greedy Sampling\n",
    "  We pick the three best predictions given by the LSTM and sample\n",
    "  one of them with very high probability of picking the best one'''\n",
    "  best_inds = np.argsort(distribution)[-3:]\n",
    "  best_probs = distribution[best_inds]/np.sum(distribution[best_inds])\n",
    "  best_idx = np.random.choice(best_inds,p=best_probs)\n",
    "  return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM to Generate Text\n",
    "\n",
    "Here we train the model on the available data and generate text using the trained model for several steps. From each document we extract text for `steps_per_document` steps to train the model on. We also report the train perplexity at the end of each step. Finally we test the model by asking it to generate some new text starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n",
      "(10).(4).(1).(5).(7).(3).(6).(8).(9).(0).\n",
      "Average loss at step 1: 5.318128\n",
      "\tPerplexity at step 1: 204.001534\n",
      "\n",
      "Valid Perplexity: 279.74\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      "\t er tt the the tht t tht t e t t tht e e e t t e e t e ththe t e t e the t e t the the tht e t e t t tht e e t tht t t tht t e e the t t the e ththt t e the tht ththe t the the t e the e t t t e t t t e thththt t t t t e t t e e t e tht e e e e e e e e t t ththt e t t e tht e the e thththe the t e e t t the t e the tht e t the e e e e ththe ththt e ththt e t e t e e t e ththt t the t t tht e t e t tht e ththe t the ththththe t tht tht t the e the e t t e e ththe e e the e the e e t tht e t e t e the t e tht e t thththe t tht t e t tht e the t e e t the t e tht tht t e e e e t tht e e tht t e t t the e t t e e e tht e ththe t ththe ththt e ththe t t the e t e e t the e the the e e e e e e e tht e e t t the t the t e e t tht tht the e e e e e e thththe e t e t tht t e the e ththt t e t the e t t e thththe e ththe e e t the e the e e e e thththe e e e t t t ththe thththt the t thththt e e e t e the t tht the tht e e e ththththe e the e t e thththththe t e e the e t t e ththe the ththththe th\n",
      "====================================================================\n",
      "\n",
      "(8).(4).(2).(6).(3).(5).(7).(0).(10).(1).\n",
      "Average loss at step 2: 5.188997\n",
      "\tPerplexity at step 2: 179.288600\n",
      "\n",
      "Valid Perplexity: 234.68\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      "\t th t the te  a a t a t te  t te  ae e  te e  t ae  t a a t ae  a a ae  te  t te  te  t te  a te  ae e  t t ae e  a te  a ae  ae  te  a a ae  ae  t ae  a ae e  ae e e  t ae  a a te  t a t ae  ae  te e e  t a t ae e  a a t te  t a te  a a ae  a ae e e  a te e  te  a a t a t t t te e e e  a t a t t a a te  a te e  a t ae  ae e  a a te e  t ae e  a a t te e e e  t ae  ae  te  te  a ae  ae  ae  ae  t te  te e e  a ae  a t a t t t te  t a a t t te  te e e e  a te  ae  a ae  a te e  a a a ae  a a t a t ae  a t t a t t a ae  t t a t te  ae  t t t te e  ae e e  te e  t te e  te e e  a t a te  t t t t a t t t te e  t a te e  te  t ae e  t ae  t t a te  te  a ae e  te e e  a t a a t ae  te  ae  t t t t a t ae  a t a te  a t te  t a ae  t a ae  a te  te  te  ae e  t te e  a ae  te  te  a a a t ae e  te e e  a a te  t a t a a t a a ae  t a a a t a t a a ae  a te  t ae  t ae e e e e  t t a t a a t ae  ae  te  a ae  t ae  a t te  ae e e  a te e e  ae e  t t t t te e e  t a a t te  a t t t ae  a a ae  a\n",
      "====================================================================\n",
      "\n",
      "(8).(7).(1).(10).(5).(2).(6).(4).(3).(0).\n",
      "Average loss at step 3: 5.060131\n",
      "\tPerplexity at step 3: 157.611125\n",
      "\n",
      "Valid Perplexity: 167.34\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "\t his ofe th ae the and th and ath ae the an the the ande e e an the ae and and ane an thee ane ane th ath and and thee e ane ane and ane ane an the and thee and th tthe th and thee ane and th ath thee and th thee th and ath ae e ane e e th to e and th the the and te ane and the th ande the e the the th the and and ae th the athe e ane th ae the and e the th te e e the an the ae an thee ane the e the e the an thee and e th to the and the e e th thee ane th the ande ane e and ane th ande the and the th te th the tthe e th the the the ande an thee e the the th te and and the and the an to e the e ane and and e the e the the and ane th ae e e the ane ane ane e ane ane an tthe an tth thee e th thee and ane e e an the and ande the the e and the e ane ane the the the e e the ane the ane the th ande e th ande th ath ae e th the the ae ane e e th ande the th ae e an thee th athe the th the and ath te the the the e an thee e ane th thee the an tth ath the tthe e th ath athe the e ane an the and the\n",
      "====================================================================\n",
      "\n",
      "(3).(10).(1).(4).(2).(5).(7).(9).(0).(6).\n",
      "Average loss at step 4: 4.626366\n",
      "\tPerplexity at step 4: 102.142224\n",
      "\n",
      "Valid Perplexity: 88.07\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "\t tte of the will that to our the the to the sto and and the we and and to we the the the that that ant and and ther the sthe to the we the to and that the the the the and that and and we the the and the and ther and the sour and the to the the sour the and the to our and ame the sto are and we to to our the the the and to to and and the and to the sto and witt to the we the to the sthe and and the to and and to ane to the sour to our and the wore the the the to to the the and to the sour to to to and we the the to to the the to of the and to we and and ther and the sto and the we to our and wore ther the sour the and the ther that to we to and we ther to and the the and ther and the sto to that to that we that to and and the the the the and the to of and the to the to our the there the ther and to and the and the the the the and that the the we and and and to the sto the sthere the the the and and ther to the the to and the wore and the and to are thas to wor the we to and wore the to and\n",
      "====================================================================\n",
      "\n",
      "(6).(9).(10).(2).(5).(4).(1).(3).(0).(8).\n",
      "Average loss at step 5: 4.100037\n",
      "\tPerplexity at step 5: 60.342545\n",
      "\n",
      "Valid Perplexity: 49.85\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "\t es our seed the the of of thation, and and we of amere to and of and we thation the the of of this that we the people the to we the thation we will ther of our the world the the world and the thation the world of the people the of of our thation the world the to the preat our the shat and the of our will and the of our wor the shas of our we ther of that the people and the world and the of our wort that we that to the that of thation the of the porat of this we the world and the the of our we world the the the of of the of of the world the of and will thation the this the will of and the por the seen that the preation and the of of the world of and and wore of the will the world the the the that of our of to the world the the world thations the we and and and the world that the seet the the of the will the the of and to the this and and amere to our of to the of the shat and that the pred the world the of and and the seen of and the wore thations and to ther of that the we the the thatio\n",
      "====================================================================\n",
      "\n",
      "(10).(0).(7).(8).(9).(4).(2).(3).(6).(5).\n",
      "Average loss at step 6: 3.696231\n",
      "\tPerplexity at step 6: 40.295148\n",
      "\n",
      "Valid Perplexity: 32.00\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ny that the world thation this to se and that the this and americans ame the strengte the that the seed the the world of the strength and the work and americans and to our that the stake of a stress our sour of to the to strent thation the to the cone the this and streace and the strence and are of the world that the peates the worls of to that and strength this the cour the the world that to that the seedom this to that and to our the world of and to se are of the work the seedom this so of of our the word are to the words the world to are ament the stand thas to seen of of our of of our the world of and to the the world the this that the world this and and and streace the cone of of the work of a stress our sour common the strest and thation this strence, and the world of amerity and so of to this the come of to the the work and to our this the stall of a strong of this the preces of the prom and the thation and the to americans of a unity the work the conter the to seed to the stres, \n",
      "====================================================================\n",
      "\n",
      "(8).(4).(7).(3).(6).(1).(9).(0).(10).(5).\n",
      "Average loss at step 7: 3.443291\n",
      "\tPerplexity at step 7: 31.289768\n",
      "\n",
      "Valid Perplexity: 20.60\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ice the world of the strest of thas of the powericand our nation, and the sames of americans of the world of and and the people and our commity the sames and the words of the strent of to the world of all the powerican american ough and americans and the stress of and the people of our people to be with the strence and to that we the wordd the will the stry to all all the will that the people and of american of on the sall and and to the world to to will to with we must will not for in the stry of the will ber the world they we will to our the will be of and the strest of the we world of the strence, we world that of our stry of the powerican the will the same as ing and, to the words to the will make to will be to but the stry and to ament and and america will now the workd that the strength the world the world, and and and and the stres, of the stand and the strength the will be to to they to serve to be will that the words to and will make and the will mall ament and all the world ame\n",
      "====================================================================\n",
      "\n",
      "(8).(3).(6).(4).(0).(1).(5).(2).(7).(9).\n",
      "Average loss at step 8: 3.269064\n",
      "\tPerplexity at step 8: 26.286714\n",
      "\n",
      "Valid Perplexity: 18.93\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      "\t gue the cause the cannot the presions the world of and preces that american our compless the prosing and the strength and ought the storld that ours of this strength and our nation, the call the we cannot the commons of the work to berty and destrong of freedom and our commit of this do not best and and the people the work to best and and stries of the prosity our coughs of or serve and we americans all our seen to the prosity of freedom our country and our nation to the words the works of our people the world to belies of the strength and our nation, and the world the works of the strong and our nation to the have and to serve and we world and the progrest of of our nation of our nation the strength and ded ing the problems and to the work of thation the words.\n",
      "\n",
      "this the contion of freedom.\n",
      "\n",
      "the cannecion of and people the world the world that we can tory aments and the call the prosity, and we and the prosion of the world, we the world that we have the serving to best and the seen of t\n",
      "====================================================================\n",
      "\n",
      "(0).(7).(3).(6).(2).(4).(8).(1).(5).(9).\n",
      "Average loss at step 9: 3.096363\n",
      "\tPerplexity at step 9: 22.117365\n",
      "\n",
      "Valid Perplexity: 15.93\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ith and our nation our comethis of americans americans, and the presis the precents and destrom of they with the world our people and our nation of our journey of the people of the strong of our meach to best people with americans and somet us not us our counts the cannew that the preserica restress of the problems the meart of the peace that will the world the precional one the work to the cannot to this americans and time that ament these will the preserty our people and destrong to make and american of the contizen of our prosing to the call the progrest of our country, and the words of time and destross to the world that american that will respection they are of the preces of one and they are still the call be to americans, the world the words of comple in to serve the work the people of our country. we more to a stries to to berty this america's greation that world to the world that we must american and the precestion that those the values, the world of americans and the people the \n",
      "====================================================================\n",
      "\n",
      "(7).(1).(0).(8).(2).(6).(4).(9).(10).(3).\n",
      "Average loss at step 10: 2.964279\n",
      "\tPerplexity at step 10: 19.380725\n",
      "\n",
      "Valid Perplexity: 12.59\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      "\t nt our couration and the prity and to mall to thisk your common agains. and the caust they are that that will make to the world by the we will be the words of the prosing, and that we will all but the service and the pries, and the world, the stand to thisk your great and that is strength to the prity and and the work of our complace, with the work and with will the for the stated of the states of all be wime american the world of the stake america we will belUNKUNKUNK\n",
      "\n",
      "we will make all be that is and directs of one ameries, we will make america with and bless and dinger, and the work as and the words of the people, with the work agath and the strengtess and that all belieng to the unith to all be the caust of our strengther the work of this jobs.\n",
      "\n",
      "the words to belies of to this will make the sere to belUNKUNK and all the strues of the will make american our nation, we will make of our natizens, the cause the sames call the serviction and we america we will the will make america will been wil\n",
      "====================================================================\n",
      "\n",
      "(7).(9).(2).(4).(6).(1).(3).(8).(5).(0).\n",
      "Average loss at step 11: 2.826727\n",
      "\tPerplexity at step 11: 16.890091\n",
      "\n",
      "Valid Perplexity: 12.36\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  be the work on a greater that of our coments and we will not that the more that the power the preation of our canizent is to a revernment, the world the world the people to been the preatest of the presibility of our chare that american a the world with americans our have of the strengthis rements of our progres, we have with our promise of a nerabled, we work yoUNK that we work for this stated the world of all their new of that of thate of our platent, a new that we are not the words with to serve in the mart for the peace of to that americans, the prestiont the peace of their national onUNKher the world.\n",
      "\n",
      "the words to and the people of american of american that the starity, the words.\n",
      "\n",
      "those of our nother the work of this there to the worlUNKUNKUNKUNKUNKUNKUNKlet we will the breek of and there that we will not the worlUNKUNKUNKUNKUNKt that we will all to the preservers will this strong and the works of our prosidents to be respone of time that we have to the promise of the must of the people and ou\n",
      "====================================================================\n",
      "\n",
      "(3).(4).(10).(0).(5).(8).(2).(7).(6).(1).\n",
      "Average loss at step 12: 2.712988\n",
      "\tPerplexity at step 12: 15.074251\n",
      "\n",
      "Valid Perplexity: 13.52\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ship of the undersities.\n",
      "\n",
      "the work of our libeliver the said, in this story the untry and our commitients the unity of the prity of our life.\n",
      "\n",
      "to the worker to be america's chat the world of courance and restorng on the canning to the staricans, and the unity of the country. we are america's grander to our belief in the hose work you to be under nation, that in that that we reflecting to the country. we will not the world to the standing our liberty and the under in one country, and we will the under of can dent our common our country, and the undersing to the standing this just an the consions of our contion, and the undersing and divisions of the couldry as the called by that in the unity of our can sound the couratime, UNKUNKUNKUNK and consibility and strength that which the state to the world and that this reation of our cans to the strengthis story's danger. we we do seed that is the world.\n",
      "\n",
      "and the world in servision the carllents to make of this declage and this respect the world of o\n",
      "====================================================================\n",
      "\n",
      "(5).(4).(1).(6).(10).(7).(9).(0).(2).(8).\n",
      "Average loss at step 13: 2.584845\n",
      "\tPerplexity at step 13: 13.261238\n",
      "\n",
      "Valid Perplexity: 11.68\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ely of our cances that that the under of the sour time and the struggles in the world of and we the unity, are there americans, we will make american this we the world of our courance of our service and enduration of our historm that the would of all our sound. it of this spirents to that is the states of the strongth the unity, we have will make american the world of the people to the had enal lives this convictions, we must the world they wite the worlUNKs and the world of the soundâ€UNKn a belize as we remorn the world with all be that the humity and the worlUNK\n",
      "\n",
      "the world to be that is a strengs to the power to our best by the world to service of this congress.\n",
      "\n",
      "the world, the samerican soundâ€UNKUNKUNKe this day of freedom and the world.\n",
      "\n",
      "we will make a struggle of our nation of our history of our complect, we have the world to still be to a strength and our journey, are the world, fortUNK\n",
      "\n",
      "we had every citizens to the humit must that will not the world to the strught our presibility and w\n",
      "====================================================================\n",
      "\n",
      "(9).(1).(6).(5).(0).(4).(3).(2).(8).(7).\n",
      "Average loss at step 14: 2.522067\n",
      "\tPerplexity at step 14: 12.454315\n",
      "\n",
      "Valid Perplexity: 11.64\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "\t my faith and common danger. and that with the under interest of the unities to service and whose who america's belies to the stand and when the called that the call.\n",
      "\n",
      "they are that is not the work on that we are untry, all that is all, and to the work to a societyUNKand the words of our sound than that will make and we must be that in our country. and we much the stand and with that we have state an untrues to a century and we must to the world to the stand the carlding to those who are those who world are strong of the world and the progress of the commons that this day, and our belition, and the unities of common the internations or the world they wort of our history and the source.\n",
      "\n",
      "the world. and we will be the unity, with the worlUNKs the unity and democracies to the prisityUNKUNKUNKhat we will not the some, and demands and our country, with our liberty and the enemies. we have been undershistory that and alled, in somple is the strengthered the worlUNKs and democracy and the unity the un\n",
      "====================================================================\n",
      "\n",
      "(1).(2).(5).(6).(9).(0).(7).(3).(4).(10).\n",
      "Average loss at step 15: 2.401228\n",
      "\tPerplexity at step 15: 11.036721\n",
      "\n",
      "Valid Perplexity: 12.77\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      "\t place, we will read ing the world wing god bless you, and the world.\n",
      "\n",
      "and there and the world to believe and the will make america will never the sacred by and the words:s of life. is our own the says, UNKUNKUNKUNKhat we had to thisk you today, with our country and the same, can beyou are now been than community and with the same glose and divisions the world and the sounion, and will make are spirit is conviction and our country and the said, and the world is still reaUNKs and that we must and the world is now we will not just as well and america. we have changed the said, UNKUNKUNKhave and the prosideal, and we will make america will many work until our country and the same, UNKUNK\n",
      "\n",
      "we has weach is the world will reforms, we have chill the worlUNKs we will not be my accomplishmentUNKUNK they are beliefe and the world of their our wour nation to the sacrifice and we will make the world of this day of our country and there are unity, with the same, call by or every america, not we america will be ame\n",
      "====================================================================\n",
      "\n",
      "(8).(7).(0).(10).(6).(9).(2).(4).(3).(5).\n",
      "Average loss at step 16: 2.287692\n",
      "\tPerplexity at step 16: 9.852177\n",
      "\n",
      "Valid Perplexity: 11.60\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  your nation, but be american to the strong, hisher with a new to us and that is the power and that a place of life and divisions of life, are that is service that our children and sometten will the for the hare of our mostimerive that is a compassion is the strength to service and when that we can fellow and their every, we must be a more will be that we have been mightent to the unity and that in this is a communities of the meach in our time. there and divicts ned to an untry and our work is done. the world with god bless you, and the sound. their hope that one day that the some of the some of the deepest withing to a societyUNK€”not the work is the world, by that all the way, we have children of our nation, the world and our journey to all that is the stand and each others are that is the strength and the world will to hear a cone of they are unity, and their own government beginning of the power and the human in a place and the world of the hope of our mememrers of our nation of the \n",
      "====================================================================\n",
      "\n",
      "(5).(6).(7).(4).(1).(9).(3).(10).(2).(0).\n",
      "Average loss at step 17: 2.199160\n",
      "\tPerplexity at step 17: 9.017438\n",
      "\n",
      "Valid Perplexity: 11.61\n",
      "\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      "\t his the words of our people, with our own ways, UNKUNKhat we will may heart the worlUNKs and the worlUNK\n",
      "\n",
      "they will be will stand in the people, that is the world and whope and the placentUNKUNKUNKhat we we cannot yet strive of our people to act on this dream.\n",
      "UNKet this our strength. we america great will not be we do on interests with our courage of new togeness, and they will befor the words with all our people.\n",
      "\n",
      "the americUNK will not believe that the stands of our national live of our nationUNK\n",
      "\n",
      "so muse that the people of the people to the people of all the world and to all of the people to the mare to act of the persleng, the wouls with americans, we will make america will the worlUNKs and the some of our country with the will of our people to all on the work and strong, an our own UNKUNKhand we are of our people to the pardence of our membered the words of the meaning of the unitUNKs because the unities of the people of the strong and diresionUNK€”not on the parting to all the worlUNKs and ble in\n",
      "====================================================================\n",
      "\n",
      "(1).(9).(6).(10).(7).(8).(5).(0).(2).(3).\n",
      "Average loss at step 18: 2.150282\n",
      "\tPerplexity at step 18: 8.587281\n",
      "\n",
      "Valid Perplexity: 12.80\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      "\t spiritional policies which is not be a soldier, and we will being the world, the service, to all of the world to be said, but t forty for the history and UNKith and our bestory of a societyUNK we are now to a now the power and standing american dream.\n",
      "\n",
      "and when you have be said, and we must unity, witall the world this dream.\n",
      "\n",
      "and always, america's greaters and to shose our soundâ€UNKn unity, dayer again. we will been the hear for the back of freedom. in our journey, with the world an dreat one with our belief in an undiminished, they are promise of the strong of our hopeful, and the endence and dediversitUNK\n",
      "\n",
      "the underty and the world to be the undersUNKes by the unity, wheness, our best efforts of our country of our mort is so viduce free ourselves. we will beflead to the world and the world with our country, and with the progress.\n",
      "\n",
      "the world and the world of service that we will make to the unity, which is this world and purseful our grees the world of a new sucless they are of the promise\n",
      "====================================================================\n",
      "\n",
      "(2).(9).(8).(4).(5).(1).(3).(7).(10).(0).\n",
      "Average loss at step 19: 2.057660\n",
      "\tPerplexity at step 19: 7.827635\n",
      "\n",
      "Valid Perplexity: 13.03\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  must that it hope on the same, UNKs accessinisher.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will make to do not weary the matchare the unities. when theigh against day, and the enemiers and whose who woury that we had ensured in our staing and society and unitely day, intory, we will make america great agaries will be a seedom, there american this spirit of our societyUNKUNKUNKhat we had ensured history act that with the same dreams, and they fill the womed the work of this dream our own life is and each otUNKUNK we do not mean that we will be upon will be my accept to our nation whear a new courage work untiUNK we have been this country, we along aUNKount and their partic life. we will make america wealthy again. we will make america great with the breath our country, in prosidence of our nation's continuing through the will be weary in well be to the strisgth therUNK and throughout is as well and the people of their time.€”has we will not being the word of countries and the same not because there us the worlUNKs and the same and p\n",
      "====================================================================\n",
      "\n",
      "(4).(1).(7).(5).(0).(9).(8).(3).(6).(2).\n",
      "Average loss at step 20: 1.969951\n",
      "\tPerplexity at step 20: 7.170327\n",
      "\n",
      "Valid Perplexity: 13.34\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  of our nation, but be americans, we must ended in our common publice. is our nation's hope that our dememration of interest faces of our lives and every lifens, and the worlUNKs been americans bas wathen that when in the parting of the songe to servitain our country in the values of the world the world who fill.\n",
      "\n",
      "there courance of other we have america will.\n",
      "\n",
      "on the stand against andolities to believe they are befored that the progress to a more to the strues of our societyUNKust on all these is this spirit, so that you all of this service. we contillent to those who are all, is to unity, dutUNK, for the government and statest citizens to all our continuuman cannot that we had interests will been this storm.\n",
      "UNKod bless you all of this century, let us go for those who find the power and consibility, but the words of the spirit is progress that we had interder the hards and the partic interest duties of those who are and will be such tory is a gratter ing today. aUNKe not judged in our times \n",
      "====================================================================\n",
      "\n",
      "(1).(8).(7).(3).(10).(4).(6).(5).(0).(2).\n",
      "Average loss at step 21: 1.856962\n",
      "\tPerplexity at step 21: 6.404254\n",
      "\n",
      "Valid Perplexity: 13.89\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  people of the human we must answer the stard of conviction that marken ince time and the hope of thericUNKs must a bear that we had but never believe in earth.\n",
      "\n",
      "things with god's helUNK we must and strength and our children, with a new strengthened the world and our commitment and when the world an stand to responsidependUNK\n",
      "\n",
      "fereedom we have been the american dream.\n",
      "\n",
      "today we do no wards for our forget: the world to be said, but as we have there is a charged the world an dream of a national poverty, and not just my goalUNK--and the call of ing promose of our most prossityUNKUNKshor, the new deepess of our nation's continuing moral strength and the poor of and productive work to the stately, today, to the barror of our liberty. and the people of a more preserved to promise of a nation, but let us all every cities against the words of mich instressest is truly, today and we must beingness. and the powerful, for the world of americUNK and the powerful, for as the canning that on this way and wit\n",
      "====================================================================\n",
      "\n",
      "(5).(2).(9).(7).(6).(1).(3).(4).(0).(8).\n",
      "Average loss at step 22: 1.755867\n",
      "\tPerplexity at step 22: 5.788464\n",
      "\n",
      "Valid Perplexity: 14.44\n",
      "\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  to be muse that it is to the tested the world and purposeful our country to service and this madeness. we will make the can an enturn bacts to mankind, to heare that we with patsition, but we will make america, well alled, we will make america, reasone of our country in the basis of our citizenship is no longer a new work of freedom and destroy the welllet us an abral us are not will strive faith and there are strant us can be today. anot that the unfort, and american that will not be my accomplishmentUNKbut the affirmation of our societyUNKUNKUNKhat we had been a starting our citizens, let us remember that we have the ame journey of our nation's continuing moral strength and regimpliess the basis of our nation's continuing muction that merica's bright flame of freedom which is not best dower to the peace and the poleUNKity from the law and challenges of our trual there is now there is new power the congress to complese we are not to the poold and ther demrengd the problems whenew americUNK a\n",
      "====================================================================\n",
      "\n",
      "(3).(1).(2).(0).(10).(9).(7).(8).(4).(6).\n",
      "Average loss at step 23: 1.697571\n",
      "\tPerplexity at step 23: 5.460667\n",
      "\n",
      "Valid Perplexity: 14.24\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ide today the world to so vision these words aUNKe defended or a staken and challenges and discipline. we will make america wrole of from its c renemial this day we will shave we must answer re america's belp nor someetimity and comeany to be said, but task to out anot where all, our nation, for we comengething abeat under to an again.\n",
      "\n",
      "we see to than in this spirit that their own losese willow be done. it is the affirmation of our cantly to act then wething the some of our country. and the price in our owth our country, we will bring the work of freedom.\n",
      "\n",
      "we relies of our nation resses under the safely and demand to liveUNK\n",
      "\n",
      "the part to be remore to a new jourse of life.\n",
      "\n",
      "we will mainhis together as abort to those nations with compased, we refore of the price and times again. we will make america goes on and society we will common fronce but our nation's conting to shore with those who make our canssen americans was spirit of hishinging. a canllenged in demsessends to make the daramer the\n",
      "====================================================================\n",
      "\n",
      "(5).(8).(2).(10).(6).(9).(0).(4).(1).(3).\n",
      "Average loss at step 24: 1.676428\n",
      "\tPerplexity at step 24: 5.346427\n",
      "\n",
      "Valid Perplexity: 14.28\n",
      "\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      "\t zens with the basis of opportunity. from that we have the words: you all, we must do not weary as well be dom in our citizens and the world to be said, but this depence americans that ourshis compuntoied in this we here people of a nation, and we will make america are hasUNK\n",
      "\n",
      "the precise the crace and this century. and different. and that is a call of service to on and difelse we must be done to the world, that great for our work, with the faith and will not be my accomplishmentUNKbut as americans are greaters and to be said a singlelious and we must answer the call of hildorn and hichill manytain to because we have the world to such to be said, but the people, the world is do america fills time. we refored ourselve you to shand will be in its character nation, and a passion and the world.\n",
      "\n",
      "we must the some of the world to serving that we have childing, we rained it is no again. we mill endures and hed, to responsion the law and challent al strength and the summit of the world is dow i wil\n",
      "====================================================================\n",
      "\n",
      "(7).(1).(0).(2).(9).(10).(4).(6).(8).(5).\n",
      "Average loss at step 25: 1.596979\n",
      "\tPerplexity at step 25: 4.938091\n",
      "\n",
      "Valid Perplexity: 14.07\n",
      "\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ross our citally which is the end to service and a society must be nation, the price and every citizen the same glose us aUNKold. thereUNKest and simple can service. the new world. to free this is the spirit of his ecountry, we all dom interriv to all. the compart that americans.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "togething to all one. we will not is the spirit of their price and that is still you, and governments the world, but the hear the words. the same ceasUNKe persing to rest would be seek a of our history also has whentunity of our lives and in us,her a plains with the dignity of freedom, an us nerow that we had built a last, for those above your courage, the seases of our courage and with god's helUNK the same diviction of our citizens and ity years on endh and peacy and our children, ke a societyUNKUNKUNKhat we had ensuren every tet sense timUNK€”not only with faiters. we must answer a new century, these unity the progress that make those wherfimon day. and all the world's greater, and we will bring bateerstances of the\n",
      "====================================================================\n",
      "\n",
      "(1).(3).(9).(6).(7).(8).(5).(10).(0).(2).\n",
      "Average loss at step 26: 1.577257\n",
      "\tPerplexity at step 26: 4.841658\n",
      "\n",
      "Valid Perplexity: 14.58\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ervers and the earth.\n",
      "\n",
      "i will never before embrace again. we will not be make and the words whate our strong of freedom and destory of our societyUNKUNKUNKhat we had enabled time and women to their prights, and our children, with a new strents to peUNKorm gress the story of our nation's conging this werstance in the valley from the world to belssion and our children's childreat old and spirit of herith arach, no our nation's grand again. we will make america wealthy again. we will make america great again. that the promise of our nation where comens, they are of the struggle destruction.\n",
      "\n",
      "hat chill enourage. the worlUNKs free hardshiUNK my and the same for engr that it the author of themselves in this spirit of unistant with alls of lifelentaintestd acts but the there in a greater tolerance, and time and greaters with soled it more this dream on and generoting of a new beliefing heart. and their price and equal to all of our mostinates to pride and face for allion, lift of dealUNK\n",
      "\n",
      "with america\n",
      "====================================================================\n",
      "\n",
      "<_io.TextIOWrapper name='lstm_peephole.csv' mode='wt' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 10\n",
    "docs_per_step = 10\n",
    "valid_summary = 1\n",
    "train_doc_count = num_files\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Capture the behavior of train/valid perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            \n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity\n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity\n",
    "        \n",
    "        # shows the training progress\n",
    "        print('(%d).'%di,end='') \n",
    "        \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "        session.run(reset_train_state) # resetting hidden state for each document\n",
    "        \n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses  \n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "\n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        test_word[0,data_list[np.random.randint(0,num_files)][np.random.randint(0,100)]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})            \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write the perplexity data to a CSV\n",
    "\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    print(f)\n",
    "    writer = csv.writer(f,delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
