{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending LSTMs: LSTMs with Peepholes and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\software\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure president stories are downloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  ../data\\speech_1.txt\n",
      "File  speech_1.txt  already exists.\n",
      "Downloading file:  ../data\\speech_2.txt\n",
      "File  speech_2.txt  already exists.\n",
      "Downloading file:  ../data\\speech_3.txt\n",
      "File  speech_3.txt  already exists.\n",
      "Downloading file:  ../data\\speech_4.txt\n",
      "File  speech_4.txt  already exists.\n",
      "Downloading file:  ../data\\speech_5.txt\n",
      "File  speech_5.txt  already exists.\n",
      "Downloading file:  ../data\\speech_6.txt\n",
      "File  speech_6.txt  already exists.\n",
      "Downloading file:  ../data\\speech_7.txt\n",
      "File  speech_7.txt  already exists.\n",
      "Downloading file:  ../data\\speech_8.txt\n",
      "File  speech_8.txt  already exists.\n",
      "Downloading file:  ../data\\speech_9.txt\n",
      "File  speech_9.txt  already exists.\n",
      "Downloading file:  ../data\\speech_10.txt\n",
      "File  speech_10.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a directory if needed\n",
    "dir_name = \"../data\"\n",
    "num_files = 10\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  #Download a file if not present\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "\n",
    "filenames = [\"speech_\"+format(i, '01d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "['speech_1.txt', 'speech_2.txt', 'speech_3.txt', 'speech_4.txt', 'speech_5.txt', 'speech_6.txt', 'speech_7.txt', 'speech_8.txt', 'speech_9.txt', 'speech_10.txt']\n",
      "10 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    print( filenames)\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "   # assert file_exists\n",
    "print('%d files found.'%len(filenames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file ../data\\speech_1.txt\n",
      "Data size (Characters) (Document 0) 3443\n",
      "Sample string (Document 0) ['fo', 'r ', 'my', 'se', 'lf', ' a', 'nd', ' f', 'or', ' o', 'ur', ' n', 'at', 'io', 'n,', ' i', ' w', 'an', 't ', 'to', ' t', 'ha', 'nk', ' m', 'y ', 'pr', 'ed', 'ec', 'es', 'so', 'r ', 'fo', 'r ', 'al', 'l ', 'he', ' h', 'as', ' d', 'on', 'e ', 'to', ' h', 'ea', 'l ', 'ou', 'r ', 'la', 'nd', '.\\n']\n",
      "\n",
      "Processing file ../data\\speech_2.txt\n",
      "Data size (Characters) (Document 1) 6871\n",
      "Sample string (Document 1) ['se', 'na', 'to', 'r ', 'ha', 'tf', 'ie', 'ld', ', ', 'mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'mo', 'nd', 'al', 'e,', ' s', 'en']\n",
      "\n",
      "Processing file ../data\\speech_3.txt\n",
      "Data size (Characters) (Document 2) 7320\n",
      "Sample string (Document 2) ['se', 'na', 'to', 'r ', 'ma', 'th', 'ia', 's,', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ' b', 'ur', 'ge', 'r,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'sp', 'ea', 'ke', 'r ', \"o'\", 'ne', 'il', 'l,', ' s', 'en', 'at', 'or', ' d', 'ol', 'e,', ' r', 'ev', 'er', 'en', 'd ']\n",
      "\n",
      "Processing file ../data\\speech_4.txt\n",
      "Data size (Characters) (Document 3) 6255\n",
      "Sample string (Document 3) ['mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' q', 'ua', 'yl', 'e,', ' s', 'en', 'at', 'or', ' m', 'it', 'ch', 'el', 'l,', ' s', 'pe', 'ak', 'er', ' w', 'ri', 'gh', 't,', ' s', 'en', 'at', 'or', ' d']\n",
      "\n",
      "Processing file ../data\\speech_5.txt\n",
      "Data size (Characters) (Document 4) 4540\n",
      "Sample string (Document 4) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'to', 'da', 'y ', 'we', ' c', 'el', 'eb', 'ra', 'te', ' t', 'he', ' m', 'ys', 'te', 'ry', ' o', 'f ', 'am', 'er', 'ic', 'an', ' r', 'en', 'ew', 'al', '. ', 'th', 'is', ' c', 'er', 'em', 'on', 'y ', 'is', ' h', 'el', 'd ', 'in', ' t', 'he']\n",
      "\n",
      "Processing file ../data\\speech_6.txt\n",
      "Data size (Characters) (Document 5) 6082\n",
      "Sample string (Document 5) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'at', ' t', 'hi', 's ', 'la', 'st', ' p', 're', 'si', 'de', 'nt', 'ia', 'l ', 'in', 'au', 'gu', 'ra', 'ti', 'on', ' o', 'f ', 'th', 'e ', '20', 'th', ' c', 'en', 'tu', 'ry', ', ', 'le', 't ', 'us', ' l', 'if', 't ', 'ou', 'r ', 'ey', 'es']\n",
      "\n",
      "Processing file ../data\\speech_7.txt\n",
      "Data size (Characters) (Document 6) 4520\n",
      "Sample string (Document 6) ['th', 'an', 'k ', 'yo', 'u,', ' a', 'll', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 're', 'hn', 'qu', 'is', 't,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'di', 'st', 'in']\n",
      "\n",
      "Processing file ../data\\speech_8.txt\n",
      "Data size (Characters) (Document 7) 5961\n",
      "Sample string (Document 7) ['vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' c', 'he', 'ne', 'y,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'pr', 'es', 'id', 'en', 't ', 'ca', 'rt', 'er', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'cl', 'in', 'to', 'n,', ' m', 'em', 'be']\n",
      "\n",
      "Processing file ../data\\speech_9.txt\n",
      "Data size (Characters) (Document 8) 6680\n",
      "Sample string (Document 8) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'i ', 'st', 'an', 'd ', 'he', 're', ' t', 'od', 'ay', ' h', 'um', 'bl', 'ed', ' b', 'y ', 'th', 'e ', 'ta', 'sk', ' b', 'ef', 'or', 'e ', 'us', ', ', 'gr', 'at', 'ef', 'ul', ' f', 'or', ' t', 'he', ' t', 'ru', 'st', ' y', 'ou', ' h', 'av']\n",
      "\n",
      "Processing file ../data\\speech_10.txt\n",
      "Data size (Characters) (Document 9) 5973\n",
      "Sample string (Document 9) ['th', 'an', 'k ', 'yo', 'u.', ' t', 'ha', 'nk', ' y', 'ou', ' s', 'o ', 'mu', 'ch', '.\\n', '\\nv', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bi', 'de', 'n,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'me', 'mb', 'er', 's ', 'of', ' t', 'he', ' u', 'ni', 'te', 'd ', 'st', 'at', 'es']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    # make all the text lowercase\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Breaking the text into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    # Creates a list of lists with the bigrams (outer loop different stories)\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries (Bigrams)\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57645 Characters found.\n",
      "Most common words (+UNK) [('e ', 1825), (' t', 1518), ('th', 1292), (' a', 1274), ('s ', 1040)]\n",
      "Least common words (+UNK) [('o:', 1), ('76', 1), ('b.', 1), ('az', 1), ('”s', 1), ('k;', 1), ('yb', 1), ('yâ', 1), ('tp', 1), ('”f', 1), ('”u', 1), ('kf', 1), ('-l', 1), ('40', 1), ('\\nl', 1)]\n",
      "Sample data [78, 15, 243, 56, 295, 4, 16, 34, 25, 7]\n",
      "Vocabulary:  346\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "#print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the RNN. More specifically the generator breaks a given sequence of words into `batch_size` segments. We also maintain a cursor for each segment. So whenever we create a batch of data, we sample one item from each segment and update the cursor of each segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\tpr (96), \tr  (15), \the (6), \te  (1), \tou (23), \n",
      "\tOutput:\n",
      "\ted (48), \tfo (78), \t h (47), \tto (32), \tr  (15), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\ted (48), \tfo (78), \t h (47), \tto (32), \tr  (15), \n",
      "\tOutput:\n",
      "\tec (112), \tr  (15), \tas (87), \t h (47), \tla (144), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tec (112), \tr  (15), \tas (87), \t h (47), \tla (144), \n",
      "\tOutput:\n",
      "\tes (26), \tal (63), \t d (57), \tea (52), \tnd (16), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\tes (26), \tal (63), \t d (57), \tea (52), \tnd (16), \n",
      "\tOutput:\n",
      "\tso (126), \tl  (53), \ton (21), \tl  (53), \t.\n",
      " (113), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\tso (126), \tl  (53), \ton (21), \tl  (53), \tou (23), \n",
      "\tOutput:\n",
      "\tr  (15), \the (6), \te  (1), \tou (23), \tr  (15), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM, LSTM with Peepholes and GRUs\n",
    "\n",
    "* A LSTM has 5 main components\n",
    "  * Cell state, Hidden state, Input gate, Forget gate, Output gate\n",
    "* A LSTM with peephole connections\n",
    "  * Introduces several new sets of weights that connects the cell state to the gates\n",
    "* A GRU has 3 main components\n",
    "  * Hidden state, Reset gate and a Update gate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "\n",
    "Here we define several hyperparameters and are very similar to the ones we defined . However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "batch_size = 64\n",
    "#num_unrollings = 50\n",
    "num_unrollings = 100\n",
    "dropout = 0.2\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test input (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "valid_inputs = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters and Cell Computation\n",
    "\n",
    "We define parameters and cell computation functions for all the different variants (LSTM, LSTM with peepholes and GRUs). **Make sure you only run a single cell withing this section (either the LSTM/ LSTM with peepholes or GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard LSTM\n",
    "\n",
    "Here we define the parameters and the cell computation function for a standard LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_peephole_dropout.csv\n"
     ]
    }
   ],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "#algorithm = 'lstm'\n",
    "algorithm = 'lstm_peephole'\n",
    "#algorithm = 'gru'\n",
    "\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "print( filename_to_save)\n",
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTMs with Peephole Connections\n",
    "\n",
    "We define the parameters and cell computation for a LSTM with peepholes. Note that we are using diagonal peephole connections (for more details refer the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "ic = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "fc = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],0.0,0.01))\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "oc = tf.Variable(tf.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],0.0,0.01))\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "algorithm = 'lstm_peephole'\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "# Definition of the cell computation.\n",
    "def lstm_with_peephole_cell(i, o, state):\n",
    "    '''\n",
    "    LSTM with peephole connections\n",
    "    Our implementation for peepholes is based on \n",
    "    https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf    \n",
    "    '''\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + state*ic + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + state*fc + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + state*oc + tf.matmul(o, om) + ob)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Units (GRUs)\n",
    "\n",
    "Finally we define the parameters and cell computations for the GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Reset gate: input, previous output, and bias.\n",
    "rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "rh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "rb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Hidden State: input, previous output, and bias.\n",
    "hx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "hh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "hb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Update gate: input, previous output, and bias.\n",
    "zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "zh = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "zb = tf.Variable(tf.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "algorithm = 'gru'\n",
    "#filename_to_save = algorithm + filename_extension +'.csv'\n",
    "\n",
    "# Definition of the cell computation.\n",
    "def gru_cell(i, o):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    reset_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rh) + rb)\n",
    "    h_tilde = tf.tanh(tf.matmul(i,hx) + tf.matmul(reset_gate * o, hh) + hb)\n",
    "    z = tf.sigmoid(tf.matmul(i,zx) + tf.matmul(o, zh) + zb)\n",
    "    h = (1-z)*o + z*h_tilde\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM/GRU/LSTM-Peephole Computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "  state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "# Note: there is no cell state for GRUs\n",
    "for i in train_inputs:\n",
    "    if algorithm=='lstm':\n",
    "      output, state = lstm_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "    elif algorithm=='lstm_peephole':\n",
    "      output, state = lstm_with_peephole_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "    elif algorithm=='gru':\n",
    "      output = gru_cell(i, output)\n",
    "      train_state_update_ops = [saved_output.assign(output)]\n",
    "        \n",
    "    output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "valid_output = saved_valid_output\n",
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "  valid_state = saved_valid_state\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "if algorithm=='lstm':\n",
    "    valid_output, valid_state = lstm_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "    \n",
    "elif algorithm=='lstm_peephole':\n",
    "    valid_output, valid_state = lstm_with_peephole_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "elif algorithm=='gru':\n",
    "    valid_output = gru_cell(valid_inputs, valid_output)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output)]\n",
    "\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(valid_state_update_ops):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "if algorithm=='lstm':\n",
    "  test_output, test_state = lstm_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "elif algorithm=='lstm_peephole':\n",
    "  test_output, test_state = lstm_with_peephole_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "elif algorithm=='gru':\n",
    "  test_output = gru_cell(test_input, saved_test_output)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output)]\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(test_state_update_ops):\n",
    "    test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "We calculate the training loss of the LSTM here. It's a typical cross entropy loss calculated over all the scores we obtained for training data (`loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies(train_state_update_ops):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch). But since GRU doesn't have a cell state we have a conditioned reset_state ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "    # Reset train state\n",
    "    reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "    reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = tf.group(\n",
    "        saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01)),\n",
    "        saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.01)))\n",
    "    \n",
    "elif algorithm=='gru':\n",
    "    # Reset train state\n",
    "    reset_train_state = [tf.assign(saved_output, tf.zeros([batch_size, num_nodes]))]\n",
    "\n",
    "    # Reset valid state\n",
    "    reset_valid_state = [tf.assign(saved_valid_output, tf.zeros([1, num_nodes]))]\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = [saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using the Adam optimizer as it is one of the best optimizers out there. Furthermore we use gradient clipping to prevent any gradient explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for decaying learning rate\n",
    "gstep = tf.Variable(0, trainable=False)\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# Decays learning rate everytime the gstep increases\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Sampling to Break the Repetition\n",
    "Here we write some simple logic to break the repetition in text. Specifically instead of always getting the word that gave this highest prediction probability, we sample randomly where the probability of being selected given by their prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "  '''Greedy Sampling\n",
    "  We pick the three best predictions given by the LSTM and sample\n",
    "  one of them with very high probability of picking the best one'''\n",
    "  best_inds = np.argsort(distribution)[-3:]\n",
    "  best_probs = distribution[best_inds]/np.sum(distribution[best_inds])\n",
    "  best_idx = np.random.choice(best_inds,p=best_probs)\n",
    "  return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM to Generate Text\n",
    "\n",
    "Here we train the model on the available data and generate text using the trained model for several steps. From each document we extract text for `steps_per_document` steps to train the model on. We also report the train perplexity at the end of each step. Finally we test the model by asking it to generate some new text starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n",
      "(5).(1).\n",
      "Average loss at step 1: 5.844168\n",
      "\tPerplexity at step 1: 345.215103\n",
      "\n",
      "Valid Perplexity: 342.77\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      "\t myanlehilehianhileanhileleleartherthhianthhiany thththththerthhiany anththththhianhilehilelelear, anhitherhilelehierhianththerhianthy anhianthhilehierany hianhilehianthhilelelearlethhianlehilehilelelehianthleththerhierhianhianhilehier, er, anthy anthhianthy anhilehier, anhierhilehilelelear, er, , , erany any anhilelehier, , eranhithery hithththerhianththhianthhieranhihileanlelearlelelear, , any anhiany thththerthhianthy thththerhianthhianthy thletherhiany thththerhihierhilehianhiany thhiththerhieranthy hianhithanhileanhianhilelehianthhianthhileanhilehianththhileanlehianthththhieranthy thlelehilehierany hianhiththhiererhihilelehianththhieranthhieranhihiany hilelehilelear, ererhihianhilelearlethhilelearthhianththhieranthy anththhilelelehilehilehianlethlelelehianhianthy thhithththery anhiththhianththerththery hitherhilehianhiery thhiertherhihiany thhiththertherthhileanleththerhiany ththhiany anhithhianhilehianlelelehilehierany thlelehilehileanthhilelearlelehier, anththhilehieranthy anththy \n",
      "====================================================================\n",
      "\n",
      "(7).(3).\n",
      "Average loss at step 2: 5.832946\n",
      "\tPerplexity at step 2: 341.362794\n",
      "\n",
      "Valid Perplexity: 339.55\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      "\t thleththy anthanththy ththanth tany thany thy an tththththy  tthanth tanth t ty ththy thanthththththther t t tananthy thy  tth tanthany an tanthy thy any  tanthththany thany thany an t tan tan tan t ty any  t t tththany ththy thy than tany anth t tan tan t ty thanththan t t t t tanany any any  tanth ty th tththanththany  tanththy  tththy thanth tanthy  t t tanany thy anththan t ty anth tanth t tany thany ththy an tanth ty th t t tan tany  tan tthanth tan ty an tthany  tthany thththan t t tan ty  t tth t tanan t ty ththththy an t tthan t tanthy an tany  tan t tthan tanthy anthththy  t tany anththththan t t tany anthy thththy anthy  t tththy  tth tth t tany th tany  t t tan t ty than ty thththy  tanthy an tan ty th tthththy an tththan tanth tany anth tth tan ty ththany  t tan tany  tth tanthththy any thy th tthththan tan ty anth t tany ththany  tany  tthththan ty than t t tthy  tany ththanth t t t tha tthan ty  tanthy any  tththy anthy any ththy thanthy anth tany  t t tanan ty th tany any \n",
      "====================================================================\n",
      "\n",
      "(4).(1).\n",
      "Average loss at step 3: 5.806115\n",
      "\tPerplexity at step 3: 332.325382\n",
      "\n",
      "Valid Perplexity: 335.20\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "\t holeththan t t ty  t tan tan ty y an tany  ty  ty than ty  t tany  tan tanany  ty anth t t tany  ty any y th tany  t ty any thany y  ty thany  ty any y th ty an tananththy y than t ty y  t t tan ty  ty any  tany thany th t ty  t t t tany  ty anth tany  tanan t tan ty  ty thanth t ty an tan tanany th t t tany  tan t t ty any  tan t tanany  ty than tan tany y an t ty any than ty y thany  tanan tany y anththy y  t t ty y an tan ty y  ty  tan ty  t t ty an ty  t ty y any y  t ty anth tan tananth tan ty y thy anthth t t tany y anththany  t ty any  tany y an ty  ty th ty y th ty anth t t ty anth t tanany y than tan t ty an t tany an t ty y  tany an tan ty  t t tany y  ty an ty an ty y thany than tany  t tan tan t t ty  tananthth t tanany y thy  ty thanthy y thy anthy thanth tan t t t t tanan tan t ty an tany  ty thy an t tan tan tany y than ty anthy  t t ty y anth tany  t tany anth ty  ty  ty than t tan t tan ty y any  t t ty any than tanthanth ty y thy any y th t ty  ty  tan t tananth ty y an\n",
      "====================================================================\n",
      "\n",
      "(3).(7).\n",
      "Average loss at step 4: 5.725302\n",
      "\tPerplexity at step 4: 306.525707\n",
      "\n",
      "Valid Perplexity: 330.01\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "\t re i t t t t t ty  ty  t ty y y  wy  ty  wy y  ty y  t t t t w t wy  ty y  t t t t w t wy  w ty y y  ty  ty  w w t t t ty  wy  wy  w w t t ty  w ty  w w w w w w w t w wy y  w w t wy  t ty  w wy y  t ty  t w w t w ty  wy y  w w wy y y y  t w w w t t w ty  ty y  w w t w ty  ty  t ty  t t wy  t wy y y  t t w w t t wy  t w ty y  wy  wy y y y  wy  w ty  ty  ty y  ty y  ty  w w t w wy  t w t t w w w t t w w t ty  t w ty  t ty y  t t ty  t w ty  t wy y  wy y y  w w wy  t ty  t w t t w wy  t w w w w ty  ty y  wy  t w wy  t w w t ty  ty  t w ty  t t t wy  t t t t w wy y  t t w ty  wy  w wy  ty y y  w w w wy y  w w ty  ty  w w w t t w wy  w t w wy y  wy y  wy  t t t ty y  ty  wy  t wy  wy  w wy  t w t t wy y  t t w t t t ty  ty  w w t wy  t wy  t t wy  ty  w w t t ty  w t w ty  ty y  ty  ty  w w ty  t ty y  w t t w wy y y y  t ty  ty y y  w w ty  t t t w ty y  ty  w w wy  t wy y y  w t t t ty y  w t ty  w w t wy y  t t w t t ty y  t t t w w w t wy  ty y  ty  ty y  w ty  w ty  t t w wy  t t t w t t\n",
      "====================================================================\n",
      "\n",
      "(4).(9).\n",
      "Average loss at step 5: 5.414086\n",
      "\tPerplexity at step 5: 224.547159\n",
      "\n",
      "Valid Perplexity: 325.04\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "\t sith t tan t w w w t tananth w w t wan w tan wanan tan tan w wanan w t w t t w wan t w w w t tan tanan w w t t t t w tan wanan w w wan tanan t wan wanan w t wanan t t w w w w t w w t t w tan t tan w wanan w w w w w t t w t w w wan t tananan t tan tanan t w w t t t tan wan t wan w t t wanananananan tan tan w w w w t t w w t wan w w tan t tan w w w wan w t t wan t t w tan w t w t w t wan w w tan tanan w t wanan w t w w w t tan tan t w wan w w wan tan w w t w w wan wanan tan tan tan t wan t t tanan tanan t t wanan t w wanananan t tan t w w tan t w w w tan t w t t w w w tan w t wan w t w wanan w w w t t w tan tan t t w tan tan wan t w tan w w wananan w tan wanan w wanan tanan t wan w tan w tan w t t tan tan w w wan t t w w tan t tanan t w w t wan w w wan w w t t wan w wan w t tanan tan t t t wanan t t w w t wanan w tanan w tan t t t t wan wananan wan t t w t t t w t wan w wanan tan t wananan t w t tan w t t tan wan w t w t wan wan wanan w t w t t t t w t wan t t w wan wanan t tan w w t w t w\n",
      "====================================================================\n",
      "\n",
      "(1).(3).\n",
      "Average loss at step 6: 5.331651\n",
      "\tPerplexity at step 6: 206.779005\n",
      "\n",
      "Valid Perplexity: 324.98\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ele ththe th t tth a athe e  ae  t t te e  t a ae  t ae  t ae  a te e  te  t a te  t t t t a a te  a ae  a te  te e e  te e  a t t t te e e  ae  t t t a te  t a a t a t a a te  t ae  te e e  t te  te  te e e  a ae e  t a t t a a te  te  t t t t a a a t t t a t t a t a ae  ae e  a a a t a a ae  te  te e e  ae  t t t a a te  a a t t a t a a t te  a te  te e  t t t t a a t a a a t a a a t t ae  ae  t t ae e  ae  ae e  t ae  te  te e  a t t t ae  t t te  a a te  te  ae  te  t t t t t a t t ae  te  a t t t t te e  t a ae  t a a a t t ae e  a ae e e  t te  te  t ae  t ae  te  t t t te  ae  a a a t ae  a t t t t t ae e e e e  te e  ae  ae  t te  ae  t t ae  a t a te  a t a ae  a a a te  t ae  a te  a t t a t t a t t a t a a a a ae  te  t a t a t a a t t ae  ae  t te  a ae  te  t ae  t te  t a te  te  t ae e  a a t ae  ae  a t a t a a t a t a t te  te e  t te  t t a t a t te  te  a te  t te e  a a a t t a a ae  t a a t ae e  t a te e e  ae e  t t a te e  a te e  a ae  ae  t a a t ae e  a t a t a\n",
      "====================================================================\n",
      "\n",
      "(2).(1).\n",
      "Average loss at step 7: 5.280842\n",
      "\tPerplexity at step 7: 196.535325\n",
      "\n",
      "Valid Perplexity: 327.06\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  pththe  ae  t a te  t ae  ae e  t t a t t t a a ae  ae  te  t t a t te e e e  te  a a ae  ae  t t t a te  t ae  te  a a a ae  t t t te e e  te  t t a a t ae e  t ae e e e  t ae  a a t ae  te  a a a a t t t ae e e  t a t ae  a t a t a t a a te  te  ae  t te  a a a te  t ae e e e  te  t ae e  te  t a ae  te  a t ae e  ae e e e  t ae  a t a t t a a t t ae e  a t te  a ae  a a t ae e e  a t t te e  a a a t te  t ae  a t te e e  a a a t te  ae  te  t a t te  t t a te  t te  t te  t t t a a t te  a ae e  t t t t a t a te  te  t ae  te e  ae  a a ae  t a t te e  a ae  ae e e  t t te  te e  ae  ae  te  ae  ae  a t t a t t ae  te e  te e  ae e e  a t t t a a t te e  t t t ae  t a te  te  te e e  te  a a ae  t t a a ae  te e e e e e  a a t a t ae  te  a a a a t te  a te  a te  ae  t te  t te  t a te  a t ae  te  t a t a t ae  a t t t te  t te e  a a te e e  a a t t ae e e e  t te  te e  te  a a a t ae  te e  a t a te  ae  ae e  te  te  ae  t t te  a ae e  te  t t te  a t a t t a ae  te  t ae  t a\n",
      "====================================================================\n",
      "\n",
      "(6).(1).\n",
      "Average loss at step 8: 5.254365\n",
      "\tPerplexity at step 8: 191.399993\n",
      "\n",
      "Valid Perplexity: 328.64\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      "\t elth te e  t t a ae e e  a t t t ae e e  t a a t t t a a t ae  t ae e  te  te  te  t t a a a t a a t te  ae  te e  a t a a ae  t te  t t a te e  te  t t t t te e  a a t a a te  a a t te  a te  t t ae  ae  a ae  t te e  ae  t t te  ae  a t t a t a t t ae e  t te  ae e  te  te e e e  ae  t t ae  a a ae e e e e e e  te  t t t t a a te e  ae  t te  ae e e  a ae e  a ae  a a t ae  a te  a t a t t a te  t te  a te e  ae e  t te  a te e  a t t ae  ae  te e e e e  te e e e e e  t te  t te  t te  t a ae  ae e  t t t te  a te  t te  a a a t t te  a a ae e  t a ae  t te  te  te  te  ae e e e  t t a t a t a t t te  a a te  te  te  t ae  a ae  t t t a a a a t ae e e  t te  t ae  a t t a te  t t t a t te  ae  ae e  te  ae  te e  a te  te e e e e  a a t a t te e e  a a te e e e e e  te  te e e  a ae  t a ae  ae  t a a a ae  t te  te  a a t t a te  a t t ae e  a a ae  ae  t t te  t a t t t te  a t te e  a t te  t te  a t t te  t a t te e  t a a te  t a a ae  ae e e  t t te  t a t a t te  a a a ae  ae  a\n",
      "====================================================================\n",
      "\n",
      "(2).(1).\n",
      "Average loss at step 9: 5.205337\n",
      "\tPerplexity at step 9: 182.242308\n",
      "\n",
      "Valid Perplexity: 329.50\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "\t f  te e  te  a t t ae  t a t te  te  t te  t te  t t ae e  te  ae e e e  a te  t te e e e  a t t a a ae  t t a t t a ae e  ae e  a a a a ae  te  t t a a t t t ae e e  ae  a te e  t te  a ae e e e e  t a t t t te e e e  te  a a ae  te e  ae e  ae e  ae  a t te e  t a t t a te  a t t a ae  ae  t a te  t t t te  te  ae  t a te e  te e  te  ae e  t t t a t t ae  te e e  a ae  t a a a ae  a a a t te  a a ae e  a a ae  te  t t a a a ae e  a t a t t a t ae e e e e e  t a a a ae  t ae e  te  a a t a te  a a te  ae  t t ae  ae  t te  te e  t ae  a a a t t te  te  t a t te e e e  t t te e  t te  te  te e e  a ae  te  t t ae  ae e  a a ae e e  te e e e  a ae  t te e  t t t ae  ae  te e e e  t a t t t a te  t t ae  t t ae e  t a t t ae  ae  t te e  a a a ae  t te e e  ae  ae  a t t te e  te e  te  t t te  ae e  te  t t a a ae e  a a te  ae  a te e  t te  te e  ae e  ae e  t a t te e  t t a ae e e  t te e  te  t a t ae  t t ae  a t ae e  te  t te e  t a te  ae e e e  te e e  a t t a a t t t a t te  t\n",
      "====================================================================\n",
      "\n",
      "(7).(2).\n",
      "Average loss at step 10: 5.218899\n",
      "\tPerplexity at step 10: 184.730620\n",
      "\n",
      "Valid Perplexity: 328.64\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      "\t us tthth te th tthe  te e  t te e  t a t te e e e e  te  t te  te  a te e  ae  te  ae e  a t te  te e e  te  a t a t ae  ae  t a a a t ae  a ae  t a t te e  a t a t t te  te  a te  a a a ae  ae  te  a te  t a a a t t a a a t t t ae  t t te  te  te e  t te  t t t te e e  a te  a t t t t ae e  ae  te e e e  te  a te  ae  te e  a t te  a a ae  t a a t a te  a t ae e e  ae  ae  a a t a te  a ae  te e  a te e  a t ae e  a a ae e  a te e  a t ae  a t t te  te  te e e e  t t ae  t te e e  a a te e  t ae  t t a a a t te  ae e  ae  t ae e  a t te e e  te e e  a t ae e  a te  te  a t t t ae e  a t te e  te  t a ae e  a ae  a te e e  ae  a ae e  a te  t t a t a ae  a a a a a a t ae e e e  t a a t t a a te  te  a te  a a ae  te e e e e e  a te  t t t t a te  te e e e e  t ae e e  a ae  te e  a te  te e  t ae  a t a a t te e e  te e  te  t t ae  ae e  a t t a te e  t t ae  a te e e  te  te  ae  ae e  te  a ae  te  ae  t t a ae  t a t ae  t t t t t a a t a t a t a te  te  ae  t t a a t ae  ae  a ae e \n",
      "====================================================================\n",
      "\n",
      "(6).(3).\n",
      "Average loss at step 11: 5.249873\n",
      "\tPerplexity at step 11: 190.542144\n",
      "\n",
      "Valid Perplexity: 328.07\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  ce  te  te e  te e  t t ae  t a te e  t te e  t a a te  a t t a t t t a a t te e  te  a t t t t t te  t te e  t ae e e  te e  t t a t t te e e e e  a t ae e  a te  a t a t ae  t ae e  a te  a a t ae  ae  t t t a te  t te e  t te e e e  t t t t ae  t a t t t te  t t a ae  ae  t t t te  ae  t ae  te e  te e e  t a t ae  ae e  ae e e e  t a ae  ae e  ae  t a ae  ae e  t t ae  te  t ae  ae  t t t ae  ae  a te  t a a te  t ae  t t t t a t a te  a t a a t t te  t t t t ae  ae  a t a ae e  ae  te e e  ae e e e  ae  te e e e  a ae  te e e  ae e e e  a a a t te  a te  ae  a a a ae  t ae e  t ae e  a te  a a te e e  a t t a ae  a a te e  ae  t ae  t t a a a a te e  a a a ae  t a ae  t a te  te e  a t t te e  a t a ae  t te e e  te  t t te  ae  te  a ae e  ae  te e  t a te e e  t t ae  t t t a a te e  t a te  te e  t t a ae  te e e  t a a t a t a t t a t te  a ae e e  a ae  a ae  a a a te e  a t ae e  t te  t t t a a te e  a t a ae  a te  t te  t ae  te e e  ae  ae  t a te e  ae  a ae e e e e  te \n",
      "====================================================================\n",
      "\n",
      "(0).(3).\n",
      "Average loss at step 12: 5.247798\n",
      "\tPerplexity at step 12: 190.147165\n",
      "\n",
      "Valid Perplexity: 328.21\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      "\t t  te e  t t a ae  t ae  a a a t t ae e  t t a ae  a ae  a a t te  a a a t ae  ae e  ae e e  t t t ae e  ae  a t a te e  t a te  a t te  te  ae  te e e e e e  te e e  te  a te e  a t t t a te e e e  t te e  te  a te  t t ae  t ae e e e e  ae e  a ae e  a te  t ae  a t a a t te  a t ae e  t t t ae e e  te e e e  t t a t t a a a t ae e  ae e  t a te e e  a t ae  t a t t a ae  ae  t te  t t t t t t ae  ae  ae  ae  ae  t t ae e  t a t a te  ae  a a t t te  t ae e  t t te  a t t a a te  a a a a te  a t te e  te  ae e e e  t a ae  te  a te  t a t t a a a ae  a a t te  ae  a a te  a t a a ae  te  t t t t t t t t t te  te e  t t te  t a a t te  a a te e  t t t te  t t te e  t ae e e  te e  ae e  t a t a ae e  ae  t te  t ae  t te  a t ae  ae  te e  ae  te  ae e  te e  t te e  t t te  t a t ae  ae e  ae  te  t ae e  ae e e  te e e  t a t a t a t a ae  a a a a t a te e  a a a te  a a ae  a a ae  te  t te  t te  a a ae e  ae  te  a t te e  te  ae  t a t t a te e e  ae e e e  a a ae e e  ae  t t a a\n",
      "====================================================================\n",
      "\n",
      "(3).(0).\n",
      "Average loss at step 13: 5.236812\n",
      "\tPerplexity at step 13: 188.069646\n",
      "\n",
      "Valid Perplexity: 328.57\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ste e  a a t t te e e e  ae  a t a a ae  ae e  t a t a t te  te e e  a t ae  t t t t te  a a t t a ae  t te e  t t t te e e e  t a a a ae  a te  t a te e  a a a t t te e e  t te  te e e  ae e  a te e  t te  te e e  te e e  te  t a a t a t t te  a a ae  ae  ae e  a t t a a ae  te  a a ae e e e e  t te  a t a ae  te  t a te  te  t te  t a te  a t te  t t a ae  a a t t t t ae  ae  t a t a te  ae e e e  t te e e  te e  a t te  te  a t a ae  t te  te  a a a ae e e  te  a a ae  te e  a ae  t a a ae e  t te  te  ae  t ae  t ae  te e  a ae  a a a t a ae  te e  te  a t t a ae  a ae  a a a t t ae  ae  t ae  t a ae  te  te e  a t ae  a a te  a t ae  te  te e  a t t ae e  t t te  te  a a te  t te e e e  te  t t t t t t te e  a a t t t t ae  t ae  t ae  a a t a te e  t te  a t t a a a ae  a ae e e  a a t t te  t te  t t t a te  a a a te e  a t te  te e e  a ae  te  t a a te e e  a te  te  te  t te  a ae e  ae  a t ae  t te  a te  t a a t te e  t a ae  te  ae  a t a t a t t ae  ae  t te e  a ae  t t t\n",
      "====================================================================\n",
      "\n",
      "(4).(0).\n",
      "Average loss at step 14: 5.241730\n",
      "\tPerplexity at step 14: 188.996712\n",
      "\n",
      "Valid Perplexity: 328.25\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "\t pre  te thth a ae  t a t t a a a t t te  te e  te e  t te  a a a ae  te  te  a te  te  a a ae e  a t te e  te  a a a a a a ae e e e  a a te e e e  a a t te e  t a a ae  te e  t t t t a a t t ae  a a t a a a a ae  te e e e e  ae e  ae e e e e  ae e e  te  a ae  ae  a t te e  te  a ae  t t t t a a te  te e e e e  t t a t ae e e  ae  a a t ae  a t a t a ae  ae  ae e  te e  te  t te  t te  te  t ae e e  a a a ae  t t ae e  a t te  te  ae  t a t a te e  te  a te  t t ae e  a te e  a te e  a a a t t a te  te  a a t a t a te e e  te  ae  ae  a a t t t t t te e  t ae e  t ae e  te e e e  a a ae  ae e  t te e  t te  t t t ae  ae e  t te  a te e e  a te  t a t t a ae  te  a a ae  te e e e e  t t te  t t ae  t t ae  t a te  te e e  ae e  a t ae e  te  a a te  a te e e  ae e  t a t t ae  a t t a ae  t t ae  te e  t t ae  ae e e e e  te  a t a te  a te  a t a a t t te e  a t ae  ae e  t t a a t ae e e e e  a t t ae  ae  t a te  ae e  te e  te  ae e e  a a a a t t ae  t t a ae e  ae e e e  te  a a te \n",
      "====================================================================\n",
      "\n",
      "(4).(0).\n",
      "Average loss at step 15: 5.226445\n",
      "\tPerplexity at step 15: 186.129927\n",
      "\n",
      "Valid Perplexity: 327.31\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      "\t anthth tthe thth t tthe  te e the the e  te the the  te e ththththe e the  te th te e the the the  te  tth tthe e e e  t t te  tth te  tth t te the  tthe the  t tthththe  tthe  t t te e e ththe  tth tthe  te  t tthe e e  te e e e the th t tth te e the th te e e e  te thth te  te e e e e  t tthe e  t t te the e  tth te ththe th te  tthth tthe e  tth te  t t t te  tth t tth te th tth t te e e  te ththth t te e the  te ththe the e  t t tthth te  tththe  te e the the  tth t tthe the e th te the  tthe e ththe e e e ththth te the e thth te e thth tthe  te  tthe e  te  t t t te  te  t te  t te ththththththth te e  t te e th te e  t tthe  t te e e th t te e  te the e e e  tthe  t t t te e th t t t tth t te e the e ththe  t te thth t tththth te e  tth t tthe  te  te  tthe  te e e th te  tthe  tthe e e  t tthth te the e  t te the ththe  t te e  tththe  te e thththth tththe e  te the th t te  te the e  te thth te th tththe  te thth t te th tth te e e e  te  te thth tthth t te the th t te e  te e  t\n",
      "====================================================================\n",
      "\n",
      "(0).(5).\n",
      "Average loss at step 16: 5.217619\n",
      "\tPerplexity at step 16: 184.494325\n",
      "\n",
      "Valid Perplexity: 326.45\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      "\t rithe  te thththe  te e e e the e ththe  te e  tththe e  te e  t tthe e  tththe e e e th te  te the  tththe th te the the e e  te  te  tthe e e  te e th t t tth te e  tthe the  t tth tth tthe th tthe  t te  tthe e ththth t t te e the e e  t tthe the  tthe th te thth te e e thththe e  tth te  te e e e the e the  tthe e  te e e  te th te e th te thth t t t te the  te th t tthth t te e th t te  t t te the the  t t tth te th t t te e the the e thth te thth te the e the the the e e e  te the  te  te the the the  tthth t t tththth tthe the e e  tthe the th te e  t t t te  te the e e e e ththe  te e e th te th te  te  te ththe  te e ththe  te  t t t t te e  t te e  tthe e e e e  t t t te the  te  te e ththe e  t tthe  t t t te e e  te  t te th t te the e the  te  t tthe e th te e th tthe e th te the the e the the e e e e  te  tthe the the  tthe e e e e th te  t t te e e ththth te e  te  tththe  tthe e  te e the  tth te e e e  te th tthe e e e  t te e  tthe e th te e e e  te  t t t t te e  tth t\n",
      "====================================================================\n",
      "\n",
      "(7).(4).\n",
      "Average loss at step 17: 5.220348\n",
      "\tPerplexity at step 17: 184.998575\n",
      "\n",
      "Valid Perplexity: 325.75\n",
      "\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      "\t esththththe  tth t te e the e  te th te e e thth te e  te the e the  t t te  t t t te e  t te  t t te th t t t tth te e  tth tththe the e e e e thth tthththe e th te e th tth tththe e  t te thth t te e e the e  tththe  te e  te  te  t tthththe e th tthe e th tthe  t tth te the the th te the e thththe e  te  t t tth te ththe  tthe e ththth tthe e e e the e e th t t te the th t te  tthe the e e th te e ththth tthththe e e  tth t tthththe thththe  tthe the e e  te e e  tthe  te  te  t te  t te e ththe e e ththth t te e e  te  te the e e thth t t te  te e e  t te  te e the e  t tthe e the the e th te e th te the thth te  te the the  tth te e  te th t tth t te  tth te e e e  tthe e e the e the ththe thth tth tthe the  te  tthe the ththe e e e  tththe e  te the the the  te e e th tthth te th t te e e  t te th t tthe e e thth tthe e e e thththththe  te  tthth t tthe  te e th te the th te  te  t te e  tthththe  t tthe e  t t te e e the ththththe the thth t t tthe ththth t tthththe  te th te  te \n",
      "====================================================================\n",
      "\n",
      "(9).(8).\n",
      "Average loss at step 18: 5.222915\n",
      "\tPerplexity at step 18: 185.474064\n",
      "\n",
      "Valid Perplexity: 325.10\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      "\t loe  te e e  te e e e thth tth tthth t t t t t tthththth t te the  tthe th tththe e e the  te th t te ththe the e th tth tththth te e  tthe the e  tthe e  te e  t te e e e th tththe e  te e e e the th te  te th tthe e the the  tth te e ththth te e ththe ththe  tthththe e the th te e e e e e e e  te  te  t te ththe the the thththe  tthth tthththth t te e e  te e  t te e  t te the e e the e e  tthth t tthe the  te e e the e e e ththe  te e the the ththe  t t t te e th t te  te thththe e  t tth te e e e  te e the  te e  te  t te  te e e e thth t t t te  tthth t te  t te  te thththe the e e the  tthth tthe  tthe th tthe e e e  tth tthth tth tth t te the e ththth t t te the the  tthe  tththe  te thth te th te  tththe e ththe e e e e thththe e  t t tth tththth tth tthththe th t t te e e e the e  t t t t t tth t tth t te thththe the e e thth tth tthth t t te e e e e the  t te e th t te  te th te  te th tthe  te thth te e  te  te  t te e e  t te  te  t t t te  te e e the  t tthththe e e th tthe \n",
      "====================================================================\n",
      "\n",
      "(6).(9).\n",
      "Average loss at step 19: 5.248213\n",
      "\tPerplexity at step 19: 190.226018\n",
      "\n",
      "Valid Perplexity: 324.77\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  se thth te  t t te  tth te  te thththth tthe e  tthththe e e e the  tthe the thththe e  tthe thththth te thth tth t tthe e ththe e  te  t t te the  t te the e e the the ththe the ththe  t t te e e e  tth tthth tththth te e e e ththe  tth te e the  tththe  te th tth tthe the e e e  t te th tthe e e th te e e th t te e e e  te  t te th tthe ththe e e  tth te th t te the e e e e e  tth tth te e th te  t t t t te  te e e th t te e e e e  te thththththe th te  tthth te e e e e the  tththe  te th t t tth tth tthth te e  te e e e e  te  t tthe ththe ththe ththe e  te e the e the e e  te  te e e  te e e e e  t t t t tthe ththe  tthe the e th te  tthth tththe e thth t tthe  t tthththe e the  tth t t t te  t tth tthe e e  tththe  tth tthe e e  t tthth tth tthe  t tth tthe the e e e e the ththe e e e e  t te e e e ththe  te the  t tth te  te the e th t tthe e e thth te  t tthth tththe e the e e the  t tththe  te the  t tthe  te e e  te e  te e e  t te  t tthe  t tthe e e  t te e e the ththe e e  t\n",
      "====================================================================\n",
      "\n",
      "(9).(8).\n",
      "Average loss at step 20: 5.215771\n",
      "\tPerplexity at step 20: 184.153763\n",
      "\n",
      "Valid Perplexity: 324.60\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  ae e e e the e e thth tthth te e e  t t tththe thththe ththth t te e ththe  te e e e e thth t te the e e e  tthe  t tthe  t t te  t te  t tthth te  te e e  tth t t t te e  tth te  te thththe th te th te e e the e thth te  te e  tththth te  tthe  te  t tthth tththth te e ththe the  te e ththe  t t t te the  te e  te e  t te e e e the e e  tthe  te e e  tth te e e e  te e th te  tth tthe e  tth te th tthth tthe e e th te th te e  tthe  t t t t t tthththe e e e e  te e e th tththe e e the e e  t t t tththe e ththth te e e e  t te e  te  tththe  te e th tthe thththe e e the  te  te e  te e e e e the e e e e th te e  t tthe  t tthe  te  tththe  tthth tthe the  tthe e  tthe e ththththe  te e e th te e  te  te th te e e th te e  t tthe  te  tththth tthe  te e e thththe  t t tthththth t te  tthe th tthth tthe the e  te e e  te e e th te  t t tthe e  t te  t t te  te th te e the e e e e th tthth t t tth te e  t te the  te e e ththe e  te th te e e e ththe the thth t tthe  t te e th tth tthe e th\n",
      "====================================================================\n",
      "\n",
      "(5).(4).\n",
      "Average loss at step 21: 5.211379\n",
      "\tPerplexity at step 21: 183.346794\n",
      "\n",
      "Valid Perplexity: 324.41\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ele  t t tththe e e  te  t tththe the e e  te  t te  t t te e  t tthth t tthththe e e e e th te  t te  te e  tth t t te e ththth t te the e  tththe the e  te e the  tth te  tthth te e e  te e the  te th te e  tthe e th tth te e ththe e the  tththe th tthe e ththth te e e ththe e the th te e e e e e e th te  t te e ththe  tthe thth t t t tthe  te e the e e  t te e e e  te e  tththe the e e  te th te thth t te e  t te  te e  tthe e e e th t t t te e  te e  tthth t te th t te e  te e e the e thth te e  t te  te the  te e  te  te thth te e e e e the  te  te  te e e the the  tthe thththth t t t tththe  te  tth te e  te  t t te e  tthe e e e e the e e e  t tthe th tth te th tth te ththe ththe the  te  tth te e th t te the thth te e e  tthe the  tthe e e  tth t t t tthth tth tthe e  te  te the  t t te the th te ththe  te th te e  tth t te ththe  tththththththe e  t te  te  te  tthe  te e the e ththe e e e e e the thth te e e e e e ththe e thth t t te th tth tthe e  t te th te e e e the e e  tth\n",
      "====================================================================\n",
      "\n",
      "(5).(9).\n",
      "Average loss at step 22: 5.210764\n",
      "\tPerplexity at step 22: 183.233961\n",
      "\n",
      "Valid Perplexity: 324.12\n",
      "\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      "\t emth t tth t t tththe e th t tthe thth t tth te e e th tth te  te the e th te e e the the e e  te  te th t te the e thththe  te  te  t te e e  t t tthe  t t t t t tthe th te  tth te e  te  tththth te e th tth te the e e e e  tth te  te  te e e ththe e e  te the e  t te thth te e  te  t tthe  tthe e  t t t t t te the e ththe  te e  t t te e the the e th t te e  tthe e e  t te e thth t te e the  te e e e e ththe  tth te  te e the  t te e e e th te thth tth tth t te e e  t te thththe  t tth t t tththe e e e  tth te e  te  t te ththe th tth t te e e thth tthth te e thth t te e e  te e e e e e the e e  tthe the e  te  t tthe the th tthth tth t tthe e  te  te thth t te e the th tth te e  t tthe  tthththththe e e  tthe e  tthe  te  tthe e ththe e  te the ththth te e  tthe e e e the ththth te  te e  te e e  tthe e e  tthththe the the  t t tthe thth te e th t tthe e  t te e th t t t t t tthththththe th te e ththe  t t t te e e e e  tth t t t t te the  te e e the e e th te e th te  te the ththe e \n",
      "====================================================================\n",
      "\n",
      "(4).(7).\n",
      "Average loss at step 23: 5.208785\n",
      "\tPerplexity at step 23: 182.871788\n",
      "\n",
      "Valid Perplexity: 323.67\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  cthe  t te the  t te the the  te  te e e the  t te e e  te the  te e e  te th t te th tthe  tth te  te  te e ththe the the e  te  t tthth tth tthe the e e th te  tthe e  tthth t te ththe  tthe th t te e  te  tthe e ththth t tthe e th te e  tthe  t t te th te e  t te the  te the  tth tthe e  t te th te e the e e e  te thth te e  t tthe thththth tthe thth te the e e  tthe e e the thth te e e the  tth te e  t t t t te  tth tth t te th tthe th te the  tth t tth tthe  t te ththe  te  te  tthth t tth t te e e the e  t t tthe e  t t te  te  te  te e e e  t te e thththe  te the e  t te e e e e th t tththe  tth t te thth tthe  te e e ththth tthth te  te  t te the e  te  t tthe  te the  te  te th t te th te  t tthe th t te e e  te  te  t te  t te e ththe e e the the e e e  t te  tththth t te thth te e e  t tth te  te th te e e e th t tththe  t te e the th t t te  t t te ththe the thth te e e  te th tth te  te e e e thththe e e  te e the  tthththe e th tthe  t te e e e e  tthe th te th te the  te \n",
      "====================================================================\n",
      "\n",
      "(4).(7).\n",
      "Average loss at step 24: 5.204870\n",
      "\tPerplexity at step 24: 182.157210\n",
      "\n",
      "Valid Perplexity: 323.08\n",
      "\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      "\t li te the  t t tthth t te th te  te  t t te the the  tththth te e th tthth te the th te  t t te  tthe th tth t t tthe e e the th te th t tthththe  t t t te  tth te e  te th te  te e e  tth t te  tth te e e  t tthe  te e e e e e  te the e e e e th tth tthe e  t te the e e e  te  te ththththth t tth te  t te  te  tthththe e  te e  tthe  te  t te  te thththe  t t te the ththe the  tth t t te  tththe e thththth te th tth t t tthe th t te e e e e  t tthththth t t tthe  t te e the  te the e e e ththe e e e  t te ththe ththththththe  tth t tthththe e  t t te e e e the the  t tthe the thth te  tththe the e the th t tth tth tthththe  t t t te e  tth tth tth te e  tthe e e e e  t t t te e e  t tthe e  tth tththth te e e e  t te thth te  te  t te  t te  te  t t te  tththe e  tthe th te e thth te e th t te thth tthe e the  te e e e  te e th t t tththth te th te  t te ththththe e thth te e e e e the the  t te e e e the  tthe e ththe e e e e  tthe th te e th te  te  t t tthe thth te the e e e  tth t t\n",
      "====================================================================\n",
      "\n",
      "(3).(4).\n",
      "Average loss at step 25: 5.217078\n",
      "\tPerplexity at step 25: 184.394656\n",
      "\n",
      "Valid Perplexity: 322.83\n",
      "\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ui te the e ththththe the  t tthe  tthe e th t t te e the e th t t te the  tthe  te the e ththe  tth t t te e e the the th tththe e e thth tthe e e the e  tth tthe e  t te th te  t t tthe  tth te e e e e e e thth tthe  tthth t te  t te th te e e e e e e e th te e  te th te  te the the  t te e th t te the th te ththe e  t t t tthe e  t t t te th te the thth t te  tth tthe e e  te  te the e  tthe e the  t te ththe e  tththe  tththe the  t t t te e e e e th tth tththe the thth t te  te  t te e  te  te the the  te th tth te  te e the e e the e e e  te thth t te ththe  t t te thththe  tth t t tthe  te  tth te e  te e  tthththe the  te the e  te e e th te the  tthe e e the  tth te the e e e the the e  te ththe the  tth tthe  te e e e th te th tthe e  te  tththe  te e e the e th te e th te the ththe e  te thth t te th tththe  te  te the  te th tth tthe thth t te e e e  te  te  tthe e e e e  t tthe th te the e  t t te e e  t te  te e  tthe ththe e  tthe the  te  te th t te e  te ththe the the e \n",
      "====================================================================\n",
      "\n",
      "(6).(1).\n",
      "Average loss at step 26: 5.236631\n",
      "\tPerplexity at step 26: 188.035459\n",
      "\n",
      "Valid Perplexity: 322.89\n",
      "\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      "\t lye ththth tth tth t ae  a a ae e  t t t te  t t a a a a t t t t a a t ae  a a te e  a t t te e  t t t t a te  a ae e e  a a t a ae  t te  a a ae  ae  a a ae  ae e  t ae  a a ae  t te  te  ae e  t a a a a a te  a a te  a a t t a te  a te e e e  ae  ae e  ae e  a t ae  a ae e  t t te  te e  t ae  t ae e  te  te  te e  ae  t ae  ae e e  t te e  ae  te e  t a t t ae e  ae e e e  te e  t ae  a ae e  ae e  te  te  a te e  a a a a te  t a a a a a te  a ae  a te e e  t a t ae  a a ae e e e e  a te  te  a t a a t te  te e e e e  t te  a t a t ae  a a t t te  t t t te  t ae  ae  te  ae e  ae  ae  a te  a ae  ae  te  te  t t a te  ae  ae e e  ae e  a ae e  a a te  te  ae  a ae  ae e  t ae  t a t te  ae e e  a te e  ae  t ae e  te e  ae e  te e e  t a ae  t te  t ae e  t a ae  t a te  a t ae  a a te  a te e  ae e  a a ae e  ae  te  te  te  te e  ae  ae e  te e  t a te e  ae e e e  t t ae  t te e  ae e e  a a t a t ae  a te  t ae e  a ae  a t a a a t ae  te  a ae e e  te  a ae e  ae e  te  ae e  a t\n",
      "====================================================================\n",
      "\n",
      "<_io.TextIOWrapper name='lstm_peephole_dropout.csv' mode='wt' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 2\n",
    "docs_per_step = 2\n",
    "valid_summary = 1\n",
    "train_doc_count = num_files\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Capture the behavior of train/valid perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            \n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity\n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity\n",
    "        \n",
    "        # shows the training progress\n",
    "        print('(%d).'%di,end='') \n",
    "        \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "        session.run(reset_train_state) # resetting hidden state for each document\n",
    "        \n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses  \n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "\n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        test_word[0,data_list[np.random.randint(0,num_files)][np.random.randint(0,100)]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})            \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write the perplexity data to a CSV\n",
    "\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    print(f)\n",
    "    writer = csv.writer(f,delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
