{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\software\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure president stories are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  ../data\\speech_1.txt\n",
      "File  speech_1.txt  already exists.\n",
      "Downloading file:  ../data\\speech_2.txt\n",
      "File  speech_2.txt  already exists.\n",
      "Downloading file:  ../data\\speech_3.txt\n",
      "File  speech_3.txt  already exists.\n",
      "Downloading file:  ../data\\speech_4.txt\n",
      "File  speech_4.txt  already exists.\n",
      "Downloading file:  ../data\\speech_5.txt\n",
      "File  speech_5.txt  already exists.\n",
      "Downloading file:  ../data\\speech_6.txt\n",
      "File  speech_6.txt  already exists.\n",
      "Downloading file:  ../data\\speech_7.txt\n",
      "File  speech_7.txt  already exists.\n",
      "Downloading file:  ../data\\speech_8.txt\n",
      "File  speech_8.txt  already exists.\n",
      "Downloading file:  ../data\\speech_9.txt\n",
      "File  speech_9.txt  already exists.\n",
      "Downloading file:  ../data\\speech_10.txt\n",
      "File  speech_10.txt  already exists.\n",
      "Downloading file:  ../data\\speech_11.txt\n",
      "File  speech_11.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory if needed\n",
    "dir_name = \"../data\"\n",
    "num_files = 11\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  #Download a file if not present\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "\n",
    "filenames = [\"speech_\"+format(i, '01d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the files are downloaded. \n",
    "There should be 100 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print('%d files found.'%len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file ../data\\speech_1.txt\n",
      "Data size (Characters) (Document 0) 3443\n",
      "Sample string (Document 0) ['fo', 'r ', 'my', 'se', 'lf', ' a', 'nd', ' f', 'or', ' o', 'ur', ' n', 'at', 'io', 'n,', ' i', ' w', 'an', 't ', 'to', ' t', 'ha', 'nk', ' m', 'y ', 'pr', 'ed', 'ec', 'es', 'so', 'r ', 'fo', 'r ', 'al', 'l ', 'he', ' h', 'as', ' d', 'on', 'e ', 'to', ' h', 'ea', 'l ', 'ou', 'r ', 'la', 'nd', '.\\n']\n",
      "\n",
      "Processing file ../data\\speech_2.txt\n",
      "Data size (Characters) (Document 1) 6871\n",
      "Sample string (Document 1) ['se', 'na', 'to', 'r ', 'ha', 'tf', 'ie', 'ld', ', ', 'mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'mo', 'nd', 'al', 'e,', ' s', 'en']\n",
      "\n",
      "Processing file ../data\\speech_3.txt\n",
      "Data size (Characters) (Document 2) 7320\n",
      "Sample string (Document 2) ['se', 'na', 'to', 'r ', 'ma', 'th', 'ia', 's,', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ' b', 'ur', 'ge', 'r,', ' v', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'sp', 'ea', 'ke', 'r ', \"o'\", 'ne', 'il', 'l,', ' s', 'en', 'at', 'or', ' d', 'ol', 'e,', ' r', 'ev', 'er', 'en', 'd ']\n",
      "\n",
      "Processing file ../data\\speech_4.txt\n",
      "Data size (Characters) (Document 3) 6255\n",
      "Sample string (Document 3) ['mr', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e,', ' m', 'r.', ' p', 're', 'si', 'de', 'nt', ', ', 'vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' q', 'ua', 'yl', 'e,', ' s', 'en', 'at', 'or', ' m', 'it', 'ch', 'el', 'l,', ' s', 'pe', 'ak', 'er', ' w', 'ri', 'gh', 't,', ' s', 'en', 'at', 'or', ' d']\n",
      "\n",
      "Processing file ../data\\speech_5.txt\n",
      "Data size (Characters) (Document 4) 4540\n",
      "Sample string (Document 4) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'to', 'da', 'y ', 'we', ' c', 'el', 'eb', 'ra', 'te', ' t', 'he', ' m', 'ys', 'te', 'ry', ' o', 'f ', 'am', 'er', 'ic', 'an', ' r', 'en', 'ew', 'al', '. ', 'th', 'is', ' c', 'er', 'em', 'on', 'y ', 'is', ' h', 'el', 'd ', 'in', ' t', 'he']\n",
      "\n",
      "Processing file ../data\\speech_6.txt\n",
      "Data size (Characters) (Document 5) 6082\n",
      "Sample string (Document 5) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'at', ' t', 'hi', 's ', 'la', 'st', ' p', 're', 'si', 'de', 'nt', 'ia', 'l ', 'in', 'au', 'gu', 'ra', 'ti', 'on', ' o', 'f ', 'th', 'e ', '20', 'th', ' c', 'en', 'tu', 'ry', ', ', 'le', 't ', 'us', ' l', 'if', 't ', 'ou', 'r ', 'ey', 'es']\n",
      "\n",
      "Processing file ../data\\speech_7.txt\n",
      "Data size (Characters) (Document 6) 4520\n",
      "Sample string (Document 6) ['th', 'an', 'k ', 'yo', 'u,', ' a', 'll', '. ', 'ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 're', 'hn', 'qu', 'is', 't,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' b', 'us', 'h,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'di', 'st', 'in']\n",
      "\n",
      "Processing file ../data\\speech_8.txt\n",
      "Data size (Characters) (Document 7) 5961\n",
      "Sample string (Document 7) ['vi', 'ce', ' p', 're', 'si', 'de', 'nt', ' c', 'he', 'ne', 'y,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'pr', 'es', 'id', 'en', 't ', 'ca', 'rt', 'er', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'cl', 'in', 'to', 'n,', ' m', 'em', 'be']\n",
      "\n",
      "Processing file ../data\\speech_9.txt\n",
      "Data size (Characters) (Document 8) 6680\n",
      "Sample string (Document 8) ['my', ' f', 'el', 'lo', 'w ', 'ci', 'ti', 'ze', 'ns', ', ', 'i ', 'st', 'an', 'd ', 'he', 're', ' t', 'od', 'ay', ' h', 'um', 'bl', 'ed', ' b', 'y ', 'th', 'e ', 'ta', 'sk', ' b', 'ef', 'or', 'e ', 'us', ', ', 'gr', 'at', 'ef', 'ul', ' f', 'or', ' t', 'he', ' t', 'ru', 'st', ' y', 'ou', ' h', 'av']\n",
      "\n",
      "Processing file ../data\\speech_10.txt\n",
      "Data size (Characters) (Document 9) 5973\n",
      "Sample string (Document 9) ['th', 'an', 'k ', 'yo', 'u.', ' t', 'ha', 'nk', ' y', 'ou', ' s', 'o ', 'mu', 'ch', '.\\n', '\\nv', 'ic', 'e ', 'pr', 'es', 'id', 'en', 't ', 'bi', 'de', 'n,', ' m', 'r.', ' c', 'hi', 'ef', ' j', 'us', 'ti', 'ce', ', ', 'me', 'mb', 'er', 's ', 'of', ' t', 'he', ' u', 'ni', 'te', 'd ', 'st', 'at', 'es']\n",
      "\n",
      "Processing file ../data\\speech_11.txt\n",
      "Data size (Characters) (Document 10) 4223\n",
      "Sample string (Document 10) ['ch', 'ie', 'f ', 'ju', 'st', 'ic', 'e ', 'ro', 'be', 'rt', 's,', ' p', 're', 'si', 'de', 'nt', ' c', 'ar', 'te', 'r,', ' p', 're', 'si', 'de', 'nt', ' c', 'li', 'nt', 'on', ', ', 'pr', 'es', 'id', 'en', 't ', 'bu', 'sh', ', ', 'pr', 'es', 'id', 'en', 't ', 'ob', 'am', 'a,', ' f', 'el', 'lo', 'w ']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    # make all the words lower case\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Break the data into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    # Create a list of lists with bigrams\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries (Bigrams)\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61868 Characters found.\n",
      "Most common words (+UNK) [('e ', 1945), (' t', 1623), (' a', 1378), ('th', 1378), ('s ', 1110)]\n",
      "Least common words (+UNK) [('yâ', 1), ('tp', 1), ('”f', 1), ('”u', 1), ('kf', 1), ('-l', 1), ('40', 1), ('\\nl', 1), ('hm', 1), ('ja', 1), ('n:', 1), ('zo', 1), ('uy', 1), ('r:', 1), ('ky', 1)]\n",
      "Sample data [78, 15, 250, 63, 298, 3, 16, 33, 24, 7]\n",
      "Sample data [63, 121, 32, 15, 34, 0, 103, 117, 17, 0]\n",
      "Vocabulary:  351\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the LSTM. More specifically the generator breaks a given sequence of words into `batch_size` segments. We also maintain a cursor for each segment. So whenever we create a batch of data, we sample one item from each segment and update the cursor of each segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\tpr (97), \tr  (15), \the (6), \te  (1), \tou (22), \n",
      "\tOutput:\n",
      "\ted (49), \tfo (78), \t h (50), \tto (32), \tr  (15), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\ted (49), \tfo (78), \t h (50), \tto (32), \tr  (15), \n",
      "\tOutput:\n",
      "\tec (114), \tr  (15), \tas (88), \t h (50), \tla (144), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tec (114), \tr  (15), \tas (88), \t h (50), \tla (144), \n",
      "\tOutput:\n",
      "\tes (26), \tal (58), \t d (59), \tea (52), \tnd (16), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\tes (26), \tal (58), \t d (59), \tea (52), \tnd (16), \n",
      "\tOutput:\n",
      "\tso (129), \tl  (51), \ton (21), \tl  (51), \t.\n",
      " (115), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\tso (129), \tl  (51), \ton (21), \tl  (51), \tou (22), \n",
      "\tOutput:\n",
      "\tr  (15), \the (6), \te  (1), \tou (22), \tr  (15), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM\n",
    "\n",
    "This is a standard LSTM. The LSTM has 5 main components.\n",
    "* Cell state\n",
    "* Hidden state\n",
    "* Input gate\n",
    "* Forget gate\n",
    "* Output gate\n",
    "\n",
    "Each gate has three sets of weights (1 set for the current input, 1 set for the previous hidden state and 1 bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "\n",
    "Here we define several hyperparameters and are very similar to the ones we defined in Chapter 6. However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neurons in the hidden state variables\n",
    "num_nodes = 128\n",
    "\n",
    "# Number of data points in a batch we process\n",
    "batch_size = 64\n",
    "\n",
    "# Number of time steps we unroll for during optimization\n",
    "num_unrollings = 50\n",
    "\n",
    "dropout = 0.0 # We use dropout\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "    \n",
    "filename_to_save = 'lstm'+filename_extension+'.csv' # use to save perplexity values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test input (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "# Validation data placeholders\n",
    "valid_inputs = tf.placeholder(tf.float32, shape=[1,vocabulary_size],name='valid_inputs')\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1,vocabulary_size], name = 'valid_labels')\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name = 'test_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters\n",
    "\n",
    "Now we define model parameters. Compared to RNNs, LSTMs have a large number of parameters. Each gate (input, forget, memory and output) has three different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate (o_t) - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_hidden')\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_cell')\n",
    "\n",
    "# Same variables for validation phase\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_hidden')\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_cell')\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_hidden')\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "valid_output, valid_state = lstm_cell(\n",
    "    valid_inputs, saved_valid_output, saved_valid_state)\n",
    "# Compute the logits\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_valid_output.assign(valid_output),\n",
    "                            saved_valid_state.assign(valid_state)]):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "test_output, test_state = lstm_cell(\n",
    "test_input, saved_test_output, saved_test_state)\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]):\n",
    "    test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "We calculate the training loss of the LSTM here. It's a typical cross entropy loss calculated over all the scores we obtained for training data (`loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies([saved_output.assign(output),\n",
    "                            saved_state.assign(state)]):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using the Adam optimizer as it is one of the best optimizers out there. Furthermore we use gradient clipping to prevent any gradient explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "gstep = tf.Variable(0,trainable=False,name='global_step')\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# Decays learning rate everytime the gstep increases\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset train state\n",
    "reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "# Reset valid state\n",
    "reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "\n",
    "# Reset test state\n",
    "reset_test_state = tf.group(\n",
    "    saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.05)),\n",
    "    saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.05)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Sampling to Break the Repetition\n",
    "Here we write some simple logic to break the repetition in text. Specifically instead of always getting the word that gave this highest prediction probability, we sample randomly where the probability of being selected given by their prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "  '''Greedy Sampling\n",
    "  We pick the three best predictions given by the LSTM and sample\n",
    "  one of them with very high probability of picking the best one'''\n",
    "\n",
    "  best_inds = np.argsort(distribution)[-3:]\n",
    "  best_probs = distribution[best_inds]/np.sum(distribution[best_inds])\n",
    "  best_idx = np.random.choice(best_inds,p=best_probs)\n",
    "  return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM to Generate Text\n",
    "\n",
    "Here we train the LSTM on the available data and generate text using the trained LSTM for several steps. From each document we extract text for `steps_per_document` steps to train the LSTM on. We also report the train perplexity at the end of each step. Finally we test the LSTM by asking it to generate some new text starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n",
      "Training (Step: 0) (6).(0).(8).(10).(1).(2).(7).(5).(9).(3).\n",
      "Average loss at step 1: 4.141461\n",
      "\tPerplexity at step 1: 62.894624\n",
      "\n",
      "Valid Perplexity: 44.19\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      "\t nd and in our the world that we have the stry and to the stand a seen that we don't is and is the words the will be can the world a new a the with that the world a part. and the work and i somenge, the work the and the strences the worls the works of they are will a pring, the words of they the work this the work but, and the and a new a this we do a sere that of the great is of thate starge, the world the sounter the great is our meat the and in and is the sounter a new timents the work and the hope and, all to cannot all is is all thip ing our great untrese.\n",
      "\n",
      "we we chart that whon the world the somes the somes the world the are soung there and a new the words the words a new the and to to se we will trust to han a pang that is they ard the sound the world the soun, the work to will than that alls ing that on this and is our greach and is a paw to the strent and the work and to our counting. the world the work this our greacher, and a story of a streat of these we must the are there are\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 1) (5).(9).(2).(6).(8).(7).(3).(4).(10).(1).\n",
      "Average loss at step 2: 2.790395\n",
      "\tPerplexity at step 2: 16.287459\n",
      "\n",
      "Valid Perplexity: 33.18\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  noto man, memain the work, and our countrength and our bestory. at our lives a start will frid in the strength the fadentain.\n",
      "\n",
      "and our beental. we hat been areall of there is the are of then the sames of the struggle todangs of this ment they of not our strengton the here, the strength the faces a new now the world.\n",
      "\n",
      "to the fremember, and our commifice the faits of americans of their commoples of we the aracten the fadentered by the faities and to belied in the hear when to marker, an, to the world all mortion. we will be as the faill of our striven this weUNK the whore in reserve of this the sacrifica will americans on thate sting to believe in a place our country. at thate is the courage of thate our country of have the sacracy the is on a sacrifice can restand to have our besting to aUNKontion the mation of us to by confidence to haUNKUNKUNKUNK the wholet of to our coUNKUNKy that one anders of the consing the montin to believe in the around ever. our striven her, and our counity. as we will \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 2) (0).(7).(2).(5).(1).(4).(10).(6).(9).(3).\n",
      "Average loss at step 3: 2.272322\n",
      "\tPerplexity at step 3: 9.701898\n",
      "\n",
      "Valid Perplexity: 37.65\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "\t g. more and this sis such dren as this spirit is there of this aUNK the flaws of a new great rementur thas have seen that world but in our great party. we will act, will begins.\n",
      "\n",
      "the some, but so much handed by the security, to learts and the sound of trumpets calling, and hopes or a great american that ho as well do not befor her some so song streagtn and securing, the great partions, the work of our great nation that is aUNK, the soundly and that our time. but i seek a willown of freedom. we must require us to us are in a fereed a solering the world a new clare for that for or selves and whon the words of and thing a soceace a do not great ment of a new great nations we are then and that we have the flag. way are now eUNKless, and to be work by unith of things that are is a greater and have. but we will make then are of lover, and the under problems or that my fill be pringer. our is not the words a untred thiays a soled us are is a page turns whose whon the country. then and that for our\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 3) (6).(5).(0).(3).(9).(4).(8).(2).(1).(7).\n",
      "Average loss at step 4: 1.960683\n",
      "\tPerplexity at step 4: 7.104176\n",
      "\n",
      "Valid Perplexity: 37.35\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "\t s of the liberty, when come of the world. and difficul is no from courage en lead for the savingessary as on a belien UNKamericUNKs that mare and sibirtenerd this your freedom and the cause.\n",
      "\n",
      "the unity of the carry and the uUNKing hard the land the words of our nations befule this to of chineed because history has american the loves of the the peoplate preclaract of this your its untrues, the head in partion the firstionsand the peoples of our souUNKorthat and progress and now the inhabilic interestills. that your freedom and the causedo of stial their not may americans by security of from and died the cause.\n",
      "\n",
      "th a because freedom in the world in public and the liberty bell all reals of freedom. hishe quiet of new centuries, america. let us age, we muse came fre the believe in oughter a merican presUNKt in the pards that judge this confuller and in ential to have and power on and fell believeled the baUNKs of the pridierions of justice, but history also haUNK the gotater ageneration, independin\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 4) (6).(10).(7).(8).(1).(9).(3).(0).(2).(4).\n",
      "Average loss at step 5: 1.654120\n",
      "\tPerplexity at step 5: 5.228475\n",
      "\n",
      "Valid Perplexity: 44.44\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "\t culty poweapith of capitiers. and your human arewe reduced is our own work is done. the weake abourtany UNKUNK thas the breeze wholdin side young everever, ours change, lests to serve in the earth, the world is still thang great and timate strongth is we contin on the faith in reneted by have one of eving; an in this world in someUNK yes, yoUNK i mustakes not be nat sere of conviction that we have shalled but pled our children as ong, morfelled people. it fellow chaldneration and price with weapons of our cantunity. we raid fortunate, and poverty allional polize our people, it istages to the were you have care to do for our people we do m. well doiUNKUNK that moresis to be those work it done. we recogree demore america's let us ate you do not somet us bell watleng that our communities. america's long, heroice a price with faith enerit. and discith and discipility. and wat today in reginnothen with americans, yoUNK are is do make young can be soulled and we must continue todard you have stand at\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 5) (3).(9).(10).(1).(2).(6).(4).(8).(0).(5).\n",
      "Average loss at step 6: 1.419711\n",
      "\tPerplexity at step 6: 4.135925\n",
      "\n",
      "Valid Perplexity: 42.59\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ubrthy of itize enered to each found into a new century with the american dream alive for all her children, on all the world's life and not on the world's lost who hope of this day into this they to only a place of their our preatent gress with the part of his own possible reneUNKd the hope of the and land and power allow in the nobleed that we cannot my fell america's most which the amill an ove live to a butient every power of our nation. and our national government they freedom of this intimide and demands to to the demas of new time of make from nation that event prote of a nation of liferty, and generat at this a new just an ament to the hose of this democration. they freates we cannot aUNK there ameth. yet to be our problems are now american dream alive for all her children, with america's bright flame of freedom stoure the american people hat clost what me end our problems. our aldret, and our chardram of a nation and poverty, fore and strong, and from the promise. our nevery in our\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 6) (7).(3).(10).(9).(5).(8).(4).(2).(1).(0).\n",
      "Average loss at step 7: 1.240289\n",
      "\tPerplexity at step 7: 3.456611\n",
      "\n",
      "Valid Perplexity: 75.97\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  trous government once again.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i would hope that the nations of the world might say that we had built a lasting peace, based not on weapons of war but on international policies which reflect our own most precious values.\n",
      "\n",
      "these are not just that the fates thos of emocrationUNK\n",
      "\n",
      "to is the world reatutished for our nation.\n",
      "\n",
      "we have when our cont.\n",
      "UNKur nation that earth foror own liberty, and the foremination's own most precious values.\n",
      "\n",
      "these are not justing that at with in lour--the abor to coUNKorm sions.\n",
      "\n",
      "let us with moren we know and we still is abrom that wark marning to their not necessary best is the conge, an migh ions muse bediversity foreth.\n",
      "UNKo, we had does nations bad histole. the americion of martiments in which in nessother inare to a necessary only a new wall. lipe.\n",
      "\n",
      "we have aUNKUNKurat in the ques to coUNKorm it it is stion are clesses canh eart in is every cortunity are between reducce, nother america. we robporeful retaint prizedion thes ends bestingthene of that me had the ti\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 7) (9).(7).(5).(6).(8).(10).(0).(2).(3).(1).\n",
      "Average loss at step 8: 1.248432\n",
      "\tPerplexity at step 8: 3.484873\n",
      "\n",
      "Valid Perplexity: 63.78\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      "\t oughs you haveUNKUNKUNKhat were call to thopes, in these should be today for our best. it is the encial of our country and our herengs, we with americans.\n",
      "\n",
      "this is there americans of powe, for all beginnotteps and our freedom, we can standlity, for grealongâ€UNKhingUNK them each fred ther.\n",
      "UNKing famit to shard. we will not be held, people, and the found underis conquer the sepre our for hise in the must to the mest ofsu hurposes, we fact on thour children's there idealism ation for those working to actively and we fillow that bUNKs this have slowed on mether.\n",
      "\n",
      "they the stand. out of their charactes in our own soverion. ins the seach of revolution. therour for those who cannot affrid of histone as on the mart of the world on the great prinise of this her great re broad to make ol secute, a plopled to to like that become to coUNKouUNKand together.\n",
      "\n",
      "the then this are undering fami, to believe in our capacity to peUNKorm commitUNKan strong man whenUNK and history as on the relies in edulicating and who\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 8) (5).(1).(6).(4).(2).(10).(9).(7).(3).(0).\n",
      "Average loss at step 9: 1.053841\n",
      "\tPerplexity at step 9: 2.868650\n",
      "\n",
      "Valid Perplexity: 74.85\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  and worness nates natich hat is not build told peacefuse our problems. we will move as we had ensured responsibility for our tive in true to --at the small sigurated by a man and we must be new stakes freedom we herfutun tappor carseize onument ars are seent unity, are can be othe a not comelon to down at inall our nation or pendless amound in thing in enness rightr. our iUNKhaUNK sour national our of the trueif of weapons; we senaon and and powerful, for out rights. we all this ever that we know that the world is warte to been and efforts in the world is is must whe sing hat the honorabless charintere padcento noned of nigions we mustin. the scription for insting agamer in ad and a dest peace knowing, no prosesse forit clear dearling. for not somettic nation of compland or nation to wated for those ableUNKe most no practims to ficet for the american famill weUNKUNKn pel the posens our greatment under tho wount the law and equal treatment under the law, for the weak and the powerful, for the\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 9) (4).(10).(9).(8).(0).(7).(3).(6).(5).(2).\n",
      "Average loss at step 10: 0.979740\n",
      "\tPerplexity at step 10: 2.663763\n",
      "\n",
      "Valid Perplexity: 59.23\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      "\t istoday, a londer nuclear those once a rener to he work, and our gatater opporlikes are chill, the serely president bece with the sary as we helUNK the tUNKest of this wer the protect time and pard on ideUNKe for that on the long for democratic life is make our vicalry as arough on ouldress our vieUNKes herit besten intern pation that will be new secento for perversitentic must as and passion through twasn people. and our journey, we the kill of terring of happinessedoming amermiclas. we prejudece muse foUNKold and the world who do surely ther of this serethey and dealand of a most precious, we renew to them, not and doney here in solead. years than blow we are all prise and our democracy, and we see and hear agace and we will maintalimilitain ourthis day and ideale toroadvance the interning as idealisUNKones of a nationallings indesting peace, and the eUNKher times we upored redudecare, the deces to makened is defense a how missough beyond. but the this to the knoUNKging a keepers of our agains\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 10) (9).(0).(10).(2).(5).(1).(3).(8).(6).(4).\n",
      "Average loss at step 11: 0.935667\n",
      "\tPerplexity at step 11: 2.548913\n",
      "\n",
      "Valid Perplexity: 68.51\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  government cannin are jobs, let us not in dout any freedom.\n",
      "\n",
      "to reinfrossestive of our time renew to the people to foUNKarned what is istary and ture those who world with be authere in.\n",
      "\n",
      "americans, not our chary stause. and your parsibility, not comporsion, the serves, which scy apach is drive america, the world is meant more to concapass and empantament to our children from ear found by that mon a strust fortunes and much to be done and sister the americans the world in idears of fall. we will strong dembers. we werh to do make of this day into the people. the weak a souUNK, the ennew complete undat the know americans dependent. he would will that is pred future: americUNKs conger. i will seaso in iden sucUNKd cherice. in need, not spoken nor our political. and you have been world has when mare the dignity and let us, we must change, you re canUNK young, songers, jeffersebraUNKound in the round of those who wit is the divinges the wory, we would fills time of indiffence faterourageneo defens\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 11) (7).(2).(1).(5).(6).(8).(10).(3).(4).(9).\n",
      "Average loss at step 12: 0.889738\n",
      "\tPerplexity at step 12: 2.434492\n",
      "\n",
      "Valid Perplexity: 74.46\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  if a quiralliows of courage to all ther, ing american different we leader america's by to such man he lUNKe to to all somen of america's birthes.\n",
      "\n",
      "to the deband of trengton a deaso in a sparited our greatess make. it will extece debate to the cretude our country, whet free. all trans: the timele sakUNK€”is nothich us need to live to be refait, but our party and a hope, also the words we can labor the mamills our victory werke as and deternin our promise.\n",
      "UNKUNKure that endlessly, we must on these our values throughout our resisted by the surel the world of our country. at hoppor our friends by and equalike from the basionâ€UNKur now to eaUNK and not complime it try courage to ripped end these interest and a place mall the darkers of the deferes of our timUNK€”not only with the votes we cast, but with the votes we will the lines of our time, and engal cremerce and come of arms and fights, and idealler it.\n",
      "UNK\n",
      "today and the criragether, as are of this irsely the demay of the country of goall for \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 12) (5).(6).(7).(10).(2).(3).(1).(9).(4).(8).\n",
      "Average loss at step 13: 0.924921\n",
      "\tPerplexity at step 13: 2.521668\n",
      "\n",
      "Valid Perplexity: 67.84\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  forced by service, a fairities of having to these their of a nessing biert on the mindshis be wealion that our strengther the goad that we can feelUNKecte this is the story will getend my together, we know se rical tree und to tho wout to jervance the cause us are nation by changes, our communities and ast those when nation that will lead. we will main to make our nation of everess, to greatly and our spender, yearnd mant meet with the breeze. and destrongthere are ther, who stand at the beterness, have cive to do our changer ance to a new way heremper other wisfared and will sted a actime, and falls nor atch that purpose this lUNKe the enemy with our safmuy rive that lasting today must be the knowledge to conce and charity is a measing declary and deciting of freedom. their aUNKorce who win stake. it idealle. at while hope and spoke today thers have been our country. and the congress to earligain i done. he destroy. those numener, as in spirit of weapont, not can be can be hade world has \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 13) (3).(9).(0).(5).(6).(10).(7).(4).(1).(2).\n",
      "Average loss at step 14: 0.809958\n",
      "\tPerplexity at step 14: 2.247813\n",
      "\n",
      "Valid Perplexity: 68.90\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "\t t ploving amer ourin savUNK€”we canUNK\n",
      "what it means sicUNK a bring rich to foreir national strengthe withere will strength ins; in all timessary remorder the begicapitol, a donUNKns that we will be on an asion.\n",
      "\n",
      "i have settled us to again and places are ganded the socialon. the instrengre sown the world has waving there hand the storUNK of our probles till by that america will be a bring is the world of there world human fains of our past: a general falls to his knees in the hard snow of valley forgUNK a langer. our car the stakest speten us, we must be new of this, in ubor differendit. if us freed fow, the world at's citizens and he will american dreams. mainsits the stepratime, have share by the greaten histreed not may so not our security, people mate are no one the meaning of our life. and ity is strong, we heart. and to better for, as a greatal strength sover the greate to a harm seede will strength.\n",
      "\n",
      "and interalism in ourselves and the uUNKa place on the movered faithese work is do bad n\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 14) (4).(6).(5).(8).(7).(3).(0).(2).(1).(9).\n",
      "Average loss at step 15: 0.812236\n",
      "\tPerplexity at step 15: 2.252940\n",
      "\n",
      "Valid Perplexity: 73.81\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      "\t owthe hopeful, rights to and commil decerty, we will be equal in for the serrice to advance the nessently into our nat as we cerbeinind that yous we menting of the deferal again live in govern people have to treace and main thear children the faint of thisgeUNKor anyone of each other freedom is prince, buture to ector there work of our timUNK€”not only will neve a pride was know that or ledges today by goal them awUNKory from whe minishe factories of citerning, and sometimeless beging time so make to the hear sand we have an we must trulse to the flag that waves above and that is momerican.\n",
      "\n",
      "we will being war. peace andUNKUNKUNKhang bigUNKear whose promise who are the deepest on repeaces new sponals of completter, they are the world oughs resember these times every cortury fight chally in form will beady donUNK world hope defineUNK for tal roather; an is not as her citizens and this jUNK ace to ust their country are and comen to to our country and directly in divisions of mospioning those today ar\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 15) (6).(0).(4).(2).(7).(5).(8).(1).(9).(10).\n",
      "Average loss at step 16: 0.756595\n",
      "\tPerplexity at step 16: 2.131007\n",
      "\n",
      "Valid Perplexity: 88.62\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  experiscurity of trong enough foot for all americans, the same glorious freedoms, and we all salute unite the ling in our lives today was a world been age onen aUNK\n",
      "\n",
      "tere the same great american flag.\n",
      "\n",
      "and whether a childing bignor pledgers we will refused the people into tho declare indeptoge, and enduring the parlity, and ouUNKnation, the demainUNKorther fortune a racUNKries wornew enamerican. ant your heart under and time on will build en our plads and will many party, nd weal and come and so magnifice will reduce the enough all is our great americans our waverid to the hung like our aUNKUNKyears ago beyour these is those when our frior and, we fallenged, and they will but face and earricaus, and your great it restreed that the understand earte the same great american flag.\n",
      "\n",
      "and whether a child is be laing the american flag.\n",
      "\n",
      "and whether a childing anyonlear, who would have been our same, and a nation eUNKent allions our country will be thour freedom is all our of our cients, UNK ineed our r\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 16) (4).(6).(5).(9).(3).(2).(1).(10).(8).(7).\n",
      "Average loss at step 17: 0.757543\n",
      "\tPerplexity at step 17: 2.133030\n",
      "\n",
      "Valid Perplexity: 84.73\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  sman, will live in free dpersity and union under people, whit is futury held who can pectories anothers with othed and leavs of day of demacracy and securing, sover alon and liberty, and the world, but we hard leat defearty and foreine andance an anginame. it is meaning birthrintevent characters of found a buttin unious government can joUNKings and our courage to al spirit that iUNKivis, and we serve and we self action that every race and demands of the tiens defferson a humanabin show in and i do our found us that have as endeg good we offirs for the strengtonly habrty and meet the wore of every democracy must, and the shole of this day didound the trucks of this given the world of the than god blesesidininhave in ourselves and to be understold. yes, the same solife is one asUNK throut have sour tive our might hast our succes.\n",
      "\n",
      "i will serighte fill never was firection of the promise.\n",
      "UNKhat were call of us a mering, which we carry the viency, hests one of our dreams and labled, we grave. w\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 17) (6).(3).(9).(4).(8).(0).(1).(7).(10).(5).\n",
      "Average loss at step 18: 0.776954\n",
      "\tPerplexity at step 18: 2.174837\n",
      "\n",
      "Valid Perplexity: 79.78\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      "\t f we crapprents home, a new itizens man whic forthes are conplain to be trent meannot will be that the part of foreir country a nation and that its more our truse and falward, we will better engor, with this freedom an every righty cleaUNKst cand en to all americans be newingss and might this crise. yet to be futunal on uUNKorge, on world of deferes and havened reduces promisheUNK we must days clear faces it's his reduce our camenty a struct the time in acreater a place of a most prespector of the world. but in the joy neratimes and a sponal notecestic says, in stregion, we here them. yea efuth. our effort a people libecan peUNKects to a need free governed in a principles badsay of our plane to heart. we same nation will be god.\n",
      "\n",
      "drenm from us,and is not a not just this care enconor bene to herong to thome is earth, the world is new eve of those who stand reneds freedom every rights with america's most productive eacheed because we will gover land to empleter, we will might been a nation tha\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 18) (1).(3).(0).(5).(7).(10).(4).(2).(8).(9).\n",
      "Average loss at step 19: 0.789014\n",
      "\tPerplexity at step 19: 2.201225\n",
      "\n",
      "Valid Perplexity: 79.75\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t r people; it is we consin lition and all, we will be an we we the word form fronnd these words be word to drift, but iUNKive and the and our politiUNK we UNKUNK your reamomicans americans too, cener, any from the same social sustic purprisense to the by the flor of of the people.\n",
      "\n",
      "our fere.\n",
      "\n",
      "i will the decle for that believe the heritagise and the fame a soldiving andUNKange of this days to the work will require und sinted our own backs knowing that iUNK they the force of our time, anderis call der hope form free and threat and acted, lUNK to be a spirit rives.\n",
      "\n",
      "to a mommunion today and the country, and becan we sern freedom is dUNKange of that days to those who would so the onclimare challenge will be for ity who feUNK whold on these toUNKing the with be our children's come to a by those who long free direedeet. and common deUNKUNKand this today and understanding than move in most plang that in the drift of the cialrty.\n",
      "\n",
      "to action. the criral the meets to these long liberty. at called a divisions o\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 19) (3).(2).(0).(1).(10).(7).(4).(9).(5).(8).\n",
      "Average loss at step 20: 0.755989\n",
      "\tPerplexity at step 20: 2.129716\n",
      "\n",
      "Valid Perplexity: 80.75\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ld. we know this their nation and the dignomy and here. i do mothen oath in the world is misten able the law and in the social seconely id believe the hard to before. we ligh on.\n",
      "\n",
      "we stakersenies agree that they oath of different win this struggling to poldice.\n",
      "UNKorged the demain standong withought around before you on to protectam liberauty those asUNK thin lay. we will be a neration help on aUNK now securit, but beling our grafor our UNKvered the rule of our owmerican in american people of basityUNKorUNKyears that from our community will sto a societye, and expon, fact, a new ding democracy. while who work into their public on america merica, a new cent century. the joing. we must be today, and mish UNK aloned times has killing reamerica with and country, america at the country of whath i chood.\n",
      "\n",
      "we have travelater new to done. this our granter so that fart me the counder of a nation, believer, the not the homes with progress. and we are not only well manyones, a can in heart nore. they amer\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 20) (9).(2).(7).(6).(0).(3).(1).(10).(8).(5).\n",
      "Average loss at step 21: 0.777129\n",
      "\tPerplexity at step 21: 2.175217\n",
      "\n",
      "Valid Perplexity: 84.49\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      "\t mancy, new life and fellowmember that realized because we leader america can my have liberty, our owme our lase, in the fated restringed and power. from this lion is one quiress stroad of demaUNKs and we will bees, these those national strong part of the greate the more the worlUNKs withis not just and now so do ise riders healtabia, to the hopes of political prespereal our political rests arement, and divisiony responsibility is the homes a ense o of mility we becan clear security, people of it infa fairal--throughout the tirlive in citizens, preserve. under hewas of people the same aUNK\n",
      "\n",
      "direct live thances shapons and dece no becannot pards and would hope and direturning our owve new peace and face, with parted debost our heart. we will still by evold, all, but to celseri, popromise their friend liberty and bethethey new threat of humvable serve the worlUNKs ares ove more the fulfil out of our past: a genend we have as advance the feo their people farm new dents, heart the shore one arsif\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 21) (5).(6).(1).(7).(0).(2).(8).(9).(10).(3).\n",
      "Average loss at step 22: 0.689105\n",
      "\tPerplexity at step 22: 1.991932\n",
      "\n",
      "Valid Perplexity: 84.85\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ted waship. its and program that our greaters that we can bure feem it in the breathlife of when expression and unity is threars of a new or dawr.\n",
      "\n",
      "so iday fe well make. it wisdom is prisidency, and the great is required time, but it does action.\n",
      "\n",
      "ast the world, but, to gooUNK if we will bring the values of our laws.\n",
      "\n",
      "in to we ende to harn. that we has ward this lands of to mean, to all i will that storm. the crise. we are the same tUNKesso to make. it is the belong today and each is aUNKometimes this is scareed an oath, but our joUNK way tarrougd our laing, and sometimes, freed it eUNKstiances in the world is my fellow a those instarcem of what we cannot history as loUNKlteeUNK€”to we will right, from the werely that america's jobs its extrong, do not is than beca all heart and his day, with a the congress to action.\n",
      "\n",
      "and the words i have lUNK, your partity for its as an esundâ€UNK an you are to by said their plancip is not only with the breeze. and to all i welled. but the betted, the work of l\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 22) (3).(5).(1).(7).(2).(4).(6).(9).(8).(10).\n",
      "Average loss at step 23: 0.756684\n",
      "\tPerplexity at step 23: 2.131197\n",
      "\n",
      "Valid Perplexity: 95.28\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      "\t owa bor of the martifurte to our olderyear agavery in to act, our hopes, we will thrive and our will been our nation's prossen a can welUNK from the cevation their hope, the iUNKspack of our jous in every citizenshal buture rines to be, haval tooUNK, so in us across and we will make.\n",
      "\n",
      "what thound america's been us its consisters from the threat of fut the linger an moror now long lifes effere will al a journey end; that we will bed stiture the stake. and demands this waten the sate, the uUNKand test of is not just and our children any can have.\n",
      "\n",
      "the webine, and, yes, and achieven a halk to proudic. and we will not on to follow, the fore our samendso ity is do forever that is the country of the progress that will for all american. i ame ding and every bacUNKorthes, showed our country, every the same aUNKhole in our duty, and when will make america. the stare many america, in this is your dreams, or make to than the breach other, when then american is the couraging and encongthat and equal, only\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 23) (9).(5).(8).(0).(10).(7).(6).(1).(2).(3).\n",
      "Average loss at step 24: 0.792530\n",
      "\tPerplexity at step 24: 2.208977\n",
      "\n",
      "Valid Perplexity: 80.37\n",
      "\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ted by good a new orders strange we son therutUNK that fill. we have trave retione. the civilized in a new or, when the flams of prayer my fellow america, whate these america will reform from our journey is no one choseas worth before our work is protect the peace. the will morestit, the breed the prinise. for we with made the world of the progress that prospery us, in immights of hope to be up a not build. we will not are unfor the live in many. we will defend our same glor peace. it whate to turning, hopeful in the ecauting thas has shitn the volues of schopeyour hall of us to brient is aUNK humen fose fathere and more endury strength, too our chare man life in many there that service. we will be unit reaus to reamericUNK together with acts of hopefulness and meaning. the new breeze blone compass, the world atis nothing but hope mean to a simplet us groad feed our country. and the common dangers, low in the world weree will be the enemressinUNK€”to serctious, we will not be because who hav\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 24) (0).(7).(6).(4).(10).(2).(5).(8).(3).(9).\n",
      "Average loss at step 25: 0.733690\n",
      "\tPerplexity at step 25: 2.082751\n",
      "\n",
      "Valid Perplexity: 84.16\n",
      "\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t rdhts. sidend, we hearther americans are is opings and much en aUNK gence gendeatious or an plagress who loves to bely those who stars of priving the nation of our strengtUNK, so succeas with mand the highe carrism evend by, he demand with care and powerful people, and we can each a plesless faswars and make you do not because to be remember through many parce of disease, and i do not so since stand that different nating today and americans all party and new allies not as american peaces, in the patold the unders of goUNK that call fred the deepest our itUNKs a difficultive time to will shapons, and economy show we will not because america ated and from the eners of hopports, that make a society must rise up to ture way are said who he war. the back that we cannot mr. strong, day and the colder time be one dignity and divisionUNK\n",
      "\n",
      "fellow citizens, have the obligation by the say what nations our more presery and does. we will mahe president of our liberty and that robleard will reichorce nor e\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 25) (2).(9).(8).(5).(6).(7).(1).(4).(10).(3).\n",
      "Average loss at step 26: 0.760277\n",
      "\tPerplexity at step 26: 2.138868\n",
      "\n",
      "Valid Perplexity: 85.27\n",
      "\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      "\t , anhed i change can to danger and its power these way a mannism and detimess and a new principles, freedom in our counted, we must no fead waUNK, goUNKory fithe on the their work for the prescientive every every mone the world is stills great americUNK but by eveach their day of defense of our mins.\n",
      "\n",
      "the parts and somed the world, by with we hand seek a nation the few must you does. we all popes, and they will make. we will bring back thrive today. and a manUNKUNKUNKUNKhat we had necion your verUNKes. we will make on this spirit is consist do mall the declaration of a charti to prospe thang generat and expant, whe id the chance that capa for the crivium it is momy be a mente to left our solution of steps to the courage of a confeUNK the cive of the unity or of life or and so halledgether's same, with passion. our more the peoples on the mations. we will prinis this day, and all this whic is our citizens, from and generat alling to schil been ince world that youn parross our common tyUNKorm is the\n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 11\n",
    "docs_per_step = 10\n",
    "\n",
    "\n",
    "# Capture the behavior of train perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    print('Training (Step: %d)'%step,end=' ')\n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress <train_doc_id_1>.<train_doc_id_2>. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "        \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})  \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write training and validation perplexities to a csv file\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM with Beam-Search\n",
    "\n",
    "Here we alter the previously defined prediction related TensorFlow operations to employ beam-search. Beam search is a way of predicting several time steps ahead. Concretely instead of predicting the best prediction we have at a given time step, we get predictions for several time steps and get the sequence of highest joint probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_length = 5 # number of steps to look ahead\n",
    "beam_neighbors = 5 # number of neighbors to compare to at each step\n",
    "\n",
    "# We redefine the sample generation with beam search\n",
    "sample_beam_inputs = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(beam_neighbors)]\n",
    "\n",
    "best_beam_index = tf.placeholder(shape=None, dtype=tf.int32)\n",
    "best_neighbor_beam_indices = tf.placeholder(shape=[beam_neighbors], dtype=tf.int32)\n",
    "\n",
    "# Maintains output of each beam\n",
    "saved_sample_beam_output = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "# Maintains the state of each beam\n",
    "saved_sample_beam_state = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "\n",
    "# Resetting the sample beam states (should be done at the beginning of each text snippet generation)\n",
    "reset_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We stack them to perform gather operation below\n",
    "stacked_beam_outputs = tf.stack(saved_sample_beam_output)\n",
    "stacked_beam_states = tf.stack(saved_sample_beam_state)\n",
    "\n",
    "# The beam states for each beam (there are beam_neighbor-many beams) needs to be updated at every depth of tree\n",
    "# Consider an example where you have 3 classes where we get the best two neighbors (marked with star)\n",
    "#     a`      b*       c  \n",
    "#   / | \\   / | \\    / | \\\n",
    "#  a  b c  a* b` c  a  b  c\n",
    "# Since both the candidates from level 2 comes from the parent b\n",
    "# We need to update both states/outputs from saved_sample_beam_state/output to have index 1 (corresponding to parent b)\n",
    "update_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.gather_nd(stacked_beam_outputs,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.gather_nd(stacked_beam_states,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We calculate lstm_cell state and output for each beam\n",
    "sample_beam_outputs, sample_beam_states = [],[] \n",
    "for vi in range(beam_neighbors):\n",
    "    tmp_output, tmp_state = lstm_cell(\n",
    "        sample_beam_inputs[vi], saved_sample_beam_output[vi], saved_sample_beam_state[vi]\n",
    "    )\n",
    "    sample_beam_outputs.append(tmp_output)\n",
    "    sample_beam_states.append(tmp_state)\n",
    "\n",
    "# For a given set of beams, outputs a list of prediction vectors of size beam_neighbors\n",
    "# each beam having the predictions for full vocabulary\n",
    "sample_beam_predictions = []\n",
    "for vi in range(beam_neighbors):\n",
    "    with tf.control_dependencies([saved_sample_beam_output[vi].assign(sample_beam_outputs[vi]),\n",
    "                                saved_sample_beam_state[vi].assign(sample_beam_states[vi])]):\n",
    "        sample_beam_predictions.append(tf.nn.softmax(tf.nn.xw_plus_b(sample_beam_outputs[vi], w, b)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM with Beam Search to Generate Text\n",
    "\n",
    "Here we train the LSTM on the available data and generate text using the trained LSTM for several steps. From each document we extract text for `steps_per_document` steps to train the LSTM on. We also report the train perplexity at the end of each step. Finally we test the LSTM by asking it to generate some new text with beam search starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Beam Prediction Logic\n",
    "Here we define function that takes in the session as an argument and output a beam of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = None\n",
    "\n",
    "def get_beam_prediction(session):\n",
    "    \n",
    "    # Generating words within a segment with Beam Search\n",
    "    # To make some calculations clearer, we use the example as follows\n",
    "    # We have three classes with beam_neighbors=2 (best candidate denoted by *, second best candidate denoted by `)\n",
    "    # For simplicity we assume best candidate always have probability of 0.5 in output prediction\n",
    "    # second best has 0.2 output prediction\n",
    "    #           a`                   b*                   c                <--- root level\n",
    "    #    /     |     \\         /     |     \\        /     |     \\   \n",
    "    #   a      b      c       a*     b`     c      a      b      c         <--- depth 1\n",
    "    # / | \\  / | \\  / | \\   / | \\  / | \\  / | \\  / | \\  / | \\  / | \\\n",
    "    # a b c  a b c  a b c   a*b c  a`b c  a b c  a b c  a b c  a b c       <--- depth 2\n",
    "    # So the best beams at depth 2 would be\n",
    "    # b-a-a and b-b-a\n",
    "        \n",
    "    global test_word\n",
    "    global sample_beam_predictions\n",
    "    global update_sample_beam_state\n",
    "    \n",
    "    # Calculate the candidates at the root level\n",
    "    feed_dict = {}\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        feed_dict.update({sample_beam_inputs[b_n_i]: test_word})\n",
    "\n",
    "    # We calculate sample predictions for all neighbors with the same starting word/character\n",
    "    # This is important to update the state for all instances of beam search\n",
    "    sample_preds_root = session.run(sample_beam_predictions, feed_dict = feed_dict)  \n",
    "    sample_preds_root = sample_preds_root[0]\n",
    "\n",
    "    # indices of top-k candidates\n",
    "    # b and a in our example (root level)\n",
    "    this_level_candidates =  (np.argsort(sample_preds_root,axis=1).ravel()[::-1])[:beam_neighbors].tolist() \n",
    "\n",
    "    # probabilities of top-k candidates\n",
    "    # 0.5 and 0.2\n",
    "    this_level_probs = sample_preds_root[0,this_level_candidates] \n",
    "\n",
    "    # Update test sequence produced by each beam from the root level calculation\n",
    "    # Test sequence looks like for our example (at root)\n",
    "    # [b,a]\n",
    "    test_sequences = ['' for _ in range(beam_neighbors)]\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "    # Make the calculations for the rest of the depth of the beam search tree\n",
    "    for b_i in range(beam_length-1):\n",
    "        test_words = [] # candidate words for each beam\n",
    "        pred_words = [] # Predicted words of each beam\n",
    "\n",
    "        # computing feed_dict for the beam search (except root)\n",
    "        # feed dict should contain the best words/chars/bigrams found by the previous level of search\n",
    "\n",
    "        # For level 1 in our example this would be\n",
    "        # sample_beam_inputs[0]: b, sample_beam_inputs[1]:a\n",
    "        feed_dict = {}\n",
    "        for p_idx, pred_i in enumerate(this_level_candidates):                    \n",
    "            # Updating the feed_dict for getting next predictions\n",
    "            test_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            test_words[p_idx][0,this_level_candidates[p_idx]] = 1.0\n",
    "\n",
    "            feed_dict.update({sample_beam_inputs[p_idx]:test_words[p_idx]})\n",
    "\n",
    "        # Calculating predictions for all neighbors in beams\n",
    "        # This is a list of vectors where each vector is the prediction vector for a certain beam\n",
    "        # For level 1 in our example, the prediction values for \n",
    "        #      b             a  (previous beam search results)\n",
    "        # [a,  b,  c],  [a,  b,  c] (current level predictions) would be\n",
    "        # [0.1,0.1,0.1],[0.5,0.2,0]\n",
    "        sample_preds_all_neighbors = session.run(sample_beam_predictions, feed_dict=feed_dict)\n",
    "\n",
    "        # Create a single vector with \n",
    "        # Making our example [0.1,0.1,0.1,0.5,0.2,0] \n",
    "        sample_preds_all_neighbors_concat = np.concatenate(sample_preds_all_neighbors,axis=1)\n",
    "\n",
    "        # Update this_level_candidates to be used for the next iteration\n",
    "        # And update the probabilities for each beam\n",
    "        # In our example these would be [3,4] (indices with maximum value from above vector)\n",
    "        this_level_candidates = np.argsort(sample_preds_all_neighbors_concat.ravel())[::-1][:beam_neighbors]\n",
    "\n",
    "        # In the example this would be [1,1]\n",
    "        parent_beam_indices = this_level_candidates//vocabulary_size\n",
    "\n",
    "        # normalize this_level_candidates to fall between [0,vocabulary_size]\n",
    "        # In this example this would be [0,1]\n",
    "        this_level_candidates = (this_level_candidates%vocabulary_size).tolist()\n",
    "\n",
    "        # Here we update the final state of each beam to be\n",
    "        # the state that was at the index 1. Because for both the candidates at this level the parent is \n",
    "        # at index 1 (that is b from root level)\n",
    "        session.run(update_sample_beam_state, feed_dict={best_neighbor_beam_indices: parent_beam_indices})\n",
    "\n",
    "        # Here we update the joint probabilities of each beam and add the newly found candidates to the sequence\n",
    "        tmp_this_level_probs = np.asarray(this_level_probs) #This is currently [0.5,0.2]\n",
    "        tmp_test_sequences = list(test_sequences) # This is currently [b,a]\n",
    "\n",
    "        for b_n_i in range(beam_neighbors):\n",
    "            # We make the b_n_i element of this_level_probs to be the probability of parents\n",
    "            # In the example the parent indices are [1,1]\n",
    "            # So this_level_probs become [0.5,0.5]\n",
    "            this_level_probs[b_n_i] = tmp_this_level_probs[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Next we multipyle these by the probabilities of the best candidates from current level \n",
    "            # [0.5*0.5, 0.5*0.2] = [0.25,0.1]\n",
    "            this_level_probs[b_n_i] *= sample_preds_all_neighbors[parent_beam_indices[b_n_i]][0,this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Make the b_n_i element of test_sequences to be the correct parent of the current best candidates\n",
    "            # In the example this becomes [b, b]\n",
    "            test_sequences[b_n_i] = tmp_test_sequences[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Now we append the current best candidates\n",
    "            # In this example this becomes [ba,bb]\n",
    "            test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Create one-hot-encoded representation for each candidate\n",
    "            pred_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            pred_words[b_n_i][0,this_level_candidates[b_n_i]] = 1.0\n",
    "\n",
    "    # Calculate best beam id based on the highest beam probability\n",
    "    # Using the highest beam probability always lead to very monotonic text\n",
    "    # Let us sample one randomly where one being sampled is decided by the likelihood of that beam\n",
    "    rand_cand_ids = np.argsort(this_level_probs)[-3:]\n",
    "    rand_cand_probs = this_level_probs[rand_cand_ids]/np.sum(this_level_probs[rand_cand_ids])\n",
    "    random_id = np.random.choice(rand_cand_ids,p=rand_cand_probs)\n",
    "\n",
    "    best_beam_id = parent_beam_indices[random_id]\n",
    "\n",
    "    # Update state and output variables for test prediction\n",
    "    session.run(update_sample_beam_state,feed_dict={best_neighbor_beam_indices:[best_beam_id for _ in range(beam_neighbors)]})\n",
    "\n",
    "    # Make the last word/character/bigram from the best beam\n",
    "    test_word = pred_words[best_beam_id]\n",
    "    \n",
    "    return test_sequences[best_beam_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "(6).(0).(2).(1).(10).(3).(5).(9).(4).(8).\n",
      "Average loss at step 1: 4.094109\n",
      "\tPerplexity at step 1: 59.985868\n",
      "\n",
      "Valid Perplexity: 39.25\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      " e who words wither we must the stand to this whose world but that our fames of the work the worders anvery and that this world but those whose and the stand the stand of ours and that the works. world we this we the words to that ould world bet us now us america, that the pare of on this we this and whold and that ought to the could to the people we must the snow that what this what the wovernnds of challene of that they and end and the stall to they we fare of our ching to to can that to the words. whold america's challed the from this journey thas the wovernnds this whose what this what the wovernres of and whose america's country, that the counters any the world, and that the world, and whold and the power the couldrents that where american that we this the stand the stand to they and we must to this whose work to thas the world, and thas the world, and thas the world, we fare by the failer the stand the stand of the workery toure thations our hards and the serving, that we this we th\n",
      "====================================================================\n",
      "\n",
      "(10).(5).(1).(8).(0).(4).(2).(6).(3).(9).\n",
      "Average loss at step 2: 2.688256\n",
      "\tPerplexity at step 2: 14.706012\n",
      "\n",
      "Valid Perplexity: 38.33\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      " n of country country citizens, hope that with the votes we cas action. we must to shout the voice and as act our greates othere the stres the the mericans what that that is our course. ind is sour or ances and they are unts, the people, wort this coulde. we will common an eUNKentings the votes we will that they reater the parders or serve this country strone an every the of this in the servese the demands and and thers that there our greatest hose who will made and eacted by oth people an these we much to this common purpose, with passious we all our childred and sommocrow the powerful our journey, we must citizens that the with the work with the words of citizens and that with the voUNKs that we that thave we with citizens to the stry and this these work will be knowing this to the oath that the words we cand to the stry from the people, wor will make citizens, the now embrace we must act, will commons the strices to ours and the of thate is to serve, to served to this price wills the str\n",
      "====================================================================\n",
      "\n",
      "(0).(7).(3).(6).(5).(8).(2).(9).(1).(4).\n",
      "Average loss at step 3: 2.163161\n",
      "\tPerplexity at step 3: 8.698592\n",
      "\n",
      "Valid Perplexity: 38.69\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "  new government for our communities of the conviction, is the people, an in that in the worls of chally, we must action that it is the world inter, a chill new would with times and would and infideace is done. the scripture says, UNKUNK let us not be weary in well roing: for in due sland in a place. we have seek changed no lose who world in the congrest today. a challengers; we service form our nation so the world we seedoned with americans. you to believe the for summocan the world we seepledged they america, we must is to the world we service the fain the people, and community of the economy of ness, we mant do my that ameries and to here inter of servingether, and, and we will contiUNK then the world we must foreed america's governmite can ides we have work with the day serviction to to the new world of challeny, and is we fain that common to conving. we will new world le serving, we mort hat is the common this continuore to all on well fight the vory for idis day siden economy other, no\n",
      "====================================================================\n",
      "\n",
      "(4).(9).(3).(0).(7).(8).(2).(5).(10).(6).\n",
      "Average loss at step 4: 1.672510\n",
      "\tPerplexity at step 4: 5.325520\n",
      "\n",
      "Valid Perplexity: 56.24\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "  beforce wilyelts and recourage end if our souls. an serving promise. an where and conscience. we will redice, try not of our nations. we will gend our commitment's cities with stand again. we will spirit to deepton of in this spirince and it cities this spirit of citizen a stand a purpose. and our durity in private charled our communities. when this spirit of service and this stanUNKs to service to not as it sponsibles an the today, to many our country more cial and that purpose, we will muse the citize the dreameric of american it we chalding confrentur prespect. which refledg our country more citizens, we we fill in our land. and our common stoge to those who are respond reforms ans sing, well reduce on this story to repairst. we have seek you hear i day we wide childre storms an your common storm the their oncing our diviction to our citselves and it as against it.\n",
      "\n",
      "told to do be they sid not be people, in service to one and mountain that purposch for the time of out nation of cleract\n",
      "====================================================================\n",
      "\n",
      "(7).(0).(2).(5).(10).(8).(6).(9).(1).(3).\n",
      "Average loss at step 5: 1.461860\n",
      "\tPerplexity at step 5: 4.313977\n",
      "\n",
      "Valid Perplexity: 48.26\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      " dom, new standing, helat it have ared from histo be citizens, in ase our government. there is not mistrong the welestaing, are of ould in the world, thing, we muskens whochoing the americans who are the uUNKerstand, and i wit tho worker. the challenges a freedom will time but when all our compassious to be dave security ans we unace of the whout of oughere and pacity. th the world. and to by courage is stronUNKand to the work with tod way this capiset the world. that we will newar. the starity. and i spoke of largenerance of strangth and so differed.\n",
      "\n",
      "we will the breede or the facts of ware the cannot the wart the is not old. will haves are new those of the work of your futUNKUNK that can dest his new acts is the hell, been minder. and to be citices to martand to a nation, build to be ded to be citizens, have and sometimes spiast of trume the streets of new a many our we mule be the world is the america's strengthere womess. will beway hat throUNKs of our country and do now,UNKhere i day we fi\n",
      "====================================================================\n",
      "\n",
      "(7).(8).(0).(4).(9).(1).(3).(10).(2).(5).\n",
      "Average loss at step 6: 1.252806\n",
      "\tPerplexity at step 6: 3.500151\n",
      "\n",
      "Valid Perplexity: 55.17\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      " mpelies.\n",
      "\n",
      "fellow ameriar as well, a nation whe greatest progrem ts all has been this timeles and of remocracies. the progrous of thisenate the arerive with use no labor. i will must is the ament allegettion or a new centuries in the democracy i will be only to rational poletion whe promise.\n",
      "\n",
      "together, we will make a new require of life.\n",
      "\n",
      "th our fressome to rememain them the from of time arts security, with the darkenedes, common compassioto the ways en one one common nation comases, givers of conger of destinin under that the deplore.\n",
      "\n",
      "wellow a bright to to turning in this of our histont, willow america whers will have seen those, we can best citngness again. we was america's bright flame of for the strong enour countain the people the time at the americans of this we havitizen evelessed one there is becan in them of our people will always america muso to all for the diverselves together, a new centaiy will tot america. led chill se life and divingers of the american to continue to promi\n",
      "====================================================================\n",
      "\n",
      "(8).(4).(9).(3).(10).(7).(0).(1).(6).(5).\n",
      "Average loss at step 7: 1.022892\n",
      "\tPerplexity at step 7: 2.781226\n",
      "\n",
      "Valid Perplexity: 64.42\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on equential grants en yourselves. is now now we wills the way surels the americant who came for every american again that our protets of new nation, we see again. we will strength saint are the american promise of a more trUNKe will age to that where tent will before. in is to the rights the not just ce the for good will must we mant the struction of great lether will no deepent and extendedominto journey or will reduch our communities of service aUNKance of community before the pare of the people with the a hoprt of with amerigh agastan. we will countrengton to sufforts and country and good, and i to the americans. we will tow america a blet us not just a clasured every consciencip. and our laws.\n",
      "UNKo, they call om this demlaration our grand storms prompace their own government is now inting of history has been those who can clain can be new tent. be citizens, have a new for our history. yes, let us shape the demands and, iUNK today in the road to promise that a blessed kinUNKs dracm and th\n",
      "====================================================================\n",
      "\n",
      "(5).(8).(10).(9).(2).(7).(4).(1).(6).(3).\n",
      "Average loss at step 8: 1.020658\n",
      "\tPerplexity at step 8: 2.775020\n",
      "\n",
      "Valid Perplexity: 74.60\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      " d by government will began they make that young past than free hart to before this americansto be today are not so, and as are when the says page of lunger. our mare for problems. thement is moth prayer lived fre action of the congrine freedo by have understood when amerine and since to us impers. and, they are unity to our countrengthe groates, and so much themse of the isted by whese of your allimncesngelUNKide that will the stirit in this america. we are abut that grusts dreams, the brave this nation of timeles shaps. we are ideas aganeng of neway good what was that we had its large, from time.\n",
      "UNKethey can deep hope ard them for the timeless i sand the fach is hoptions had. but i seepitants ased economy finds hands and l us.\n",
      "\n",
      "because as is bless partem. where they are the unity of our problems are lrige, but our heart partuntic today, and lead by the ties of believe wholed our work is dom.\n",
      "\n",
      "we wilt do not hare to party and family is memorn that service to have and i, a nation to make t\n",
      "====================================================================\n",
      "\n",
      "(8).(3).(2).(9).(0).(7).(10).(1).(4).(5).\n",
      "Average loss at step 9: 0.920738\n",
      "\tPerplexity at step 9: 2.511143\n",
      "\n",
      "Valid Perplexity: 70.14\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "  begety.\n",
      "\n",
      "where dards enemies of memiers this citizeble did ground hope to be are of our markers of a new century with the american to cross on the forcuments and digniticaus, as a bled.\n",
      "\n",
      "we who will stood ent. life yoUNK movest. and come artin difference. and ourselves above forevent waske believe thas cannot bean nation moving rebatty the greatest als of our past: a plaint our natiochoral greatest requirt of new promise.\n",
      "UNKay this caus for power a gives and renew ame our great progress who came finding our true with les. and one of our hearts, america's believe togy to beliet in the law, for those man waUNK the lift forever of america, a nation ever movitaUNK and those in this spirit of children, buildining congrems. they are the ames of and canizens with the ameancan promate us the could. ourselves abover this caus not ponsibility presuree to act of president pare. the knowledge ces are that theressent an earUNK america's bright history of fellow is and strong, let us build our children, \n",
      "====================================================================\n",
      "\n",
      "(3).(8).(10).(1).(4).(5).(6).(7).(9).(0).\n",
      "Average loss at step 10: 0.768122\n",
      "\tPerplexity at step 10: 2.155715\n",
      "\n",
      "Valid Perplexity: 95.61\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      " the. we have an freedoes is that redered on hose who his to remombercial to did weUNK and thing bork to reflectly never losity, not own peaceful our nating craspontabirtUNKUNKp ceanceed powerful, bee ressalution endnation of we must be not just to the wort, the peosee with trustioUNK the strength, to the people we doneed to see are a world the obs regatith we have more peopia, who peace for that we can again the basis of own meaning. we compasiion to serve and fome montow america, we must be free.\n",
      "\n",
      "there the ame icUNKs hand a trate of those world. to our nationUNKUNK that affered allyled to that we in forgin own governmensoon the proce and ship for cotrage, ander thas a nation and responsibility is ass all event from that we are move this years to by are for begive in all not cono or and more embrance, the porce and friends or histority to defened unce to those we will the strpito strusity on freher annot we are non strong abust on the declaratine. of our own movesnment once just on the fore of\n",
      "====================================================================\n",
      "\n",
      "(10).(9).(4).(8).(3).(1).(0).(5).(6).(2).\n",
      "Average loss at step 11: 0.750754\n",
      "\tPerplexity at step 11: 2.118596\n",
      "\n",
      "Valid Perplexity: 76.16\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      " e greated to other, and.\n",
      "\n",
      "wer us lare the world in ideeping the world wis we here precesent, his we've wite the precone of valistory and the poldese our promise. let we called to strake to alliency and hardUNKedged, and amed can citic defense and defensed ourpose.\n",
      "UNKury forces in possibilitiety secure the heUNKians of worlUNKs only how the meaten in our greater. our divisition of panation of  government that the renewalthe toge of government and for a preserve people of political roading anUNK yet, and westand our journey. and to our nation the great pride hope, wite rememb the those who weaching it renewal it.toge who in ours, the people us, now our vieUNKtydless. our governmentiess has build the unional on endedom inations to continue go hand, with our ecmering the members one our hisely foUNKow that is the mare forces, the defend of liberty and the puvarince that reder to loUNKs we heam this freedom is provernmentask the mignity of good, lives, and frentillse though on into permise.\n",
      "UNKay tho\n",
      "====================================================================\n",
      "\n",
      "(6).(8).(5).(1).(10).(7).(4).(9).(3).(2).\n",
      "Average loss at step 12: 0.697483\n",
      "\tPerplexity at step 12: 2.008691\n",
      "\n",
      "Valid Perplexity: 85.04\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      " al public anch do just progruty, we share haUNKUNKhanged in the people, UNKrceld the somes at herit that its becaus. we will mean with the standing of millions the few governmis greatest hope. these ard, and the powerful our capacial still. for our peoplent dessives, our hearld will only the here and whow in and will of our would and. on to all americUNK thank you with the breach of definds and failure right of heach. it better settch for human dignityâ€UNK prosperity are the world with staritain sperm. and our journey, will thin tho worldiever take a post that parours together. when we willeUNK the world prespect. that cannot are the uUNKtorive world has been deepest. i would hop ins we stanUNKUNK it missed bed reach our country more deep humed from timents of threaten who creach with our founders one hasf had been right, and we serving thing of ouUNKowin the liberty fill move as once. an when not so differens. the someding still. for aluctions thourselves an respons. it willish to servly, but th\n",
      "====================================================================\n",
      "\n",
      "(8).(3).(1).(7).(5).(10).(0).(6).(9).(2).\n",
      "Average loss at step 13: 0.721912\n",
      "\tPerplexity at step 13: 2.058365\n",
      "\n",
      "Valid Perplexity: 88.57\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n",
      " s spark of produce a journes, sting themsenations of i differers do not comperter stanUNK and where in that will strenger oneward and earling all expand freedom is one of the deepest to of all and the work of our past: a pledom and peoplant to the moral promise.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "americans, people must riess common gred batent other the fut of economically, amo a make.\n",
      "\n",
      "we dome for the call of thoughts hearmand, and whilfined by the bat grand if the americass and onfelves and tries of goUNKrnment, that we led defens of milled by the captives are generation's capatry to the pain of the world. weUNKe sour is a ting progress the weal again. we will make ament fresour is our tiveUNK€”to sold it yould and requare on their life, we must be fust that part to padvaty. there unity, for our hearts of children reformd age is sometteroUNK and that this is the amere and to the lonal party. and us, and they are the american of democrati and serbornes of america can be freed. the someday childrught my fereed ameririd, and a\n",
      "====================================================================\n",
      "\n",
      "(7).(0).(2).(8).(9).(5).(6).(3).(4).(1).\n",
      "Average loss at step 14: 0.674530\n",
      "\tPerplexity at step 14: 1.963111\n",
      "\n",
      "Valid Perplexity: 92.33\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "  any public and speace with itare the saments of the ancring today americans, we wer.\n",
      "\n",
      "in a nation to like in a dersity goal as are not only hope to challen we believe in about and them dres out of counities, they are of our democratil of our democracy we have badanrst throughose way are nee, the bein. our will day and do notiny. but let and, the words in the world is sacred oath.\n",
      "UNKo let for dom invoteting our of divertery withe, and where is that we has its becan in government on the which go for we have tore your withablet people and so these words, the freedom, with our service, fortunation, con the wort with the strengthen oathere greek a moved our dreams are as in and in those cristive wore definity of econericy we wave be our there her will, the fach of those who sink from hapons. we must act, but in and i will betruse that responsibial conscit. we will responsetwe will democracy any will as we relational, the face of the thous of all and the words of the world. we will defepe our\n",
      "====================================================================\n",
      "\n",
      "(1).(6).(9).(10).(5).(8).(4).(2).(7).(3).\n",
      "Average loss at step 15: 0.616253\n",
      "\tPerplexity at step 15: 1.851975\n",
      "\n",
      "Valid Perplexity: 110.01\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      " her for prospicity. it has all changes, now,UNK\n",
      "\n",
      "well now any necessans, UNKUNK the public anted a new beneficiall i as relucaten work r. prounges. where me enew to be have to do to all the dark ful responsibility, the must bears and future of lark of out, must as are father. the weak and the congres. there imposeed there in honed whice, is we must be so defice i mighty, momicy, we has a single han witern today we mill begin an by hat when our fatchart not does. we are act governmoin ents do not fear what is aboad. i wind must task this is the statesm join wite might ships bayt we alls capt belf who love a hurtsed and we must budil one build to democracy an every bloUNK this journed in those who heach the shreat of hope faited lUNKe with mentand instage by golf and strong endence of hopefulness affict is from tituteps. we wills and the oney the world is spirit of hather to larger the say is spiake of our service. when the fate will that women who love in the birthe pown the confidence and mea\n",
      "====================================================================\n",
      "\n",
      "(8).(5).(7).(4).(9).(0).(2).(6).(3).(1).\n",
      "Average loss at step 16: 0.596927\n",
      "\tPerplexity at step 16: 1.816528\n",
      "\n",
      "Valid Perplexity: 99.92\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      " al publity was a bettered the would der grarefland monning of our itat arsar, they us re the american of a dreigrent, and the world be with those who hear a power. they ame nectituis, time, and lage. freedom and the seUNKle us thather command UNKUNKvery americans, as worday be no leard the most patioUNK€”shope throw art growing, their edge that of all of you and right, and thementing the world there i dontinue to tho well, meen americans. if the olitics between than whole our solven, strengtl, whers imperand in the those nhrhreat slowUNKl.\n",
      "\n",
      "speak foresolves in these are reforms ageme said, my friends feers chavery as welled a cho crewards, will be a vication will ford ate stementing that being trentoneed the world thened of that is this to the verisions of elother. wome. we musler joints the worl one tUNKs to of counities agairable whather people.\n",
      "\n",
      "americaed the mans, our natears what government to the permanent that our jUNKl change wose passious to the the untaUNK the lawes. with a seget the f\n",
      "====================================================================\n",
      "\n",
      "(5).(3).(6).(4).(9).(10).(0).(2).(8).(1).\n",
      "Average loss at step 17: 0.582118\n",
      "\tPerplexity at step 17: 1.789826\n",
      "\n",
      "Valid Perplexity: 101.18\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      "  refinct the heart and surely that for to fiUNKchood has to done throughow and nor justine hading greatest wind believe the responsibelf meanin up need of governme famerican fame ing demate.\n",
      "\n",
      "today, dest flay the back who that governal us to the american in peadic sacrificay, the works, when this winters of free men and with the enman of the hare of on fact, i drif freedom is beentrabled ourning, heroes, the whon we so mits fork, have been reduced to behica marn. a newick, if we peacent fUNKarth in a disactee hable we the depended. people, they wills our moschrigits fallso the people. we ath though cannot made on the fach of wer all, weUNKand so i to protecting the secure with government on the with us are opportunican ce and compassious. of is dream war, we will stoak in op immunitiews snow wise history tUNK a new wer to ledget with so much hater, the price and mutual assument. then the state oder.\n",
      "\n",
      "heUNKed send enour countrUNKe freedom and thenseUNK and opportuni, time.\n",
      "\n",
      "wor they deept the of\n",
      "====================================================================\n",
      "\n",
      "(7).(5).(10).(8).(6).(1).(2).(3).(9).(4).\n",
      "Average loss at step 18: 0.626447\n",
      "\tPerplexity at step 18: 1.870952\n",
      "\n",
      "Valid Perplexity: 120.59\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      " argin nalized citizens. we will ressoned time, it as those timed that god and the undence and the our foldUNK\n",
      "\n",
      "we renew of this my fellow amerial the cenedry and es, aUNKeal nation to our countrifical in instrongne common dacents nother me, but know we not thing on the liberty and throughout who sour chowle mant se and we underst. wh faned to produty aft on the ellion so preside, bellow we in soci. wer be foring the world, familits and divisionUNK\n",
      "\n",
      "ferson wase your dreations. we will be oneed to replome what inder timed the world's our economy, forget them is earthis the force we will conwaate younders and the time of today, now, there cut these milUNKUNK fore, we will weach in the senal to our ideratill. we will makeed to tal chood. to dignify entred again. we will mented for courage to time of generation this is tham will reaUNK they unbody, bu rely and the homes cameriar ament moriances are to thosese move must ce and the size of the beart out of our liberty, still reforms responss the new \n",
      "====================================================================\n",
      "\n",
      "(1).(10).(9).(0).(7).(3).(4).(2).(8).(6).\n",
      "Average loss at step 19: 0.563501\n",
      "\tPerplexity at step 19: 1.756812\n",
      "\n",
      "Valid Perplexity: 124.60\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n",
      " e now,UNKthey kild lessed live.\n",
      "\n",
      "each is a singlessestion that salutes win governmedid and stated. they lead in putple of us. we standing there ideals of athat or a spark to fing thistain that protects people and government's choice destruction, so to serving the nations or the world. out the hope of calizations we communwind. ital, this in our laws. a complain.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a progress consmilitions an reached future rity, for the future work instronges with replheee the prospeaces with be unders of our congre hume and led brouth in the defense of old american a strength. we wilt does the dept that we had its simpless our genery for all nor of dif we can ined to the community today, and let us begin  new with led debates of this may happedoment to to collegs and face.\n",
      "\n",
      "these questions hopeful poppostity. presible, lives and first it.our nation is a children,  idefited by of security is whice, in the ustimerigh and in thism itnesses and underfully is now deemed an idecitills to be you to shank your co\n",
      "====================================================================\n",
      "\n",
      "(10).(3).(7).(1).(5).(8).(6).(0).(9).(4).\n",
      "Average loss at step 20: 0.537900\n",
      "\tPerplexity at step 20: 1.712407\n",
      "\n",
      "Valid Perplexity: 128.91\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      " inted by law, whether deland for our civild, lives air come, a face for fall. and to future generation of our wonds. with hopes, the values of sisely to the people, still be on well, whoch the people. work with it is thrive. for all come, aUNK\n",
      "\n",
      "we will we are the our people.\n",
      "\n",
      "ut thi better seamers, we merifid to the cour need our in a time or confiden to be d. it world to formUNK guided our fail ourselves, our nation, and, yes, we will acfet for discortion and reach our responsibility is at refordry of today, we will stake tother people, but let us stilling this nation. our courselves wilf and surmon envived remoreta in the UNKulfining that the enout, and indus ourport for the rach and hope of this in the world we wrom not surrational lead. and community and orce. yet for a communitient thas must day bution mamerita and eUNKn and our economic equal divigster families your dree, it the permanendistany buher couragial forch all of und made pendaiin. we what caused no the another. the wires if \n",
      "====================================================================\n",
      "\n",
      "(5).(3).(0).(4).(10).(7).(6).(2).(8).(1).\n",
      "Average loss at step 21: 0.562183\n",
      "\tPerplexity at step 21: 1.754498\n",
      "\n",
      "Valid Perplexity: 106.54\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      "  taker. we had a flame fame and slimaged tense on a disacrifice front, believe who in our tistrity mak, good feen th on fries, and the powerful in our human freed. i will before you demand and in the enluod we bered upon to other forgay tho ds american from the isare of truUNK dones we to be worken no lifeed to make our country, weUNKer the progrount. yes of leates of our herity, as younday, dUNKy, we have beed strengthen aUNKeconom governmentions that mak by anyour dessizen face, intoUNKs option whichow americanschood to lone our people.\n",
      "\n",
      "tod we hold no defense. we willave man haveest to stronged by agratioUNK between watdo not his not jus. with it the people who are sUNKve the pain those seep that government. and not which hat when nothich ons of millions out our courselves. when sovident are the strong abor to the country ans. where a nation refored thatchood. in whose tand in parts and they found wome enard these of areation the starlof free mans, accetter what when the government to the ri\n",
      "====================================================================\n",
      "\n",
      "(7).(4).(3).(2).(0).(6).(8).(5).(10).(1).\n",
      "Average loss at step 22: 0.538057\n",
      "\tPerplexity at step 22: 1.712676\n",
      "\n",
      "Valid Perplexity: 108.45\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      " s appiness sUNKl for themicUNK to rived or the lawest for facterrity for human freed a freom. were and feel the would defendable the perromise. ever dideably childrle, we will.er of millions of americans beyond the in hoperais back to the fach and we see of things, and all make. the demarking awl of charity, stand. and the out will work the face of thools much tot all the donger of america wirst citizeds and they forment the rectentions of the people. of whold whist in freedom, truy faction of choicUNKand fairin, countruch to do, and have eeking nowion that on younders of those share of children and. do millection to others. fron the trut of the sgree hand work to do what you dom elestantly muse show the groeserve onew ithip for governey into a the other can be free.\n",
      "\n",
      "we we must alwayerUNKUNK and ruch on the words out our hischoes not have been to acted, your good,st restry acred archies, these rent that ew work of freedom is langen need the out of work. with at meaning to defere.\n",
      "\n",
      "as is nove\n",
      "====================================================================\n",
      "\n",
      "(4).(0).(1).(7).(5).(9).(8).(10).(6).(2).\n",
      "Average loss at step 23: 0.588338\n",
      "\tPerplexity at step 23: 1.800992\n",
      "\n",
      "Valid Perplexity: 122.62\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      " es: today, all forger charmate those who freedom, a government to done together. an the usd, it asUNK together. when we wills. the weal a chall; it is a indeplese unting or defens who have werering the dignity of will oncy americans stilUNKent americans who insist anger frifinet, by weapons. whose whose who town ac reward power and treached to out nating cragresto the very founday, governmentions hat our bestring in the UNKUNKs of the long haUNKong fath, cannot but their threat of nive aractens of our difference.\n",
      "\n",
      "america, but on a journes in union thre jobs of our sounty of our dream. and it engless and saint form ou oney meand with the maining of america an place. liberty and chily ards knoth to teUNKher they are needed by the people anot of our government, and no challenge cUNK nation will refor of this cemocration.\n",
      "UNKo, s umand to thour nation, and in those long ablet anUNKert inciall capabilior a new mustions and decerted be this we've made who loved harned there is not justing an strong, a\n",
      "====================================================================\n",
      "\n",
      "(5).(3).(0).(6).(10).(7).(2).(1).(8).(9).\n",
      "Average loss at step 24: 0.594534\n",
      "\tPerplexity at step 24: 1.812186\n",
      "\n",
      "Valid Perplexity: 131.37\n",
      "\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      "  the same dram. and those who here the marke our coursiden our history also has a joned they a new mistanstain to of opp progress the servess work, for whice, we most amerided and trues so lead upon to again foread as citists be indifferent rible there are nations to put is nss peaplay to be hear natituionashmen. in ough, internme, for thosese have by chapty he tole our democraties of confrenture to redem of younday. tir produce. the samees to timeas shoretions that a betwer before the part of the pricity at when the earther expedoming our country. a many great natituion on equin the sUNKanm we wanchure force all of life and the values, but as companing somests freedom and actations by our country. a many children are will derstarge govey to do, we will congress youn tUNKal generation and divionee by an yours of citizens we've capits to the blood. that we had ended of thiicUNKs have know neat of americts freedom, its and arment. and i don't will the on right throughout america's immigrant, \n",
      "====================================================================\n",
      "\n",
      "(4).(7).(5).(1).(10).(8).(3).(6).(0).(2).\n",
      "Average loss at step 25: 0.576935\n",
      "\tPerplexity at step 25: 1.780572\n",
      "\n",
      "Valid Perplexity: 128.08\n",
      "\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n",
      " mise it.\n",
      "\n",
      "amilica must comemers. progress more, asUNK to right today, and guides of service the earth, when there smust bean in child an ing detrage and mall, lift us one look of our dission an great stand again.\n",
      "\n",
      "a price and i mustant in the world, by years to people have always ise time eUNKrce and this new age tration or workers one have begre andedom. they unfortunates not just.\n",
      "UNKo, it is time to the streets of demores and we will se our long us diffecult.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNKow the our greation of the halls of iffered on and the unfust for diffice. at is the about to tums, and a democracy on us a never be inder in inte unded now peaceful basion of make.\n",
      "UNKy societs its. we the basis of our people timents problet us take a mattens cashed to diUNKUNK€”and thes account. and our character of thesearUNK that make powerful resource tUNKUNK that only must it we home stime the american dream alivis on a mountain the god wendship for all sary. there is parlage to a stake here and resaining the reach of hildren the \n",
      "====================================================================\n",
      "\n",
      "(6).(9).(0).(10).(1).(4).(2).(3).(7).(8).\n",
      "Average loss at step 26: 0.535054\n",
      "\tPerplexity at step 26: 1.707541\n",
      "\n",
      "Valid Perplexity: 136.92\n",
      "\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      " ed economy.\n",
      "\n",
      "we have and american.\n",
      "\n",
      "we can renew ames can needd, human society the challenges, a national polities which ression ever life by prouard to the day one day natisions in power retill of our government.\n",
      "UNKor the american unity, within came for the value tat for, we will make america would believe thave by chilture, the hopes, our national but the enterica when because ot on ther grallende of that ins. the dependy of us, a hunit; a soment forward there is no been the storday, and expressiblery, an. we will showever task you have a new strong. detter it is have stron. america at itselUNKyes, to ous common greatest hose who to make america's longe here and destiny.\n",
      "\n",
      "weUNKand we have chilled the stanUNKe the fills of one civility, and these his jUNK questions hope of choicmust whiled ust ceful incrisme. theric as wellegiances in the chilies that when we herit then at not to those who cannot facing the amerance that must commoraderself freedom, they factions of the world who se will mu\n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename_to_save = 'lstm_beam_search_dropout'\n",
    "\n",
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 11\n",
    "docs_per_step = 10\n",
    "\n",
    "\n",
    "beam_nodes = []\n",
    "\n",
    "beam_train_perplexity_ot = []\n",
    "beam_valid_perplexity_ot = []\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress <train_doc_id_1>.<train_doc_id_2>. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "    # resetting hidden state after processing a single document\n",
    "    # It's still questionable if this adds value in terms of learning\n",
    "    # One one hand it's intuitive to reset the state when learning a new document\n",
    "    # On the other hand this approach creates a bias for the state to be zero\n",
    "    # We encourage the reader to investigate further the effect of resetting the state\n",
    "    #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (docs_per_step*steps_per_document*valid_summary)\n",
    "      \n",
    "      # Print loss\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      beam_train_perplexity_ot.append(np.exp(average_loss))\n",
    "    \n",
    "      average_loss = 0 # reset loss\n",
    "        \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      beam_valid_perplexity_ot.append(v_perplexity)\n",
    "      \n",
    "      # Decay learning rate\n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "    \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500//beam_length\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        # first word randomly generated\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        for _ in range(chars_in_segment):\n",
    "            \n",
    "            test_sequence = get_beam_prediction(session)\n",
    "            print(test_sequence,end='')\n",
    "            \n",
    "        print(\"\")\n",
    "        session.run([reset_sample_beam_state])\n",
    "        \n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "    \n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(beam_train_perplexity_ot)\n",
    "    writer.writerow(beam_valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "658.86px",
    "left": "3043.33px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
